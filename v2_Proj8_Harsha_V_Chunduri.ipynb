{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color: #ac6aad\"> Project 8 - Introduction to Neural Network and Deep Learning (Venkata Harsha Vardhan Chunduri) </span>\n",
    "## <span style=\"color: #ac6aad\">Objective:  </span> \n",
    "##  Build a NN model to perform Digit Classification using SVHN database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\"> Import necessary libraries </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\"> Load the X_train, X_test, Y_train, Y_test, X_val and Y_val datasets from the h5py file  </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X_test', 'X_train', 'X_val', 'y_test', 'y_train', 'y_val']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load SVHN_single_grey1 into a variable\n",
    "inputFile = h5py.File('SVHN_single_grey1.h5', 'r')\n",
    "\n",
    "#List the datasets available in the input h5 file\n",
    "list(inputFile.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the X_train, X_test, Y_train, Y_test, X_val and Y_val datasets\n",
    "\n",
    "X_test = inputFile['X_test']\n",
    "X_train = inputFile['X_train']\n",
    "X_val = inputFile['X_val']\n",
    "y_test = inputFile['y_test']\n",
    "y_train = inputFile['y_train']\n",
    "y_val = inputFile['y_val']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\">Flatten the images for Keras </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: #0000FF\">This step has been performed as part of model creation. </span>\n",
    "#### <span style=\"color: #0000FF\">Flattening is peformed before output layer using the below </span>\n",
    "\n",
    "#### <span style=\"color: #0000FF\">model1.add(Flatten()) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\">Normalize the inputs for X_train, X_test and X_val</span> \n",
    "#### Since 255 is the maximum value (including 0), we will divide the inputs by 255 to normalise the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train : (42000, 32, 32)\n",
      "Shape of X_test : (18000, 32, 32)\n",
      "Shape of X_val : (60000, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Find the shape of input datasets\n",
    "print('Shape of X_train :' , X_train.shape)\n",
    "print('Shape of X_test :' , X_test.shape)\n",
    "print('Shape of X_val :' , X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see, all the inputs are having dimensions 32 X 32, so we divide these by 32X32 array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[255., 255., 255., ..., 255., 255., 255.],\n",
       "       [255., 255., 255., ..., 255., 255., 255.],\n",
       "       [255., 255., 255., ..., 255., 255., 255.],\n",
       "       ...,\n",
       "       [255., 255., 255., ..., 255., 255., 255.],\n",
       "       [255., 255., 255., ..., 255., 255., 255.],\n",
       "       [255., 255., 255., ..., 255., 255., 255.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build a 32X32 array filled with value '255'\n",
    "\n",
    "a255 = np.full((32, 32), 255) \n",
    "a255 =a255.astype('float32')\n",
    "a255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape is:  (42000, 32, 32)  and has  42000  train samples\n",
      "X_test shape is:  (18000, 32, 32)  and has  18000  train samples\n",
      "X_val shape is:  (60000, 32, 32)  and has  60000  train samples\n"
     ]
    }
   ],
   "source": [
    "#Normalizing the input\n",
    "X_train = X_train/a255\n",
    "X_test = X_test/a255\n",
    "X_val = X_val/a255\n",
    "\n",
    "print('X_train shape is: ', X_train.shape, ' and has ',X_train.shape[0], ' train samples')\n",
    "print('X_test shape is: ', X_test.shape, ' and has ',X_test.shape[0], ' train samples')\n",
    "print('X_val shape is: ', X_val.shape, ' and has ',X_val.shape[0], ' train samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\">Convert the class matrices Y_train, Y_test and Y_val into one hot vectors</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Keras 'utils.to_categorical' to convert class matrices to one hot vectors\n",
    "# Our data is having 10 output classes, so we declare the number of classes as 10.\n",
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\">Print the train, test and val shapes</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (42000, 32, 32)\n",
      "X_test shape: (18000, 32, 32)\n",
      "X_val shape: (60000, 32, 32)\n",
      "y_train shape: (42000, 10)\n",
      "y_test shape: (18000, 10)\n",
      "y_val shape: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('y_val shape:', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\">Visualize the first 10 images in X_train and the corresponding Y_train labels</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 images from X_train\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAABRCAYAAAAdIZjJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsvWmsZdl1Hvbtvc905/vmoapeva6u6qruZs/sblIkJbpJc5CjKJoVCLAdxVESBHEACwkkGUGgAEGCDI6NIIkNWbZkKzCj2VEkUSRFiiKpZrMH9lg91Pi6ql69euN9dzzT3js/1jr73Ec2WfcZQQcB7vp169W55+55r/Wttb4lrLWYylSmMpWpTGUqU5nK5CL/v27AVKYylalMZSpTmcr/32SqQE1lKlOZylSmMpWpHFOmCtRUpjKVqUxlKlOZyjFlqkBNZSpTmcpUpjKVqRxTpgrUVKYylalMZSpTmcoxZapATWUqU5nKVKYylakcU6YK1FSmMpWpTGUqU5nKMWWqQE1lKlOZylSmMpWpHFOmCtRUpjKVqUxlKlOZyjHFez9/7LNnftFCCACAaVQwOtkAAHTXPGRN+rtMgeq2AQC0Lg2gtg7c9/OVGQDA8GQVB+cUAGBwIUFUTwEAcTdEeMsHAMy8adF+rQMAEDv7EJJ1xTCA9ei7o3vnsPWhAABQf3IXP7b2CgBgNTiAtvT8i/11fPHSBQDAlZ/9++JuffzZZ3/BSkHs7r7U8AT1JZQ5DOjrntDu+U5WgeLnJSx+oHUZAPDvNt5FVVLbMqthQO/Z1wkUj2FqLRImktcQqPJ7DACf3z+rQnjg/toUPZPTWFng1/Z/AABwM26jojIAwK998F983z6e+dx/Y8OQnh0eVBBu0i8FhwKSpgHWAwyvLJkCKrXuMw8HIADjl+8t/u6NLPwhPS+0hczfgynfADLjL0iBvEpzFbcU4jlqfjJnkc7ROItqDj+kfl/6qf/yrnN44fd/1Y62q/SP0MCr0HdnWgPMRCMAQCscoeal7jva0mu7aQUbHVqnnd06VIcGQuQCukZtVrMJokrKzbeohvR5rjJE3U/cO4c5zf8gC2D4/ZlW7v+lsMgM9T3NFXxF7//WZ/7bu/bxg3/6K9byO7URGIxCAIA1AgGPVehnCDzNz0iMUpqwLFMwhvexVrA8FUJZFD8spIHOqa1m4EHE9FmmAoqHLdwXmHmHfquyOYDsU99FlsNG1Pf4ZBPd0/S7o3mBvE7r4dIv/7279vEPrzxiv9x9AADwZ//6KZz64gAA4O10gYNDAMDhM+dgfn4XAPCfnfkyYku/tewd4oFgz70r42WYWun269B40NzjjqlAgf6+pPpur2dWIra0BjQE/tXehwEAn//qY1j6Fr2zspMi2Ka2icEItk+fP7/9j79vH8/+9//A8k9C5ICK6XF/APh9+o+gZxH0aA69oYY3oL0rBwnEiNdamgGGJzEKYRoVAIANPBTntRhlkL1B+byi+bRRAFun53XVhwkVt8dC6GJhCFhJ77Fe+fkrX/ylu87hX10/Yxt8sGgIDMcOjZak9meQiPhM3dcRuiYCAMypAXw+WGKrEKD47MHn52/ks3g63OL3AJ8f3AcAMFbgdtamvxuFR2sbAIB1fxc3sjkAQM9EeDx6FwCwrDQupnSf+SLHVk7f/amzL961jx/9sf/Bxm0at3hOuHNRaGD2bdof1Y0uxOaO+05+7iQAYO/hKjoXaK7V8gjZYchftoDH56iygOSFYgQs711YQPRpbUY7Eo136ZnaVgY1pN/N6z6GC55rW7HepAbCDp83/+IX79rHR/7u/2wbN+mdMrUId2Nq8yCFVcUaS916y+fqsB6dbTLJIWNet72RW6u2VoEJebCUgAmU61chMs0hEr4HjAEyaoOtBBCa9/F6Exv/Dj3/n374z3GW18NO3oQv6Pm/fd+z79nHKQI1lalMZSpTmcpUpnJMeV8RKEgJDMmCH11YxK0fpJ+fe3gHH5jZBgDsJzW8+e3TAICwE6HGWqtencP2U4xYPT3C37j/dQDAB+vX0FZDAEAgNK6kiwCAf/jyJ5C0CAlYfMGDusnae5bDzDcBADuPBXjkU28BAH5+6WswrE9ejE9g1usDAH5h/qv4eOtN7sDfv2sXcyuddZ4a5VAKHwKZIQ3ZUxoFSqWERaJpHLpZhN0a9fGmzrBgGflQVYdwKCHQKYxFATRkqRiHgt7/QlLHH3Ueo3HWPm6PWgCA7UEdO/v0frEVwhuyxdoTCDustn/w+/cvijLEMWn93p6P6ib9vXkjg0yoYWIcNLIlWiSTHDKhPkFbp75bpSC4JqMYxBB9mk8724KJyiVqfbZutYVI+T1CwGfLwxuGEIwgGF9AR/QDpgr4fon63U20lhA8h9YICO6QNmIMRTRuTkKpIfnvVS/FfJ0s9UwrjAJCUoyW8CKyohbafYfUdIYVpIzU7I2qbi1UvMy1p+anDoHq6Ap8VaJCBRriKwNPGkwqShpoRq8y7Tm0yBogV/T3SgDUA0bKYN2a7dsQec5jaywsv0d5BhVG1pQ0GMbU92TkoQBdpSY0DiAL2xYm3FjTraecZWmlABuBUAkcejGJxNaH4hdbz7o1ZusVyJwaZJVw4zAwIYaGLPgFr4sB/70mjbM0lbDY0YS43MpnEDNccCdvocpIyS01RFvRGmjLIWYlWduHJsQbnWUAQOsdgdYrfCZZC/CYI0lh+Yy8mxhVIrdCAMWishIlUpBbZ2nLzEAOGOUbjAAeA3gKNiLE1TQryBo0bzqS7j3eyIfPiIDojyDilD8P3WfMNWHCCv8wgALpMCUyaa04ghBMIlu6DgBoy5FDjnxh0OD1fkd7DoGKRI7Yfdaojn2OeP0OjHb7ZtU7wEZObX45XsPXO+fo/VKjxlBpYjz8ZU5eiMthBycZmVz2Dl0bY2uxpPrcnjpqskSS7yZpQ2KwSiM0WtWwIZ+jmYTM+PyzTdRGJeKdzNE6HS0IeKu01p44eQOfmXsNAGAgIXntD02I3ZzO/Z6OMMhD16+Xtk8AAPb9NoJDPnv2BAqcW+QGmkGttAUk8zSeMpbIo8nxF/PJA5iIxuTG5ixmnqc5nXknhd/lfgkBkVObvf0BREpnoKlVkLcJVdSLNaQtRnQDAclLOOhqhLu0b9RuF7ZKz5tqAN2gDghtIDLarzLJ3Lr1+zlEEvBYBa7NGhLalv9+L3lfFSg9W4fkg2Kw4iG8rwsAeGblHdxXIdjshd49eCtfBwCoxAABdbh7roHeh2mA/uYHnsOST4v39+88gf2YNv9nVi7io/W3AQD/8KnP4VeqPwYA2I/bWNjtunbsPEGLqfXxLfzKiT8BQFD7L139CQDAjW+cRDpLE/mzH3kWPzfz3MR99IRB6NGJn2ivvBBVhppHC6iXRXi7Q4rerWvzaFxmGHXX4rdD6vuv3/NJtB6ijfqL576In6rT50VVgw9SMDSAIV8K2gKXeKX/B5//O1j/A2qDsIDkRdka5Zjhw1kO9ungBuj0FZNdTEvNHm6kBE/DAGGXXQUHKZRzwWjA8LuVJFcAi2AoVjCUCgDW98oLBABCPsBrgYNxrS9hHKSrobhPMs1d/zxfuQNHZgJCFwe4wHFqZutcAcV3MwHtlAUJOXb6h7x7pTAo7vWG1E7JMlagG0Tu+XpI47NU7aGT0KF9OIqQs1tOSYtRVrooCiUqUplToDwZuv83VjhlCgByczxAuVCIrBXODWe1ROHaC7wcrWDEv2uQsgEwSAIYzWOSS3dx+0GOmSo9H3kZOh71cTv2YYesoAGwslx3xuMvS5TrUUlYdgW5/wcpC2JyHRFNGWM5pHMibRtoVsbV4Qh6ji+UNYkTFdpPPRNhPSB3noLFHb64l9DHIStWW7qFS8kSAOC5g3twaW8BANDdq7k1o+oZTs5T+MC/vfoqfrr5Kr3fGmx1yHib6VqImNaDHcYQEc+rtRCN+mQdFLbct99j+woLp0AJbdylMf6btlZBNktnaDoTIGnR3OYVAV4KUKmHsMvu+k4F/iEphfJwTIHSFiIbM6LM2Kb7NyxaryGQ2fL8GPCFtp/XMVQ0t1WZoc1LXyFD11K/MiuR8YLRVmDAnRlaDz4rFw2R4UvD+wEAX90/59zmS5UePlC7Sc+bEK/1SdG4MlzA2YjuqvP+HoaW1ulmXkFV0n59Koyxkfcn7mPSlhieovNwaX0fDT4nDuMIu2YWAOAPFaLtGvVxkMIW3ioJBAF9dyU6xKLXo+dFPv4TOBMQQJGhDAHIrIeE9/RfdWvIa4ViIqHYKE3bnlPu4vUE509T3293m+iFzYn7CAD3z9yhtrR28TVBiqrxQsy9Qf8f3hwevRc41CZdqmH7MZrTwT0awcLQPZOz4SduVjD3Cu2b2UFCbmYAqJZ3iNAG1udzCz5U4cLWFpLd31tJCycDek9NJmjI72/MTF14U5nKVKYylalMZSrHlPcVgTKBQrZGbrXuPcA9bbLSVoJDbKb09y9duw/zL5O1Er1zB6ZF2uDBBYlP3UeutCX/EL+58SEAQPa7i6jukhX+az/6MZz+CFmQn66+i79z7q8AAP/owR/G7OusLQuBgw+Q9fGLa8+jxpr688kpXL5M8Po9X8/QO0HW1hfXL+Cn289P3MeGH8OwX0JL7YIYu1mEd3vUx81Xl7H6DWrz/Rf3XFCcSEq3zVJ/AMzT8//Tx34W//znblAfz30OK4rRCxMf+e02w8ZeTyL69jX3dxERCmLzvLR0pYQdjX1fT+biqngZgoCeTQFINhjkKIfssmWQ5Q45LAJSAUAME2cxW98D2MKwSsAWLhsRwjIapSues6yNJ1ywoRUlqmaNIfgNAEwZdK7GAtZtJpGlky913fUR7bMVXhWwbbKwW5UYcxHB5bmVOEipb8M8cO6zhbCPizuEUPT2amjM0fONKMHegKz8/UEVKSNlWerBZwtytjbEiVrpFthP6PnDvIIaB5eLMf+or7QLKj8u+qTHnhfCEpIEAJkEeMoCpdHyGWkQFh1ed9YK5BlbskY412S7NsIqt78djHBTEVJ5OKggHlB/jZYQjARYCeiQ0brAgyzWhlJHXHWuy8cEMU57B+j4NIZyLkXWYATllkY6R30ZnDBYqRI6bazEMqMasfWxlbfc50sJnQ0v9dbwxh593r06i+YlGoeTNzUkR5qn9Qh7J1YBAL/+4RrmH6T3N1UMj4PyR/MStfV5AIC3N3JrG9Y6NOiuIsbGxI658+xRpM79PTflPrcGtsJIzWwV8SK7hGYlkjYH8tYAHbL7Lwe8AfU1PJCo7jCyfseDtzdwvyXTwjU6th4Fyn2vxBFU8W7yp91HkHBGymLQxcaIxmwrbuB0dR8A8OnWa1iQlGwUCiAAtSG2HnyGVjNbtie2HrIxROxrB2cBAFf253H/PKEk56rb+FDlqnv+VkJr+cZoxrl521LCGA5GN1WAPQNDG6N3F9fPuKQNwOMz5oMLN7Aa0r24mbTxx9t0b2XV8lwUowRhh/acP/DQ26f18mZ7GS/urQEgBLgdEnoyGwzdO+e9ngt58YVGk70iytPOna5DgazBQe1tiWSJ76r12/ibq88CAL7ZuhdfYLfmJBL9Xhtfv5fus7kPbeHHH/42AODLs/ehk9Dfl24d9YTkC4QSbz8eYvQYtfm+5R2cqNIe3UtqWKrQ3srPKHz1xDkez3nMvc7IeS+BPyrvVRcSIkuEC0oADuHXqPI9GokMkSi/+17y/rrwIg+7H6DFV314Hx+dvwKA4g0OOa4gOYzQukKDZTqHGD20Qp/v7+Nvz38NABDA4LfE0wAA2TVofIsyIer3ncGbj9PB9cPVG/i36oQN/saFp9E7Q1CozCwap9h1WH0bVZ6vnbwJjzOmKpc2ITRt1OudGvbMhAcagJ247lwjUljMVmiybx0uYe8vqC/rz8YIr5OiZw97pVIThSXU7XkuU2jh9/eQXToDAPihn/t7+B+f+RwA4GOV2053qEkB39JCz2ZyYJ76K5IUpk7tF8bAcmaMqYbQa/TMaD6A1JPdTpkuM7CEGXcPaKBwSWSZy3q0gVe6TpRAzv7otOVBhwyneoDhC8R4lMUHcIxH4e3RcJdF0Lfu735uIPjAFNrAGxUZfxZu7WvxHYFZ31/8Q+UuHRNYVFhBqAdH4xoKt1qkMuSMqXtSOxcYLJxipYRFyJensUDOLjAIi4yVkXH3nS81qhw/Z6xApDh7xJYxWe53/g0ky5Vrv7ViLF6lfGfFy7ASlQpdj2Mntr06RpwhKpRFjeOeWmGMU1W6yFpqhJGm/lz3ZxArbrNnYXQ51xlvQBMpp1ALayHMd2diCnP8/jYUHaRnV7axs0KXS+2Kh3iGFlltrYsH6hTIV1exy+xKjXIK1J/sP4Rnr9H+k1crqGxTO05u5Khdo0tc7nRozwKwlRCNDTL8bosW/kntBwEAP3HyZTy4SC6Q5x6uIY/ozAu6kbu80pY4lpvyPcWM7RtTxn5B29K1kWZuvPO6j5h9YMmsQMaZjjoErCqCqQQ0G0t5RSCr8YXTDFzcivWky8LTgSyVYFHGrhlffE9343vJs7v3uHUaqhw7A3Jj9YcRbjZIqfGkQbP1IgDgtDeC5AFUsM7lHo5lPkthscUxQReTE7i4TQpxlinMh+R6m/d7GHA8ZU1kLku5m0Z4Y0juvI9Vrrr7oyFHLiPzchY5d94kkjUs6jUyVM5Xt/BARK7D53AWlQatx7wakkEJQLZqzmiceyND9Q79/cZb68gaHBPZE7jFU5dX4ZSgpx695MJWFmSOjZTuufTgEbQ26Qv1d0cYLZPRfXgOqK2SW/Cjc1eQ8jn38eZbuLowP3Ef5790DTNvkbv7arCE7FMECPzQycv44zNPAgBmL9YR3CgzX4cr1Ib+fRlmG6QTvH11BVd3aB+rkcArq7Qon3jwKn74PMVFf8G/AGFo/y0+V2abmmpEcdgAjCchwlLJLfZcVaZYVNTf73Qfv5dMXXhTmcpUpjKVqUxlKseU9xmBkujdS5rwT629hfui2wCA2Pgug0WEmoKQAchaFb1T1MR7F2+hwZCCEhYPz5HV+OePLqG6TciO37d49ZCsg602cD8Hg3189TI+fy+5/LwhcM8MWY1zypbZUzKB8UuUouDB8HyLnindUHeTw7SCtTpZ4TWVYpMz4C6/dAr3/jlptt6720CFswTuPYFknt6fNpWzyIOeQeMKPS+7Q/hvkVVy4X+fxy8d/hwA4H/5iX+GD4bUl2yME+qnn3oef/CrDwMAwjBALSwzo6o+jWEr6GAxImtrKeji2b17Juqfr3SJQOmj7gGb0bttnEBUOasn9DBcIeQinq0gnuVskxUNtHjOlYXkwGKLMmPOWjjXmxl4UH2yfqJt6bLt1DCHNywyxQB/wMHzWpWIhbLHysIL94TLPDEVgzYHRi9EZWCo0T6qHOQ9Gwww0qU148bHs/CYm6kZxmiGZGVmWmFjRLC1PgwcX0svDNGtMYdNOEDggtSts4DHs+fGxZPmWG68OPXL7EJdBoLDM26smkGMJZ/Q2qpM0M2pbVtRE3E6hpYxshZIjYgt70hmjgONfqRYMyX8YGXJF5ZXFPyCDyYpuVtUYqBS5pDKLWQ2OXyxbyL4jE4+PnMDf7hI2b2mFiJt0HvOze3gNAeOmzE3T89EuB6Thf3N6/eg9iyt58WXhvDuMCrXG0AUriltINhtLbIc/lVCmpbCE9iMCOH4s48/gE8vXQQAnHiig2+urdNr4tAFwxojoK9OGERuxBjSBIfQjgfb0x4tEDwDO6K1bIZDyCIBJSoRpbwKuEQkC6ik4OcTUAzAyhyw3G8dCmRNzjQNJLIau74j6eZ2HHEab+ck8ujMTTQ92jcGAtcqxMF05XAe+z1Co17eP4mP1N8BANwfpKgx113PBEj4xwhNoDGeVTFej08BAP7o9kMuq3hptotzFQq2Xvd33Lm/4I9wMqBz9iVxCrdjOtN3dAVn2cWtYB0CNatil/E3iahEuIzVno4cFxkARAHtp1iWGasi02BgFSrRAMpEFdCVivBQI484+HtWOq68YR7gBnNUZarrskhhy/ebQMEUsea2DNQ+1BU0NI15IPQRPsO7iVmadZ/rN4BX9+me/uTyW7CnaAyTuQD+Dh+8WrtkhvmVkguy+XqAuYt03nv9zGUjvogzuPdpIlb70XOv4Xe2yEPVvlJFyNmLNlQwRRB5qKBGRbKEB10tXL3K8YhNIu+rApXVFWyFYwC0jztMVNZSA0cbIJWF4awt2awjbdEiuK+5jYQ3gITFg7VbAIDPn3gQaRHb0LPoxLToe9aHZv/3Q7Wb+NezfEEr4cgKq0LhDuPSmVUwNb64K2FJ7iVMucgmkFYwws1B230usnQWXwC827wQwgCmRQvx3c82Uf8QpTM/Mr+JOzFBy9f2Z9H5Jr1n4eUqqq+zstEdItyny9cXufPl96zAFi/u/2Tu6/jFj3zDtWmL42R8HPUNRHzKbuoqfuN5ItXEx79//1YqXWxG5JcfqHLTWU9C+jxOWQ74NId5zUPvFP3+8IkRHl0j6PbR1k13oElYzPukLGZWOWW6KUcYcLzBW6MVfPkW+bh76Rx0MSUCALslBQBdEPnZsWyvsUNgEgn3LYaceYJQY7FKbVur7GOHyfJ6WYgGu/ROhB30NG2668M5pEm5XgrizfXavnMtbI5aSHrUr+i2h7xG7YyjkjCzplKnNI2077JlpLDw2J03yMrYqzg/3lbOMnUktVx4nNWoLALOIvWEQYvT8U/4B3g3pcvLWOEUXimNc1kHKkfIMYX+2OGaph5EyvEbeXnpj1+sWV0hjFgpTvKS1sJaqJRdMolyiu0k0pApNIPsK8Eh4gV6TzIXQQf0460gPgLZF6nxNZFiPyWlSb1Vw9wbNNf+xg7MISmVQghgtl1+Lsgo4wRmQC6HcGMf803KuH3n9DL+/VMUhvDJ+kX8rTmK0cysxJ/2yOD5nauPwexNpiQKg5L+wYqjSpNzrdsyRtBa58K3ee5IBWViodj6kslY9qotYxzVCM497sUlOa7QQJGCmlcl4hm+lGriKFEuv8cbWRzDu4Wfn/uGiyO9nrXxRJVe+iX/QfzR5iMAgEvDRbw4Twbgo+E2wJlmsfWR8ucA2ik4+zrCF7Yp8+765SVE87RHP7nyNh6K6Hxa9XrosCZZFRYLnN1W9xPkfOjF1kdsR+7zeLzMJlMjrE/Qx+AQ6B7Q+bERz+KRKpF2+jJHhRWokQJUkeGoNUS/GFCFsNgreTj2zhR5jcYqbYTO17QQ9tFgWo2OqbjQGZGJsRiokqJAWCAZ0Xs2hrNjNBI5AjW5AiVGKQQrfUG3hu0uGQnzJ3toNZlE1rRhrlwHAKiVZZdpuFzv4cEWaYa/faGNxk2e050RKps0DvXLLbx2nsJ3PrV4EYtnyRWYzMwh3Cyy8KyL0RtX4rOaBJrsos0jR5R6wj/AKW//+/Zr6sKbylSmMpWpTGUqUzmmvK8IlNAWwRb95FdunENygj5/on3RlUEwmYRMmVfkoAtvSBlNxgqcZMs4EgqXmMRMKAN/UEDRpcmzIBMoQVpuR1dLSyqhjDgAyKxxVklBNAYQYWOhqWap5wJRJxE5Bt3ujuoYvkMW6srlnst6sZGPrY/S3xc/tomnF64DAGebUDtOzxxg6ccoOP5bH1qD96V1en9m8dmfoUyID4Z9bHL21NB6qImSaC22BWmcdMhTJIzjLemZADWP+vWl3gew9BVeCj///ftnxgOXbRmsapWiIHgAsJYC9gCkDYVkjh56dO0GfvkkBTCe9w1e4Cyz1Coss4U3XnKhKktOkLYa4vmQXDB9PWYB5yWqZn11JMNHMmEj9Fj5ggnEiy3G4ZFxgsqA2+RJ48bCF9pZZt0sQp4wWuQbzESERJyK9l15oOuDOQS3aa22rhqM5unvvXrgsuo8qZGyD2SkfcQ5E8CNmU7ja00dg0QTIEJUzYHs1gKGgys9X6POLt+2P0KN0UAF65DYVCuk7FaLIoNZ5lFaDPsOSWzKESpMROh5BqlXRLSWeRI6LAOKhRkLHM9yxwsmYw0V0Gd/KI+FQO0wIgsQyl2g32mjdFneHLSxN0vnxILXRTCWBXSLkeTKHYvwNvPI5bkLPhVRVCZoaA3RJUvapqlz51kl4fc5yWHoYScn9PbBYAsPBmT9902MP+N57e7UsfbmUQ6f7yVibP/JHGNkpdYRDI4nesAAosiOE8Jx7vj9HOFhidxqvwz+LgL4ZVrsC0BlJWKlUgPBzxgloNltlNUAHZUIcFFmRubCzfMkEgmNlxNCFl4crLuSKk0vhiiyAncDPL9GZ8Nfq795BAlqCvY2yNy51V5KlnF9l1xK1RsewhO0Tj9cu+zQGQDuTto3HgaMRqXaQzRGclvsOl/kLgFhaDyHNk8ilV2DhAPBL3cXsNP8bn4lmQGCkVgRp7DBGLyni3kpM5LlMAUYgdIhoBepjyvRobsnruQLuDEkb4YayjJrWQk3735XQvN6vzQzj8WQ9ncUZe4snEiMgWDiVh0KtGp094wj1XlFuhJByDJEB9SgjYMZfGyOSpy1V7pIazPcx4QIYQGEh80yqUdkiFhXSD3h+MisEI4TalysElA+/VbbHx1pk3+XeXxfFajqVoL5V+kE3PFauDVDB9TcXP9InFGRrmkODjD7Ji3KP3rtYeefPh9u4lsDzoq5FcHr0OFmFwJHPlgbS4PezRrwu+wa2TS4sksQ3eAeg5OKYVqZOjeD9WQ50BZHNtXdJB9jH9/oz6BJ8w6133cZMPlcHZ1H6ZnT4Qi3RjQOsfawN6IDOdPKxdz85/d/AeoB3sx5Hc/UiD39jjbYMXRJGCuxykrIjbzMGoyt79qvxhSslkzQ4Qv6N1/5EO7dmKyPt0dN56+XuTiahVfUzQoCaIba80i4g3QmGKHNF3Jd1l17h8ZDWxY1+rRj+L6RN50L7+14BQdDWiPeQMAfFkSa2rkQvnNzHHHRHyMmwcoSzhbSImZ/4bgr1xMGoSrdv0V8UCeuAKzUetUMswEpF3UVYzcj5fj6wQzqpBujdXkIn/uV1RU2V+nwXK0dOmWpm0aOxf4IkaeXH6mNdxwJvBx5kZFipOMN9TztiGArKj2q5+yTAAAgAElEQVTCqFykk4/HYClhXbZg0xuhycRzDTVyLkhfaZfNZZVw82KlPcosPuZqKliyZZpDFQppZkuleAKpidTFM8Q2gAjYfSUVgh791uUry/jmzL0AgE80L2JgaM8NbICtQ5qv2W0D0R9x0yxEhedrbR69NXq/PzSoX+Zx2YpdRp7INYI7pFgFnRm8OSBl4NFoA32mIfl63MLvbZA7auYlD/XXbk7UP/EdrO7uPhunNBjLwhPGQLRofXlR5PardzBCnS+ZrO5BV5ihviqRhwX9QJkxaceSUdXIchwO4MXKxUwJI5wLBhbO1yFM6c6bRHrGxx4Tml4eLGA9oni1k8E+JP/W7JsGtx+nfnVNhDYbhpHI0WJ/YUMKvJ7SM7+/+zjERZrb4ekMP7Z20f1eg5/XVqA61tBdVnx3RjV8coXImqUweD6m+Xwy2nTnqYZwytckEnYMol0aoP1BFR393lnfrragGbvUc+1i2TCUkB12h8UpxAyv05pApUFrbcnvYktTX14enMbb+xRiEnSFo+GAKNeP3wdkxm2rttFbKOODCiLhSUTEqauZOFgReLxFLrbdvIH+kN5ZMRZqlpQj26yjvkFn5+Gzbfx6QjUk870KlrideqYKj+9UFcO5FKsycfGdeSRgQ1ZzVKlMwSuVqTwSqFRpUa8EHVdFAMCR2ovvJVMX3lSmMpWpTGUqU5nKMeV9RaD82x20OoQo9NbmjriDCuteBRrpLGmkkechfP4SAGBl4QH8A3wSAFBpJEhukFVy4lkNtUXarH6ggRG7Oq5mEbY0ad3P7a0j2iPNs/12H4evUBbFb59/GL/QJuvjkcoG/EWyXAZrdeQVLsvgjdA7TlS+1GhwkPpbySKWb7EVk+VAziVe5kMEjRLxKct0GCxxwPIgK30VbTXk4EjAB9BhBGAzbzgLuylj9HgMfaGxz1ZbJDKXXRTbEq3QEPg/DigzsfXNCP7+7kT9K8YXAIyyJQEg4HwzNgpcZWzjCff3wyzCdebW8cUhrnKwnobAnmFLAtZB8M8NziJhHo43uivoHpBl1j4E/EOuzt0djlWSD5x7QObjGYIC5hhITdoqXUt2VKKCt4OmI56reQn6XPbhdtrC1T5lbO31qwAHZC+1ezgZUuJAYnxsJtT3freCFYanva0OauyGTGsN7CySZXypsoD5CllCnizr3OVGvmcWni8NsmNk4VkrnJNSCAvJ7/dkiaxVVermQgrj6sp5SsPjPvqedm0LZe6y8BSsg8I9ZdyYWK9E99R4+aAxlMIqWZb2EeIIn9AxEn+OuFEOdcW5cb3EoMLku9lLAf5skQgBH7xwC6scGrCX1zE8JIt5+XDM72isI33tnKtg7xEOrO4rADS/jf2SOwu5htihQNRwbxabIyZGtB42NZ03v7X9YQxfpPWz9u0+7MHY9ycVizILT+NIe8UYx5sd45wTCWevdnqQXH/Sq0bI2+wNWAiRVWnfZDVRzs8QLhB8fP+Tu7Bsg0wLtx1c1pjfsw49nkQG1sfbQ8piHOYBTvhF1rHn0Legm2NnSP3q6KqrUdcQmfNEHBqLl2PiD3r22pki8RV+K8FaSPdHVSYlciRK8s3YKhfqYaxwd1UkMlfqI7bCoRWzKsbmWEjI3SQ8SBHO0jgfDEOXXBOJHC3O3D2QYwhUrktCVCEghuxhGCWwB0SYiSiCrtLZGc9b3DtDf1/1D9DjwPFXD09g/xZ7gXYsog6/08DxAkYHxrl005bCYcqIrikTWyYRG/pIGF0fncqxXqUxf2ewBH2TzldvqGFnaQ/pRghvi9p88gsanRv0XWGB2k3qrxUCmt3vaUOgyRmR+7qOwxEjwxolOacQR0sNsegQmOVM62XvEG1Zloq5G5/X+1tMeCy2QWi4NO2BCV22jOdr6KiIMQhh2U8/860tNK/QorRKQiSc4n99E4YVk6wm0PBK2LVI17x6ex4nNzkt+vY+Zt6hQf/nlz6Exx++DgBY9w7xxEnKwHjuY+ddRt5nz76NU15n4i6mxoO2XITyIEKwz24zJWGHNBmDJYWVGXI7DvIA1w6Y0DL2oZlU0fR9XGnRoSphgbmXAABr3gFSHqs9XXebjRQoWjQDG7hF4Avt0mJTo5z/W8Hi9958FABw5uUBxEFZK/D7SSuI0eGCob2KRcaKpg39ktlVSrdoVWbh96i9b9xZxm95lO0XyBzvHFJ2krXCxRVEKneuy14eutifawez8O6wu3XXwN/nRZ5mpesw00dcF66QlylrvU0ieRXlZZQL9GKmYaj7R2DrnA+Q7biB2z3a4PEogGR/+lrjwCkUt9MWdmJad3bouRguJCnUbboUmjUf/VM0tnsrNUfcORsO0eDDYaM/6+rfZVodYSY3xyDWHMbBkUy6QoSwjn4gHHNhjKf4S2FL6gKl3fMUC/bd/hkhbKmQSotCdbPj+p4YY6+W0hHeASiLAMuyNttEfTQhFLdtqEOAXfR+V7uYprm0jqsX6NC+eWYW6z4ZErezGRdjI3T2nrUae6cF1h+ibOCdfg39TTpvGlHolHob+E5R9fvWjeOq18M+Z25++/YJNK9wHEs3LuNA7iLGJxczwPF+hRs2sS5eSWrrxsx6snTnJVlJfBuXxpxIUigu2i1nAhfTlFdLg0SNaF/TFwTSJu3RpCkdHYLQANsO8AcWIVdAr26lUP0ylOBuElsfdxLO+s0C7OW0h4YmdJegirWL5+uZijsH216KYvR3dIQXDtcBAMFrVfDQ4+zSLu4LiHJiPFOPvlPG0BWijcR+Sn+PhMYcu3vGlSwF60hYJxGRaRdTZox0hkdDjUoy3WBsf3jKZTkfqTeYa4gah3TMNF01DXsqxtNz1wEAp/w9XEyIQqCbRIg26T31zQx+l/aurqixcxTwh2wkDKULZxia0NUNnESG981j+wn67vq9t1w4wNc3zqD1DivCB6lT/AsaEwAQ/REa73LYSJxB9ni9CoH4FI1z74xxCtSXdy/g8Aq5Ak90xyyu3FCoCQAY41x4RgHzFbpzqmMhC3NqgMZd/M1TF95UpjKVqUxlKlOZyjHl/UWgxuufSTjUwUA6JOWIhCGEKHBjC3mVrD3h+w51MHkOsUqZeqMlgbUamT2RyHExJk1b3YhQvUEoks1z1G+Qpjr4yzZ+tf4jAID/6swf4b9Y/TwA4PqPPI8umygfqVx3rrFJJMk95NxHOVIOdrWDkQsszRoCWx1C08TrDbSu0DMzHQ1vyEjZYOQ05EsLD+CXHyCemOyRPn7u/hcAAE/XrjiLXwrj0KVuHrmg/PGMFF/kWODMqN/tPozm1zgoe29nYqt3vb7n3HidVgNpk/me6j78pGCfLMn7VGKdJdq/3sBfHJ6nfg8Uoi228A2cRagji7xJY7CwduBKnnR3a6jvc0mHTlZWgLfWrSnIMitPpcoR/x3H7QMAgzXtMrZgBAZ9atytqIXFObJUVsMOtjgo9Wp/Hrtcswq5wP3naJ2uV/dcDa2N4SxevkouhMYlD96AGzeWTWOFACexYXCtjnfZxJ5dGeI2E7IWJV0AqqxeoLgGAv4xMvGy2HNImZTCueSkKF1fkcycVU2V28eC6BkFq/kp2n4ROB679UbPF0HnYozkUbjUJatKIk0zFtQJIVwQuUgFJJOpytxC6skhqIaMsccoQjePIJk4z+uPAHarBcMY4Q6RKh5kVQy4htl+XoMaclKJFKUbIEthqvRMvJLjU0tUn/OVykm8XATA+p5DrEwUQFSKpItybBvC4qKmOR1t1bF8i3nehvHRwPrvIzoyjlg0t4A3LPmbXKbbeCyzlACTTEJr4oIqhM8mVCLkTXbTtBXSYllXLLxRESAOF3BsfOEID4eLEikDL0IDAa9lFROxIwD4u/2J0W4AuJXNON6lUOW4yVxke1nNER/nNQXF51pifJd4oq2A5gF4N5/FxV26J5rXDbafovc/Obvhklk6JnBuu8xK3MoZxfAOjgSFjxwK4zsXT2al86LsmRA3spI48m4ixuZLjHFORSJz+xsoESgb+G4NwgCyOP9y7c7CbL6K4TLN1/LcIe4Jd/idGjvsXtzp1t3ZHHQySPaQWC9EkQEgdOnOkzlwmJQuvOMgUDefUZi5QOjuh+ev4cV9Ogv9l+poXWNXcl6iQmqnA9sg117erkL1uRZoppHPEwqZNXzs309tqNzbwW5Ce/21t05h5m3OyNtNy1JDFQWRlG5Qy6TFVgk3zpHIEPCFoSGg7rIV33cXnhgr4lhA/99ZsK/wuQopYYuNnWYu+8VWQgiOvpe1KtJlWhCjkxke5LpW27qOf3mZdsncaxayx074KIS3TRu4cSPExi1yk71x4gT+oxalyO6ZEV4fnQQAXAhvH3Ff3E0WKn0M84KRWEAmzIwdBu6ynHkng/88k1u+8WZ5eH3HwSn47/4G0HiBx2Z1Dp/7Gx+nrvxEhk/XX+ev2jG6ggwdrt+XClVmUlnPEbz9X5sPY/F52qgizRzFwt2kKlNXMPZKdQFZnfqU1Tx4Pc48SnOnOPoDg8qO899A32Y33B2L5vWSHiJjRWw06yGepc/7zZKR2dvzEXIcmzcYO/jDoCxErEroGdYeYWk+juvn9PktdDgz7rBTheEssIN+FQnXUDMQ6GT0zI1Om6iCAYiqxpOztI4W/S6+skcK48U7y4gu0xi3rpaZg+mpOUeZYQLpssNwU+CQs2gqJzN3ieTfUQTYiRVHaA3uJjZVjmjUQkLyARJ4+RElraBeSK06koVXLNWKl6HJWU9VmTj3w8CUsRzGyLLG3niTPUu10QDoQLg6apCiVJCNgWTl3hv6zmU8iWRWuVjAzCqXCZjORqhcKvxRCj4n3WhIV/tqN6k7BdxKOLJW3Tl0B7Ko5GhxYdb5sO8IUe0YnYrI9FhsF1UqAIChBeQYsa1zU3Ymj3+ylTI+BUahIMM3flmw10oxVthXl7FRAETIe75eg55nF/RSBd013otLAmmL+5oLN05Cw81bWpMYMOnsaMnAtMq1k/J5oAMJqelzcBBBxZO78F7o34ODmM6yhUrfucQ3hrMIOkW2oEUQctyeTJzbNoNEjxXGV4drOLjNGa65hZ2j99wXbWHI6zqzCkX+VQDjsoEBUqgByij1ZVFMXbn6o/FYzbQd3XBFjycROUyR1WidSmWcgvNk5apz3QsNKDa6RH8IWdBkeLIsRp9mSBfoPXlVYXSC2vajS5ex4NGd1zERbnHx3vggwsx2GZuoa8W9ZVwGJa19+mw8YKVG70msh2E2uQL18FNXcK5BStxKcIhbh6RpV7YtvF7G7c+doWLj2FGByDiH7JDhaqMQB+e5KPvDFpXTtF/WZw5weZ/GvLrho75VVKQwjjVf5GYs3COHTPkZY93ZmUG5M8q36q4uuqkLbypTmcpUpjKVqUzlmPL+IlDGoKDZFwbOqvZF7qyxSpjCStL2bZY5K8mmYyhVNKb5hgEO7+GgweUDFwT2jzc/DvnnpGnPvLIHjGL3fIGO5FHJj5FZhX90cBYA8C+vPIXOPrVh+6EG/tZcWRblbiKFcdCmq2QOkPuSs15qb+64z1YKF8Qpa1VXI8/6HsDZFbYSlkHZ2x3c8zv03X8y+wzkM9SXT9ffwNAW6Ih0MCQAB2nXZIK3UqobuPnSCs6OmKY+zTC8sDhR/zp59YhlVsiRkhEoXWn+YQpvQPNc2VOuJlO0ncDf2HFjIxfJIsmqVUjOmNOHPiQH/ta2BGp3mMSyUyJXNvRhgwLBg0NzVKYwXrmmCLadRH589WX8nzeeAAD0/Ah6RO9PYh9vd2icbo+auLZHMH280QCXvIOcSTDjkR3rC41bfepX8m4ds1wePexkSNpkXfVO+qjsc5tHBkGvaLSEGNI41LwEKxFZWq8ennBcUIHSDpHSFi64/LgyjmSNz6yGwNDS2kmtcvsVKIk7A5U7i1wJ49DazHqOCNRYcfTFJSDpTDgdEFLBDSpdeMZAct094jSavF8plEPEFoMexCztm7QZolJkFdgywL2XRdhmvp+DtOKyyPx+GUQuPA9qh8IB1O01XEuIR2ekg5LrKDdlBfhmdYzfCm6sxsVGGvEsl6NaWXQlVu4mXi2DzgouLwHDQe95BFeKQyXWcTNZISCKEArrQxQ8YI0KRitk1XdPe+ieYVRiPoFi127e82EOC6QXrhRO2hBIuEwWFhIszRGqHSjtMqG6lTqMx2hCXkOlNnlIxJuHyxhm9PyTc3vufH9tcxXLL9M4DRcU6hH9fdU/cEk/DZHjbc70/cbOGVSv03tGcxYnl8h31ZAjdMY4CCMOiWjIzGXzxdbH9eGc69dKyH83vnP9UPYwra9NPYN3maByErGedAkVUlrnLoytj5DJvd4rwsVJsV6EgBpw7byFELZKbVuPdl1S0Y5u4o0O3QHhbR8qG1uP7K8ySjpk0ypAF+EyPtyePk55MwC4+JVz2H6akLWfPPUSfvDUFQDAnz1zP0xAa2/pL4cQHVo/dm6mLDU0TGA5ccM0K841uXB+B8+sUA3ExHh46xZla87esqje4jvCGNgKr71+7LwViAJ3BwsN5zXayto44dPaWJiAQPv9j4HiyZApXIYVAMd4XAmyEuZPM4DddjC6dOEJ4Tqfnl/F3qP0hafn7+DF3joA4LVnz+Ls1zh77tYWwKSd1vcwPEsX352PafzyA18BACx6Xfx3lz5DbfuTGaxfp4X4f3efwA99lojTnpywmwXpofUt8jYdIkFvVGbmeAqQdDGJwEd2ktqz+0gFo8UitgCO/LOybTHzNk1mcGMPYo/6dfZfRfjfvE8AABae6eHJaMO1IeUdl9nSNWcgXQqrjqzzNwshsPfQZBtia9Rwm0gnCjxtkNq6ItAiTiAK12XkuwvE37fQNdoIar8PO6Q+iZkWDJOdZVUJRpihRhJ+l+Mf9i3CvRLCdiIC1w+ryuw/alPxDI7l3noguon5CqW27/k1GI4zsXshbgwZmlcW/h0as/YGMFrkdX0mdy7pg7yGO9ukQLWuSkQcB5K2PPRO0vyMli3SBvW9dse4lHOVWiiOORnpAIscUDKujHhCO7facUX4BoIVfCFtyQ5uhVOUjJXOhRcbHwm7YZQ0Lt7KWIFDdgvHvu9ipgYmcM8foV0YU6SEQVm0VJQp8VZKiPeIA7ICOAY/ITKrXJr5U9Ur+OoK1VLcXTgFlyMVJ/A4y+hO3EDCdTUjlSOrlwzGLjsoDGH2+fLdWMNL+xQ/5Uvt5ksMY3c+maUWVK/M7HG1C61Ckwlul1cPsPMoKebx7PzEMXvzMz0ME84IFEDGv+/XJHI++71EQKZFPItBMRUi8GDZyNG1EHGb1+OiRbhG7pKzCyW1ybX9WcTsdvF7AhxyBB0BukLvqTdinG3Td1aiQ+fCvT4zh7dnqX+7lTqi3clpYba6Ddw7Synvj1U38Ae7jwMA1Gt11K6QAbbz2AI+OEM0LxeCHbQLPlPt4YXhPQCAjVvzWNqgdu49JPCZRbrA51TfGZiB0K7NvoCrjvByfBLv9uhQqgcJ7qtsufYVtfaaInFnTGx9R30yidjAc3UDPe9oJqsrUi4sVXsAKJatMDCA0u0lAcMExsNFifoMK7MidzGx3x6exsYO9aW2Q8W6AQDaOhe6M2RA7PJFGHLWsI59vKHiI4zsd5NTXxphI6AYtL+onMcn5okMOjnn4SvdB+mdN9qovVL408c2gTHOpa8GIyy8TOtnB4v43YcJ6PiR+17DD5yhOf364w8gYgqS2vW+o3mAENCFMiUEBBveeUU4Es7vjMW+W1Tp1IU3lalMZSpTmcpUpnJMeV8RqPH6PcJYGDZHfaFdoHOgNIbhmPWZcYCZ75cw+iiB5RpUux+oYOUCWQQVleFLr1OV7bWva4jbbEEp5TR226hg70FqxzOPvIqfaZDW+noaOutQ9Cyqr1I5hdpDZ/DqiKzMn5qgj0thD7eHZKlZz0L7ZRah5b7AGMcls//wDLY+RX9/5MwVDMYyGwr3zEKlj5dvUkbh4u+sovn1awAA741rWPsTQkr+6ZmPYv3cLo9nXtYWhDmSlVLU9fuZH/or/O7SYwAA+c5pJKuTWRO7ozoG7FJBKsuaW7ktIeBWtUQaBwlEjxEjT8EbFZG5FmgRpJsttXBwgayK/ikgqxdInUV1iwlQ3xnCO+RyGv7YshXCcQYJY6FG1I+gIyAX2aKqGTRrk5fjia3vuKiu+rNIGYEK9yVkXvKYFeSsta0cowVGo6ojDHm8X+udgLdJnxs3Squyu+ahdy+vtcUYfa4bKIxEtM9/N3BZT3fiBu6p0Nx6Y+SQ+Zi1JI8DzQAIayk85nJKUw95Tu9Kc+W4XrbTBlYYzo5kVpau0cohUJ40LivpTtY6gvQVAbCt6gijIUPw2odIi6DOcp8bBWh2O+lGCBEXWTe5CwKlUi6T91Nb6YKOI5FhvU4u683WKYiIURBZuk8GWeAs0IYfI5/nDKtWAP+mdc8Lbk9lz+LyNbKq/VqGsIhAFqJMDDHWoVfxnHBV5VeVxYIihODfW38WfxgRJ9s7txdh8sns2lONDg65np6xAv1hYVFLMCiILJNHSqcULnFhLBRnXUHAIYPGBxoRWftr1QOXICBh8co8J6YMA1dSx6pyHtPUQ8yoY8sb4WRA4/1Q9SburVPJky94F9Ddmxyd6XcqeOwM8fM9FN7Gf71N7qf6DQtTpzkcnczwdIvOxJPKR2Kpwzu6hos9el7uBAgPeP0uWldTry0Tlzjgi9xx5mV2hBoP3L6uo8PuyNloiFP+Hj9foiRSWET879R6SPPJr1YdKheUH3i5c1OOJ1dZCVi/dHEXSVTWmDIxwPcwXKHzZrAqcKrJ2YW66gLTXzxYQ36Hxt/v2bKskieRV8bQl4J7cixD0ARUjgsA5r0e2uHkNWK9ly9jtUF31avNdXzkh6jG2TMzb2LnAxT4fuXmGVRu0d0p7+zDFqhvFDrXm0gyVK/QulqJW9jtEwL13Mw6/u6ZP6cfewp4jlGtE8MqouucamgM1GAsTECUHGdFya0Fr+fGX8HeFWF6f11441C4LRWEoQldowOp0S/0LCnKQQwCpwQBRMwFAJ3HUnx4jjLvvnn7NOa/QV+uXd0npav4XU7ZzZsR+mfo88/MP4eWZPeDVU6B0hZOwTE+sJV8d3HH7yWdrFpmTlRzGCalgzFwWKiUuPFZ8qnHjw9xZpEWhCe1I0PMtMKIff8r1S5+5VGiWPhf6x+HvkGHtnprA7W36WJ9++Iiuvd+NzTuj8HSGtIpqk/UrqP1AdoAX148j53Bd5PGfS9RRTyRsu7y0ZGELRSZvCxqKUaJG/txWJYKrnKsmCcd83va1rABZ9t1ypplapi6WoJi7FIyYoxcUQiX3WbGYGihxXuyd38vacgYSyFlm7QrMfo+zb/fB6rbBU2CdRl/xhcYnaQ+Pjy3iTsZPf/CzVNokn6O6s0BBqeZBHDFonWGNvWTy+/iGxHVdcy3WgDvdStLtuduEjlG9sjLkOqiLh5B7IUULrNJZJw801o4pvY49dHlVOVuVHHQvxk7SqSwLm5KwkLxZ19oBHzp6LF4qe9yxjlix6NM9oU7z6ox2gApnWIOgWPFQGVQ6LEm0VYDtH06JONFg+zMMv+WRDzH6d5e5vZKXSVYWiVXee/UAipX2eV+KFw2bf36APPf4KynWoD6Le3eWWTVqV5cXvRLFg9Xb/BYSaf0fqb2DtZO0aX8J42HsRVPdt6cru5j16PfH+U+hjVqY15XyIrivRqOsdT4Al5B5TAq2dXlKEfQ55jFrkKPWb1H2sf5KhmnsmWxs0pnxGY6D5mwOzcBwj128/pVXAooJmwmGGHFp/F7KLqBCyGd0VJYXJufm6h/APDE2Q18uvEaAOBiuoTeLRqbxasxbn+UlIJT67cdxcYdneIen8bkUrKMa10Kj6hsCwwXqc1Pnn/HKSdXsjmXtXfO62GLaS9eS5edK62vI6zPcFxM1HdKVluOxmg+JLZYSXnu8AwO+5O7KWVu3Nqv+iWbvy/0kRgol01p7ZFQhYKCxlQDxOy/TE6kON8kt2ZVprgSkwv13U4blS2OQ+2UmrVVwilx9Ad+dWKde1FYuPZEMkNNTZ5NKZRE9VkuCHz6Al55jECJBxZv4a/PExXIa+dOIl5mklJtIXtcnDtOIAr3pe85l1y4obHI1R2uri2hc5q++5PzL+D6U7TGdrorOLFDcyFvbEH02J1dicrMcwG0vUKB6mKZyVFbE9CJTF14U5nKVKYylalMZSrHlPcVgRLj2SXjCWoiR8BkE77SjlwPnucyRaCkc+fpU4u48ySpxWfWb+JSl6ye5Nk5rL1IloI47MMWJWG057Lw0pkA9WVyzyx7PdxkdKSjV122RzQq+SKsKkt2TCK5lQ7J8sIcOQf12TCAYAQtPtnG8FFCf84s7TkkLtY+MkYC9gdVV55gM2jhco1Qp59efxG/+dc+DQA4fasOu8sBrdeXscMZRA1VupF8oY+4VYpK35lV6DF7pRAWgTdZ5GrFy8osPM84AkwrhUMKLCRkgTpluQsWpwaNLTmuyxUvBGC+SVjPAiGTmu17iPaY76s3KjMX89ytC1GNXN09AK4yvNDWVaeXsUCWTz6HD/gDbFbIhbszU8etdrvoPUKuF+WNNAYrtL6GSwon1slSf7B2C394m9wx8pUG5l7lkkM3d6DPFi7LDB9dvQoA+PGZF12JlzcrLeTsvrZemXmzN6g6FDQ35fpKteeCOlPjOUt6Ehmf7wSA4WyuRPgYMo9RLw/R5wlWMEcQriL7aDYYYjUkpGHJP3R8M9pKzPu0z5phjN2ArMPEV7AFOpWIIyUjjpbhKTLXys9Cl6jfJLKTN122UGx9VNlitrOp45KxHhAvUV/qfsljdSraxyPztD6/cu8C5r9NSJbcgkNU1dVNLHQ4OaUSEI8NAHS6zoUntMHoXgralacGrtL7ptaOoLctNWYVjdVi0LDYb0IAACAASURBVHP8YneT9WgXdSar6mUh9mvUp2EtgIqLbOcSobUKELZwh0pYRsetJ0vi2xQYDmhd7yU1yCYN+ErQwYU2IRr7czXke7RmVSIcYaZVCl2P1vgr/qpz/90bbDuyykdr77p1MYn85OKLLjPuj/cfwcJz7OYNJfTHKBvuP1z/S6z7FFA+tAovJzQmmfUw4CD7uYsZ9h6g8W54iQvgH9gAsaFntBXlPSRydBiNOsiqbp+1/aFDr3xhoHg8B9bHDc74uzVs4czi3sR9tEK4ve5L494fW98Re8KihHI9VWbeeSFMndapiXyH5C+tdvDRJmWotdXQIVC9gyoWNmmuK3dipG3OclfC3bsqta58CwDk4rtxFl9oVNTkQeQAYIZcXqwP3B4yqg+N84xOnljdRzxH7Yy2hONeg9awAw4DyfOy75UI3g61rXm1iucOCcn/4dlX8YllSvz6jfvmYb/Ka2Z3D2ppLNuc3y9ToK9pHDKr3JgrWEfE+r3k/XXhjZO4mTI7R1uJVLBbQpRuIYp7GmNZZRmeqMA8QJvwRK2Dr71MvtXzX+xC3LxDD0VRWXzRlKeuDgTm65xmDoOkyD6CgOXNYLwyjVolVAR3UrnWnXOxIr6vkXHavkhSR1cQz3kIQloQSe6hyQUji00KAFpLaGZdPhxF2EnpwLon3MGQCdLgKYg6E7z1LC4OKc7g6cYVdxEoGJdeeydvucNoqEO3aJpB7Bhm7yYz0bB81orS7eLBMSiPFy8FUBKEmhJ6FvUashW6fEZzElljbG10WZHdsQgOGSbOclg+GJFmZRxUrkvWZQCmyMgTpQIitECeTa5AzasanozI1aIh8foCxVF0qhWouNh0xsHl3QsZPsJu5MO8istXyT20/mIGPE9Ep9papPV7AQBLKx083SAF6rTXdez5r1UsFTIGkFcAHXLsTC/C2x1SoJdrXef6SVHWqDNWOMVqEgn9HEnGY2iFK7RrtXDs76lWGPLlEsrMKeKRlyPkmpNzQR/nI4rrmVN9LLCCkFmJ/YDWbDtYQRBw/EnowRRs4mN0GELjeytHonThHYcQVVvp6qLt53XUFe2zcye2cemDFFNolcXyabrsTlUOHDHmnNd3e+jLa+dxeI76MtOZc1mgdjiCvUWKs1AKWKQL1BFUArD1Cvbvp3H+zNk3cYFr7UnAsaTH1nNuIaBMFb+brAc77rLdiRq4XeWacfUIWTLGFTGWpSVZUZaJBVocGxlKd05ZCYCVrzvDOnYzUohOBvu4p0ptv9RcwC2uuRZ0BFSf6TkOBCCo7TtiBl8Drfc5f4Cna+S+UTCO5mMSWfC6+Gd7HwUA/MVXH8Y9l2nsbz5Tw398/ksAgCejd50yGlvl2ORPBXvoczbWib+4CP34IwCApbCLNmdnplqhiI4cWs8xiw9s5uKGNoazjlx23u+7+FI15r4e6BC3MzrPbhy0cf/inYn7aFWpQClpnPE7bhBZiSOGoqu+YEpjP6t7GK5Qez6xdBVP8xn2UrKK7YQrX/Q9hF2Os0w0JBfXtao0ZmRqXXaelUBW4zMmMJgLCvCh41zik4ju9qFmuFBwUMZyFkS3AJ1t7/p0zsndw7JKQ7UNwe48EycQVY6ha9YdXUjYsY5i5slmA6dDWquNhT6yNq9Vz4OociZ/FDgDQtgynnRgQmg+wwZWT7PwpjKVqUxlKlOZylT+35b3nUjT1jgo1YOrqbajm6hZ0iRTrVxml40Tp4Xa/gDiBFn2O494eOQE1Rt752AR899i9Oqdd52GCU+5rL9suQHvgDmHLFyZjv+nvS+LkezIrjsRb83Myqysvar3neSwuYyG5Gg0q0aCZ4wRbGtk2JAhWIZg+8uAYf8IBgz9G5D9JfjDgPRjWbBlWJYta+SxqIWaIamZ4cxw6ebSJJvd7O7q2iv3fFtE+OPGu+8Vh2RX+oM/jvPTxWJmVsZ78SJu3HPPuQCwaKmgjkzQieksMmpI7sMDAQyz42egCi1R2FN7tzXFYN06zmjDaX2pwNklKQxH43Wqba6RcqFu6CmmeSat6EhVrratH4LJKU6pLnkjLv6llhq24BOGWw54QnNBYKElRsnxWrnMB1PkNkOBQtRap1QFvkIpLviGUkBUel6FfHLSrQayBbpO6byAbpZyvqqnV+NAHfHQKVvbmCxnY1WZF0dOUUfagZRJSw2oYyqbAGCiMyzbzGdPNTGYlio5IO/Q7yfNAKNzNOC104doWEOsl3pnEG3SvIv2hzA2/e2fXGevqMfmD7BkKRsN4KRtSCXPjTGyXmf+RMIf0uvVboidJt3/E60++zT5UnG2IvwQg8aPQ+gpZJbW1EYwpW5MlYmtq/zqKfvYz9G1qpWT4SHO2axKV1bXra9zdG02p+VnR00+bbbrKG2Hqs2TASsrIauspfbEkaLzB6HrTbBnswip8bEo6Zr//NobWGlUNNLlFlFT56MdrFqqqSlTNAXNvafP3sZLj1OWO+oto/k+0ZQijiqD3mnCmVDTarBiePTQAiaP09rzhc4NlHW6uyrk0/dQN7gY+cZ4FT++e+pY4zvp9Tkbcj/qYqlBVOFecw5ZWno/Ceo/SP/Fj6UOPBSl91pbwCaakHU1EFf3qsxST3TEmc+Gn0NHVnATS6i0ynCVij85keiPaC7fThZxIaLsWCzzI+q1B+GV6Vl86wYpqlZ+bHDwCK3LT33jGk5YhehQB7yuKUjOyvVUE/49S2+Nx8g69P3PRPtoy5ImS45k/0rl3URkTF/fH3eYvlwL+vz5sVB83yYmwl5O97MZ5ViNj09TAqiMNIXhfol1M9ojkDW/O89jDz0dSBSnaM5+sXMDa56lYtUc7o4pOxb0JLySavarNVEUlUmtVKaW1RdcCgEJ7ql3whtiLTh+T0MYXYmGPKAV0Pcc1ExMj/T9azXQf4KEYtNlic4t+hKtV+9VjJLSFbUnwOKag2IOy7ap6FJrgsmK7SN67jTtwyDvrVL0pH3w+t2RCaspgSPe0B+KTzaAqtFwQoMVZ7HIuQagG01xv1WmnzUtUqDNs/8YXVDx2IDfu/fGMi68WUslWuVdcmkVO5+h944u52jeJDVGa9NgMKSbtq3m0Ja2ZgOCpaqhqkwmocF2C8eBMlVT16TwkXXtDWs3gR1aSBtbCYo9+g55d4ReSj8LYVh559XomGnu473D0gn3IsKD0qjTh7QUnorEkYWprP14L13Bn21TP7Zz7QP8VPt9+v+omr3O0utPG4lClenj6uESCkDZIyzJj9K19p7ohTmm93QzYMM2HYD7ssFUi7CX6GrDbDdrnLiGyKtmwlx7EngwsR2TVwUFXipQzBBAbasMr2WUDv7td34G6ke0+LS2DSYrti/eIwaXn6Rr+cTCPdyxzsNvbK9THReA/cfm4F8m+9XRKYnpI1bWH0zZ6HRfRzgbUgDyS1dexp+3rwAADl5ZQXO7VKxKwAZrUhgeF1F2Bf/en6VAqAajBaDKTVBwjZuEQcTOzFOuaelGEdYiWqDOBPtYs7VFbelhTthaAjE90ui6DMpMrZmw0LUNV1U/M/UOMp2tB00zTFUseSP0fFsXpCPecNf9Pi6EtBH0VJNVT8oI3hw7IkXXp/v1pYUbePUhoscPduch7UYZ39OAPRAaT0CXjvjGYGoVa9tPe3jiLEnsh6qB120zXAXBc2CgG7xZ3+wvQ98+niK2LXMkhta+BX/MQW0c5cis7FvFktWcQgmkC3QtE08gWbFmqMsZd2RYiPJyj0EnTFmVeFC0OJhKlU+BLYC8Vam0YAS0pZ2Nb7iGs583uD7zRHCIWB5fvfXt7U/B7FiLk5MSxWdp3v3Kyovskt8UBZp28myqNjo28P0/+4+iS2VA0F94EvFFWusX/RFKp5xYFEfsAsrfS6HZIHYwjdFaps9sy8oORaF6VnqqiYOM5trFhT1caVZmmw/C6HTMtaS58rj2askbcd1h2BesLFabW5CnT5RfFPIlUrHt/Ppn8BvP/CEA4AvxNt6we8nv3XkGmy9RGcLS6wbByCplIw/5nDUOHSqm7XQgkHXqTc7pX28skdlA9ZGwiQP93rHHKMKQ94SiIZgSpZojuxYmLQQT+g6Ty4u4/2V6b/fMAW5vUhC0tnQGiz8kyl2kOTfEVkFVeqAhcDKwzcKlwqQsIRECyGoOsKXTSAgsWDpyxRsjFiVFC8QPUOI5Cs/BwcHBwcHBYUZ84j5Qpali3FtkhUQsM1Y/1OkECEk0HgAszuPgEYpUr6zs4u6QsgJztyT8fcpeifkOTMcWZi4FGD1E0eZD5+/jRkJp8aU3DMQWnVxuZOtYs5mvsY7QCCiaTZqiRiHMZlJYKI8LC5UWyJasudraHMJdOgEHt3fReescAGBwMkanQacaT5hajzGFpGbGVtJ517Y30LI91cR4ypTo8KzApZgKFxMT4E5OGbffee7LOPnn9BnP/dw6nvrqLQBAU2Ro2pNg6BV8WnwQ+nlcKbgiBW2LuUnhUymnmM5QGqLsqxQFUDazp5o+8tZPmvF5E0nFqAD8ieL7oOaCSik0SWBMlaUo++7Rf9h/JSoVXgZghgzU6/kyfnf7cwCAw/sdzFkRofGJbgQAs5ziC8tk8vS3Oi/jP+MZAMD+Ygtbj9IfPjxXUb8L82M8tUzF1s+038OqNVEEwO11bk2W0B/bbKQCn+yLlsFCg56Dej86Xauo9oWeKVOqjWD1p1GCaTUocWQulIWsYY3Ca3kZ5stslJygK0sjQg+epSwDIati27p5kwFTSl4iYEVk1LomsQrKXLNiFajur9CzZaAAsO9ZIAqmTc/5GbqS7sWdooN9S6Vt50vc/ywSCrEd+88038XBJXrN7+qnsRNSNmWhs8h/R/uCTT6LWGDvcRrj5758Hb+6Sr0065mOtswQ2DXvrfQE7md0wt7an0dj53j3cVFKZLb0oetN0PHp52aUYRzS3FORhp5aqtYHbBIJ+bxG8yxlZB5bvc+9FgHgfkLfRUMwzZ9qH+NSqaQlYLOsRVehsP3y4BlI+7MfKLStee1iOOFsZG48OvIfE++8cgr+mK7H3M9t419d+hMAQE83+X7mkEy7tGWCsaXk3txfxdJ79B32Hm/iwhIJN5a80ZHi4DID6cFgYp+piY4wKOga+lJzK6Ulb1TRdtrnv/VeusI9UB/rbOLpxs1jj9FPDGdftREs+gGAUUHXXBaAN7GeUGnKWVo1H8LboMLrfF7jclhlvq6nlKV6f2sRnS36zsFEsdCmaNeyuwKVYW3NrFZ7gtchHVa/V2a2bLe30IWxxsnTDYPlyD6L4S5n3G5uL2Mjo7+x+0SAp54kJd3fX/0Bbl8g9um30r+Bpe/aRSNJITZsC6RlgVNtukd1JuYwaSCwwh+R5TBWyS3HU5ik9C2c52xjbiTKdqRtqdAWHy8++mQDKE8yBykLwwu4h4r3nRRh1cNMVgaY2Xob0/M0+KvzmygMBURvnTK49w2aQFkbnEJWIfC5h2mD+9LCDfybN0l107w3Resu3chr45P4eotyvFeCHaw26QbcCtYgRrRBeNMuGjP0/Mlqcvl2lOHQ1hMUTQ+BlXHCxIh69D37uQcd08QdTGNWUqW5z+nhVpQhtqqnW7dWsfE6fTd1cAixZJVsZ3JeoMc6wg965wAAG38FtL9LD/OGfwF/8DC5j//a6edxD0Q7xV6Oje7x+OxJEWKpSSqFQTOGshSr9qv6FOF7VQzcalTKu1xBdWiRmS4HSBbsg9wyEJZCauwIdG9amfgkqwwzo2qqymYMUfYVDIMjPdRKJaD2awovDaA4fnBxbXoay9ZWOl5MMDlJ9zRdEig6dD+vnNrGz89dBwA8Hsa4tPISAOCN7kt4YUI91340PIPtKc21bjjF5SbVUZwMDrlRZSCAW7WmlaVa0C8qZY4KDRoB3duiZggqhTlSN6BnkKgd6Q145NBSlVdoCDbElB+gB9UDIhnv44K5DzmPCF0t3ELrynRVaYjc2pEYM1MAta/mjjTSbpXycwgEKHvSVVTFXj6HA2vC2PUm8OxGtu4pVpH1LzbwXJOajt+93MURWFqr2Unw6TXayP75+rO4FJSKWIEDbWlNALlV+ygI9HKif9TYRzA63oEtEj73EG3JFJENBCJPwbOHHOV7QLkJCCBvWyPY9Qme2SAK+muLr3HgONQxXvJJDj4oYj5kTXSIgVUj58qDCGxg3U6x3rV1Y0HG81EKjTlb53I23ud6uMQER+peHgSZC2Sn6Dv8swt/ia82iJp5drrMr+npGJ69n22Z4Va+zN/T71n7mvkGluwz3RIZEvusZLWaKQXBRpo7qo2hHe/y3Jhd1dsy4Xqrnm5gq6BgczPpop/RuDbC3rHHV8Kwcb3g2jiNCXpJdaAqLVoQhFzfmy43oE7Td/ZOTXhdyY3B9amtpTsMEVhDYn+q+fkrGgL+1O7HqUKgqp+FjZqKhoQuD7djyarM+2qCXbV+/PEpjYm18/AujnC1RTXMXTnBzdQq7242ENn7lSxLnktvJRvVB/mGx2529yEW5+33BC7MEbW3ERzi9YT2+93NLi7sV82WRavsravZVknm4Oevpxs8H054KRri44N9R+E5ODg4ODg4OMyIT5zCM02bBm58uG/NXJDC1mhCNBrcP250IsSibXlyJb6PS+tEV73ztbtc3HgyOmRL/0Ao/EL7VQDkDVIeiOU4xeIb9Po/fvMqPtUk/54n4/exZL2Z3lwUyK16Lp8D1hrHVxsstqqC9sgrcOYERcU7T57AuRdsNOt5mNukU9Xu/RaUTXX7UjNVlyuPqRQvNqwwmHsrQGi9rkwjxugi0QknT+/V+jh5nGHI5iSEVb51X7iDzbVzAID/+M2fxsPz9DmZ9o/d6sQXGtPyGgcKabMsJpVcRCuntSLyvOBsgmjGyNvWb+OExHTNvrerEBza8d3TaN2yCpZCszmnlxRH/LzYI0TWipKFhmS6x+NshZH4kH4iH40Ff4yoSfPuWmcDm7YFhNJA3KDfX+nscKp4T1WFpYlp4KCwBcTTNrdFib0cTctXrXhDnPJL4zbFVFeifBRWPRVNwafGYCQwSun1a80hZ5qkMFV/Ou0hnEHdFHiq6v3oaW4TIbyKRqY2LZUiqBQdDIsIhwWd2PZ1CxNDc1waBWXn3cQoDKwH06gI2SDWZB5TeCo23D6C6CWrLgw89vkSnoGx88oIMZORZiwznJZVdq/MHAAKKx79fM4c4jVLdcx5KWeseqqJFUv/bSmfi6CvxFvYOEsZhuBc1SZJG8ntqBb9Ec74tFateRkCWMNVk2PR0p0SEps269qUGVNlUTdButg+1viaMkRsql5/5Wd4tXXkCAS4TdJcM+V2RUveCC2bbUsQ8Lz2pca8bXERKIU9SQtz6Cn41tdruTPGZ1du2XGPj2Qmy7mz7A95bXovXcF+Vnn/PAjLj+/gV85+HwDwWHQPPZvBeyzcwi2b/VmRE2wqumZtZHh1cgYAKUpL4YkOwRlgam9V+qdJ9n6a6IB9w3qqxfTZQjRhg9i2zLnf39iE2Mwpq7KfNtmI+e3pGl7skwfWF889eIx5S7Dnmy8172FDNHgdVxGVPQBAeGINyUka+/B0gMkazaNPn7pLex0oA3WQWTGCNCgaNtvfkAgHdp5khssuhCHqHABkUsArBT6+4Gy+LIBrQ3pWnov28E5CmaNvPniIEM0Yu0/SuL5w5nWmX6+np/Bb16hafOOFAsGu9QgTEd4f0rV9/s1LiO7Se1feAXTbqqKVYiVdsqJxoUHCkCV/hP+9/xgAINj1IbNaD9XSgDnNKkV4ADSsECYWeY0d8DE19PsqT30Un2wz4WnCabN6L7yBjiHtLhjKonIiDwPAmk9OVyTOdmgSh0Jhxad6oiejOzzJPBgMLb+emIBrl+4VXSAqKR8fzbfoQZr/7kn8zsLPAAD+5aUJnu6QquC1z2/gnQuUnl8/sY3PlQ3NjoGvrL7Ni04dv/VoF+YspSLF3W1E18jkbPHcRagLNEGfWr+DlbCqjSk3rHtJFz/4HqmzrjzbQ3GLUu/+xjruf44mwT899Qov5gDw012i7b7/6ctYeZ4eJH37Lk78d/r//a3T+J8/Syne1fP7TBE9CL5UOLQ2EHnu8YNfNESlXCwUG5zpw15lhrq+hGyOXjNdMSg26DVSGgS3aayN3QxiamsAfA/GxgRylFQqPCHYZR7aQNiF2kBynYBo+qxs0ZEBguPvvE/Gt/Hs8CoActGezlfGfyfbRHVcbd3DCVafRbhpA/2erlRdJ5t9NH16zXyQoG+DjjezDSgQxRNC4B2bwr477MLbp0e1uW3QvmOds2WIPbtgPrlyjxtOawhW6TS8nOfLcVAGNACOBpc1V/rQK2q1ASuYKvq7UhimN16ZnOWgY9Ubcvp7X3dwO7P9KrMm0rRUqIGpLqEqqwkjKiNWCMFzxoSS55VUGjPEiHgi3EM5yt4HDgilw3BTKG6wfSrcZ0osMQEOdCkDbzHt1PXGOGcVfJeDPi+sEwPevCQMWmUtoxAILIUWQyMS9I6+TjDRbfuagp/7qxv38cPx8WuEJP+reR08Qs/q6hqLHKgX/5TrRWICjO2DVqc0Y5kzhaeN5M/1pUYY0utPzPXxTOsmX5vSMiUxASuBAXCd2fvTRWyO5489vn99+Y9x2TbvPVAxr+9tmeGcpR3HxucAcKIDXn+VkiiWqmuZ8nfzuWaqHkwBwK2Mulq82LvIvzvbPEBX0mcOdcBj8YTmbg7DPOYyi+9sXUQrPL7ScLwukS/Se9dbAzYajUXGJst3Fw2Gp+k5i+fWMTpBYxmcB/wrtC9+dfFNvj5jHWFoA0BIQJe1b00JUaqvA4GsbWtMQ8kUehBKZB16TdaWXBtlPOCdHj3TP4rP4t3RyrHHePuXT6P1eXpunu68x3Pvf+08jvhFup6tN+7xwTvelZX1iTBY/aHtFvDdd9iwFudPY+8pCrKWLu7jqjUOPVBzuLZL9GJzS7DyWxQKumNV/VnOVKAOyewVsJ077Hcug3UA+KjulI7Cc3BwcHBwcHCYEZ9sL7wggO7aSvwlifVOlW3J7Vnx4dY2nn+Iov/BE2to3aLsyXTd4DNdyrycCA75pCuF4ZPfUMd8Gu56E9wsSCWzVXTh2f5qyVoT8bsUqa5+r4278xRF/zv8PH71/F8DAH7t/As4tIV5C/4Y1ycnjz3GU+EBK3+aMuUU74mlPvafpKh4+fZ9zqasvNTDuxcpop5+ZYdNFSc6xI/7lIp+6cUruPT7Ngvy9m3u59P74jlcfIauydlwj1sA9FSTvZ2+/MzreO3LlE1Z+6MRzND2J3v2TXReo7EPHlvC/hl7Vv+5jx9fpn3OUExlddLVvoBqWE8Rv57dEJBdOnFO1luYLttU8pyCtEoelXgok3alEgsAqflKNZYUVfsC+7n0+0o9cgQCVbbCiJmKyP/LwWfxao/u+WavgzShU10Y5djzaV5cG1dzYqRi3JzStbw76eLOgMbbHzZJ4QYgbma4Z1sZ7Lbb2G7M22Fp/NDe54N+C/6Yrk84VAh3bNFrQ+Jw8JNqEAlzJNsQzGCmmX+QsuVWLpL9VAAyJgSoaLw8qQ+yGLv2FD5VAZ/sd4MBz/2eamIno3PbII2hylY6unYfRK141hfQoaXzAglRtuqRtcwUAD89fiZxxYs44zOvM/Q1ZQWGhpNgyCGwJOk6e75hb6atosvZlHFNNRYKhSWbjZiXHvqarvmuaiCza1gscmh7ej0wElt2rVqUBWc+tpVETzf59aWCNhAK/pXjjTE1OXJTKh0rum0hmmDTo2svUomoR9cvOjTcvmK43GCVmTKSvZNafobceqn1VJNpuL5q4N6E5uzBpMHlBaFUTG+d9CrzyF3VwBbo9ZnxeD3KZ+grCgDfaCa4YZPj7xeLXOzelQU2bXZ0X7fYn6klMqbA8sxHsmgZibUCG4GlXoXiTE0Gr9bbTh3J4pettS7GO2jbTFxuJLR9JmJUtHw7SBDajMmFdkWPHgfpkoGco+9AmWT6/h1/yt9BzRcYr9Pvp0sSGW0rKE6keNwKFmKRcQbx7Wwdt/q0/4lUwN5qZB2BIqYxFq2qT6IKDGeio0igsMKmvCXYRwwADvu0/l1vbGCUHc98GQA+94uv4NE5KhxPdYBv9UiIce3lc7jwclUCUfY7Xft+ioPP0vPxjavX8O2dnwIAnOudxWSV7t3Box4aP0XZyW+eeZV9wf5q8BCmN4hBWr2vuKzD+N7R0pIGvb5oGn52cuNhaJ/7FVEg+LC9pYZPNIDSy/M4eMxuLo8U+LsrbwMgBUnJnV+Kt/DIaZoQNx89DwhaqIum5sm9W3S4Ke5Qx1xDslfMMX8/7025FmWiQygr5VWxhChVYe/fx+lvWzXcrSX85le/DgA4cXafpdfDJMLwHboZ//bJ442zXITvpBv8fb609g5+7ys0oRt7lzH3Mk0mbO7hwn+jsb+UfwrPrxJV17jrY/VlmrmXdsaQb92ma5jlyD9F1Nvm1wr8+vrLdowRB4/1PndPtO/g2t8m6vBgfB6Lf040pdo/BG5RINnZ2cd8ZDeJ33zw+MqFQhUSXlKld0tjTCMlN/uVC10UG/S0D0/5XPcklzJWCulpiHBgN4JxVjWdzsET3niyspYQgp2eyw2B/qMy1ZSFqfV2AsQMNgZ/+NwziPbp9c0tg1IzpOIG+i3amL7dWcO34s8AIOf0ObqUiPoG7bGVVANI560z9/kGbjxE98Q/o4+o2hJlzVM9jcLWlCVdiWjVHh4WfeiGreUwgu0K6saZs1gYANS01Njr7/mK+2zJUMH3Kmf8cv7GMudNcJDGGFjn+kke8N/uxw1ubquN4BqMVHlspEkfTP+Y+q2rBUrmA+Z17HBfGPiT4wdQuyqF9T1FagrsWupiogO+/h4M5m3QJ4XmdSXVAXLp8djr1FcpXX8nL3A9PQ0AuFFTCp2J9tnIjwLPapktaz/GkIcAuAAAD+xJREFUJsR+YakLmWLFow333NwuLkQ7xxrfUGcoe77mxud71fIzSBshykwgsM9WfKCRNy1t1w9xb0Lr2la7yyq5WOS1ujeP+7tdH2zg1j6tX8k4hPBsjaX2WAE3LwWrL4c1+iPRAVNdhZFHrDgehJFO0LQT4KR/yEHnrQIc9CkjmcLr1kw61dTjkoFgYcI1ai1RYN/QUz3REau9cgim2bWpzB4fju6zQ/nQBGzJ4AmNFetcf3Fuj//uL3Vf+gnV6sdBNQwftG70Klqs5WVs1wNd2QmoGCgadl00YPuEZw8/hY2Y5tHbwxXs92h+yUywc0TWrgKifK7qBKCDSvWrfcmHmaIBVox7U4F0n577u3EXWh9/zTnb2McbY3pGrh9sYOdlKls4+xc5wrLuqaiaBke7EyQ/ovl2szPA1776IwDAdx66gNSu8Vc37uOxDtUwKyPxR/u0Qf/FjStY/yF9586NASu4TSOEsQaz8D3eO1RDH1GJBpbOjoXEnPz4INFReA4ODg4ODg4OM+KTLSIXVRQNAD/q0entud3LGFqV0XyU4N0tisJbQyA6oHB54XqE/9T4aQDAHyw8gWRErzdKALbDuN+vjrSkKqj+1sY1ikjbr96vLG/CEGKX0rrz797Bwvcp4p1eXmF10EJusDqw/NK/ePAYJzpCYujkHYuCTdc8ofELV0kV+EfJp3HJnjTCd7fg7dMp5sLvFxwVy+0DjsbF0gLQopORefgsbv4iXcS/+fgrTOGMdYSdnLIjdSXMXt7GPzz/PQDAn/yTR/HeeaJHT//pEP4dKuozhYKZaz54cKC0dqkIzKcB4qkdX2oqHx+lOEOk51tI1ui0ly5UPkqBr5gG8EcSjUPrH3M4rIrFi6LykJISKGkdY6pslKm6akMD8H/yVCQMPtR76KOw9KpA5zallYPdCR8zVDtGPmeLKzselD2lBVON5iYVIsukYEpRdULucu9lAPo2c7GzjNiaUj7auY+pLYgfLka4ndgsSS+GsMfGybpAsFz6u0jOPEWyOuXPmoFq+Dmr+TzPQNmiYD9QCP3Sq8jwa5SRSC39UmiJwvqdjRFie9Lm10+DUgWrjxTnlqow45mqy4+paFYvN/AymxVKClLJVIOjzy/0TAtWX3sAKOuwrUK8mVVZopNWhFJmEwAghMZqac4YbXNWY2xCVuFdn57CXlrRl6VSaG/Q4qzPameEU3O0rkihEdj75UuFS1YJNu9NOaMemIKpF60rNd+Dx2fQs3Okp5o4tBn3w7SJNKHPk3mlXBQG7PsT9D3c6VF249rcSfStmasHg/dTWgc3p/PYsfd286CDYtd6EuUCukU3bpBVZRMaUy7ar7eqGepGpdpMWixCOQ6+NVlj09nT/oCzbJvFAnYN3ZN2TWmpILjwHSDKCgDm21POMtA4S6Wp5jV6V3X4e0phcLFF6+Ni7X1136iWyJi+vBgf1WmVar5jQQMY0Pvviy4OhtbXydN8H72xZGNgJa05MAAx8rE9sgX6hwusAh9nAYoJPS2+JK89AIAR8GvGwGUmqygqk1odVPu0kZTBAihj7A/pRZPR8ek7APjt57+EoGczunsC6+/QYBq3ehDWLNtkGZVqgJiFE9+h39+ZnEP+dXrvP778AmczNQQ2U5rDz71/EcltmqudWxJz71NWS0xSoEGDEcoANntlohBFhz7HhIb3zFjk3CdxbAyg6Tt81Gg/0QBKJhm6b9PGFA1C7P4pGbaFgwJNu/7nmMcFq6TyxoesyFrb8bD6Yr35Ya0XWmojpZrTKIqCGwubRlT118pyiLaV0fpe1fRWa5gePQyN63m1Qdepo2Pg/XSRN7YgGPIDr4zk33/+8Rt4/teIA15/9hwW/5ooSzGaAAPbZDbLquaLcYjdnyX56N4XM/y9JykgansJG5tpiMoxXWiUu37bS1hl9I9OPo8XvkkL+P+4+CQWv3ceANDaVogOj6vC05XlgQFvbl56tH7JNG3KvhtjsmyNKBcMTNNOYC2gBtaJ/lAg3rHUz0GPmwYjDDiYEp6pbAw8jw1ZRZpXPRZ9D7rWC49T0vVee8dA53aKcNveh0bA9gw6lJVZqKmopqQrkMzTolenpbQPKFtLkHUN/BW6D0+duoOn5m8BIGO+csHfT5u4DdsrrWkwoMcD+YKCV1paCMO9ylLtc4CTzVhbEngKkbHPmacryqd2nQojeZOf6PBIY1Pf0n8CYMf8+neoBw5HVWHgYFYUlVu8PzUI+rausTcGhpXyUVqz1lI1c1wkxuP5uaU6eC+lQ8uCP+YAqvx/AC2eF9iJ/AATS73dyxbwZ3vUTPjlm2cQ3qbvE/aBsE+DWRoappsnrTaut+h5LRo19++2wZ+eoHm+ujxAN6b5sNoYcm/B0uEdAP7OA8aXGcnWDIkJWDE5zKPKkLXm3q49sPN7eCgw2KF18K+Dc7jZInWVNgK7Y/p9b9BEMaRr7h/6aPQr1+rM1hRuDjp4JaEaPo0q+NvMF1hdepg3cXti61EH7Zk2X20krqfWBFmmWLcDuJV7+OGY1q/Pz90gpTWoGeyiT89u1EmRdmmNWYwTnLH3PKk9pJ7QHABu5fOsLm14Oa7EW3xNhmWAW6MLyf7AztMQTNWWRp7HHmNTVxTxxEeSWam9b2AS2xQ6MEiXSqNLgRorjIMtq2oUBuO+DdxSDyK1ZQ6egcirIKi0CVKRgZeWdYeGTajTwB747N+yJvkw0lR/tx/ABMdfUx/6D2NIG7yI0aTaU7McZmJtBooCsGpm+e49RHYfOH23g4P7NAf+/cXTSBdtTVO7gL9H92XxNeD067R/y2kOXVJ1gc8N6cV4WtXRhgHX7MKv6NZYFGjb9WpX+5jY/XvpI8blKDwHBwcHBwcHhxnxiWag0hMdRLdtl+SbKdMzJs2A1B6NAh8oMxDtFtCn04QZDqHL1xjDWQoRhhDzdII0rQard8xoUhk4TtMqe2FM5UskBGBPt9L3q9cEPkxZVP2BbvAPghSG/Xh2sg4X1U5VwIXdgVQ4eYKuw+bXuti/StRCdFj1BlMR+MQRPdTHl09REd3V1l3czeg0lxuPM1wTFfLpdaJD/lt1VUlTZliwaoOzp/aw17VFylogmRzfe6Zs5TKeD9kvpGgIFDai9+diqMgWqy6FnEZXDc096fJRCN8qy6KegczK1h2q1kukNj2FqBQUANN5peEiv8yecrxUc38pGMxkpJkuBEgWiZoxEkznFrFA0ShNIKldEECnumSdvr+JNGALQkUqOV2uAwM1oje8sn0C7/arM02a0xj6/SbEYelvharYOtRoz1WZiXrLlnpBrvww88SPQNPP+HNCv0AmqutYigRirzJnJH8UW0wqNeKgMm2cs543sVegG9gWSNBcdO4Jw0XkopAQlhKQuYBXUkojBX9g1Ti9AXR/WA4KsmGN8xoNyOD4S1YsFNq2QHhJjtlfZ93vY8W2URnqqpgbAP9+URbQpcKuaOP6fXpGF14IsfgGjdE/nEKOyxusOSsKT5LiB4BpRkwVAMDoNP08WV+FZdDwdkezSEC2cth2gviNqx8/vq7USK2RpqxVuefK46JkoVErCK68fuI9QNlT+l62iN1mmWYgWggAgqFAa2jnSN8gGtj+a4GAb6n7YTSP/xqQQurF9gU0bdurXtbgNiSTLMBoYtejXgw5Pf65/Vywx8q7EBp/OTkHgKjUtkfzZdUbMg031hG/fmNhgLuWbciVhx2rHO3IuvFtwL3YDmrzYC0aHOkr16u1n2nX3l+uv105YRq2LafwZqgZELGCyWrXpFTEZtVzbgKDYs5S3B9kRGx2SdTUszITkGXWSeKIAS3Tf0pA2yySDqped0KBr2c9c29qymaZSBh1/DF6O4cwjVrmsdxrteb9Xvg+YFkjKAVhWQwDYPEHVKS/+ONqP9PNENr6F3qDFHJkqdZCwbN7vx6NIRfpQTNRCGEZHngexAaxNyI0TO82ZYG2Nbv1hMK8/Ph98RMNoIanQsTvlfl7AW0dd0WuKqomq6gk1W1B2kBGLHTg1SWFXqXI0jbA0c2wonaW2lx9r2KP5ZpSGaZeVCRRNEvlmOBmtSoQlUOyhyPp0geB3V8BjIsQ7YAeNm0k7k9rsu6yz10nQRrRZqQDhXXbk+7x7j10fHpv00v5oZWiklzu5W2mXCJZsPKjThfGNQ3qRIcsXU4KH8mUrq3nK8zP/6T554Mw35piq00TvmgGMH4lQzeBTUMrwyoOtSWhYqvOyyp1R3NXwzugiV1MJhCRDWql4PtpasGUAKpUrKl6I0ECxivdZWu98Ay4195xMDjjIS9vo8QHFhl7jRuazVkhDZoduj9xmCO1jsSTQQxlFXZhXyLs0fXONwMclgGFqj4/VAJl+YtUQFKm7BsFmiHdx7oM/P81eAJIqVUFUApe2QTWV9z7cTGcYC2waXGhsRnQJhv7BcejgafQsmah3WB6pJFneZBICp83CJlXC7uXUSNVgBpHi4T+rkkzmJw+U/h+NQfynOTHx4SEQWzXjDVviocjUux05RTzZc82pEiMvS/G47otD9Umspe3kdv6n9aOQvi+VVylGWDXJxP4leWGMWwkK/ICgR0XfA8t+1wEU59Vq0UsoEpFUORDlXv1L3/8+DwhmCYNharVq4kjdhHcUzEASr/cYGoQDqrnVdmgRhjAH9navnHlhh8NDMKBLUeIJDekDXsSe3u0jie5z4F1mvtIUttPLfOgp3ZTGnp8SDwO5mWKq3Y+3ik0nh9Qn8n703n8g3UqZVj0EqZdbhaV9cZac4hb8/SdR2mItzOykXm6cZN5zbGO+EDaVw10A1oHN8IeK/om2q9qWWuBUWI8PqB6MEztZfBYzXksSANYVaNIPKbbhKrunY40jF+uQxX1VtYkARQklXRx3XrAeICJ6uvDT84NExim5ExgIMqkQXZ0LpWHUmFmKiuFyXLA9qH74LNibI9Yk6YoTw+yM8dlICgU0W8AHaTtei8LBVkmSfKier0QEBNaj0WSVuUkC014thOK7I/5wB/GU66jo2bCNMjFmg3KR8FReA4ODg4ODg4OM0IYM9vJ1cHBwcHBwcHh/3e4DJSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4xwAZSDg4ODg4ODw4z4vzvwlV35ikeRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train labels for each of the above image:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('First 10 images from X_train')\n",
    "plt.figure(figsize=(10, 1))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(X_train[i])\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "print('y_train labels for each of the above image:')\n",
    "(y_train[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above, we can see the nth value in the array corresponds to the value in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\">In the train and test loop, define the hyperparameters for the model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hyperparameters for building the NN sequential model\n",
    "learning_rate = 0.03\n",
    "\n",
    "# Defining values for regularizeation parameters l1 (LASSO) and l2 (Ridge)\n",
    "# Setting the initial values of l1 & l2 to 0.01\n",
    "l1=0.01\n",
    "l2=0.01\n",
    "\n",
    "# Define batch size and epochs\n",
    "batch_size = 200\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\"> Create a Sequential model in Keras with input layer with the correct input shape, Hidden Layers, Output Layers and the activation functions </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Sequential with reLu activation function - 1 input, 5 hidden and output layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input size\n",
    "input_dim = X_train.shape[1:]\n",
    "\n",
    "#Initialising a Sequential model\n",
    "model1 = Sequential()\n",
    "\n",
    "# Adding input layer\n",
    "model1.add(Dense(num_classes * 8, input_shape=input_dim,  activation = 'relu'))\n",
    "\n",
    "# Adding first hidden layer and relu activation fucntion\n",
    "model1.add(Dense(num_classes * 8, activation = 'relu'))\n",
    "\n",
    "# Adding second hidden layer with relu activation fucntion\n",
    "model1.add(Dense(num_classes * 6, activation = 'relu'))\n",
    "\n",
    "# Adding third hidden layer with relu activation fucntion\n",
    "model1.add(Dense(num_classes * 4, activation = 'relu'))\n",
    "\n",
    "# Adding forth hidden layer with relu activation fucntion\n",
    "model1.add(Dense(num_classes * 2, activation = 'relu'))\n",
    "\n",
    "# Adding fifth hidden layer with relu activation fucntion\n",
    "model1.add(Dense(num_classes, activation = 'relu'))\n",
    "\n",
    "# Flattening the output from previous layer\n",
    "model1.add(Flatten())\n",
    "\n",
    "#Output layer with Softmax activation function\n",
    "model1.add(Dense(num_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\">Define the optimizer to be used in this model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining SGD optimizer\n",
    "optimizer = keras.optimizers.SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\">Compile the model with the corresponding Loss and metrics to monitor</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model with 'categorical_crossentropy' loss variable and accuracy metrics as this is a classification model\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\">Initializing Early stopping callback</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Early stopping callback to the fit function is going to stop the training,\n",
    "#if the val_loss is not going to change even '0.001' for more than 10 continous epochs\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\">Initializing Model chekpoint callbacks</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Model Checkpoint callback to the fit function is going to save the weights whenever val_loss achieves a new low value. \n",
    "#Hence saving the best weights occurred during training\n",
    "\n",
    "model_checkpoint1 =  ModelCheckpoint('mnist_cnn_checkpoint_{epoch:02d}_loss{val_loss:.4f}.h5',\n",
    "                                                           monitor='val_loss',\n",
    "                                                           verbose=1,\n",
    "                                                           save_best_only=True,\n",
    "                                                           save_weights_only=True,\n",
    "                                                           mode='auto',\n",
    "                                                           period=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\">Fit the model and use model.evaluate() to return the score</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/30\n",
      "42000/42000 [==============================] - 7s 169us/step - loss: 2.3020 - accuracy: 0.1041 - val_loss: 2.2964 - val_accuracy: 0.1337\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.29636, saving model to mnist_cnn_checkpoint_01_loss2.2964.h5\n",
      "Epoch 2/30\n",
      "42000/42000 [==============================] - 7s 167us/step - loss: 2.1184 - accuracy: 0.2349 - val_loss: 1.8458 - val_accuracy: 0.3664\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.29636 to 1.84578, saving model to mnist_cnn_checkpoint_02_loss1.8458.h5\n",
      "Epoch 3/30\n",
      "42000/42000 [==============================] - 7s 164us/step - loss: 1.7135 - accuracy: 0.4420 - val_loss: 1.4665 - val_accuracy: 0.5375\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.84578 to 1.46653, saving model to mnist_cnn_checkpoint_03_loss1.4665.h5\n",
      "Epoch 4/30\n",
      "42000/42000 [==============================] - 7s 168us/step - loss: 1.4133 - accuracy: 0.5549 - val_loss: 1.2896 - val_accuracy: 0.5989\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.46653 to 1.28958, saving model to mnist_cnn_checkpoint_04_loss1.2896.h5\n",
      "Epoch 5/30\n",
      "42000/42000 [==============================] - 7s 164us/step - loss: 1.2559 - accuracy: 0.6116 - val_loss: 1.1641 - val_accuracy: 0.6411\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.28958 to 1.16415, saving model to mnist_cnn_checkpoint_05_loss1.1641.h5\n",
      "Epoch 6/30\n",
      "42000/42000 [==============================] - 7s 165us/step - loss: 1.1690 - accuracy: 0.6414 - val_loss: 1.1109 - val_accuracy: 0.6594\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.16415 to 1.11095, saving model to mnist_cnn_checkpoint_06_loss1.1109.h5\n",
      "Epoch 7/30\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 1.1276 - accuracy: 0.6540 - val_loss: 1.1989 - val_accuracy: 0.6249\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.11095\n",
      "Epoch 8/30\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.0913 - accuracy: 0.6641 - val_loss: 1.0755 - val_accuracy: 0.6638\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.11095 to 1.07548, saving model to mnist_cnn_checkpoint_08_loss1.0755.h5\n",
      "Epoch 9/30\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.0672 - accuracy: 0.6716 - val_loss: 1.0379 - val_accuracy: 0.6814\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.07548 to 1.03791, saving model to mnist_cnn_checkpoint_09_loss1.0379.h5\n",
      "Epoch 10/30\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.0481 - accuracy: 0.6790 - val_loss: 1.0335 - val_accuracy: 0.6814\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.03791 to 1.03354, saving model to mnist_cnn_checkpoint_10_loss1.0335.h5\n",
      "Epoch 11/30\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.0263 - accuracy: 0.6846 - val_loss: 1.0098 - val_accuracy: 0.6872\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.03354 to 1.00983, saving model to mnist_cnn_checkpoint_11_loss1.0098.h5\n",
      "Epoch 12/30\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.0132 - accuracy: 0.6894 - val_loss: 0.9822 - val_accuracy: 0.7001 1.0\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.00983 to 0.98222, saving model to mnist_cnn_checkpoint_12_loss0.9822.h5\n",
      "Epoch 13/30\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 0.9989 - accuracy: 0.6913 - val_loss: 0.9884 - val_accuracy: 0.6977\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.98222\n",
      "Epoch 14/30\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.9880 - accuracy: 0.6965 - val_loss: 1.0011 - val_accuracy: 0.6938\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.98222\n",
      "Epoch 15/30\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 0.9779 - accuracy: 0.7006 - val_loss: 0.9623 - val_accuracy: 0.7079\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.98222 to 0.96226, saving model to mnist_cnn_checkpoint_15_loss0.9623.h5\n",
      "Epoch 16/30\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.9694 - accuracy: 0.7023 - val_loss: 0.9899 - val_accuracy: 0.6938\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.96226\n",
      "Epoch 17/30\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 0.9595 - accuracy: 0.7063 - val_loss: 0.9442 - val_accuracy: 0.7090\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.96226 to 0.94422, saving model to mnist_cnn_checkpoint_17_loss0.9442.h5\n",
      "Epoch 18/30\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 0.9505 - accuracy: 0.7081 - val_loss: 0.9885 - val_accuracy: 0.6985\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.94422\n",
      "Epoch 19/30\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.9429 - accuracy: 0.7126 - val_loss: 0.9221 - val_accuracy: 0.7176\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.94422 to 0.92209, saving model to mnist_cnn_checkpoint_19_loss0.9221.h5\n",
      "Epoch 20/30\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.9348 - accuracy: 0.7141 - val_loss: 0.9379 - val_accuracy: 0.7131\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.92209\n",
      "Epoch 21/30\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.9295 - accuracy: 0.7156 - val_loss: 0.9452 - val_accuracy: 0.7117\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.92209\n",
      "Epoch 22/30\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 0.9198 - accuracy: 0.7177 - val_loss: 0.9386 - val_accuracy: 0.7133\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.92209\n",
      "Epoch 23/30\n",
      "42000/42000 [==============================] - 8s 198us/step - loss: 0.9157 - accuracy: 0.7201 - val_loss: 0.9113 - val_accuracy: 0.7207\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.92209 to 0.91131, saving model to mnist_cnn_checkpoint_23_loss0.9113.h5\n",
      "Epoch 24/30\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 0.9046 - accuracy: 0.7214 - val_loss: 0.9013 - val_accuracy: 0.7248\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.91131 to 0.90130, saving model to mnist_cnn_checkpoint_24_loss0.9013.h5\n",
      "Epoch 25/30\n",
      "42000/42000 [==============================] - 8s 197us/step - loss: 0.9030 - accuracy: 0.7247 - val_loss: 0.9127 - val_accuracy: 0.7186\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.90130\n",
      "Epoch 26/30\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 0.8955 - accuracy: 0.7272 - val_loss: 0.9021 - val_accuracy: 0.7259\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.90130\n",
      "Epoch 27/30\n",
      "42000/42000 [==============================] - 9s 206us/step - loss: 0.8908 - accuracy: 0.7290 - val_loss: 0.9015 - val_accuracy: 0.7253\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.90130\n",
      "Epoch 28/30\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 0.8840 - accuracy: 0.7301 - val_loss: 0.8796 - val_accuracy: 0.7308\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.90130 to 0.87955, saving model to mnist_cnn_checkpoint_28_loss0.8796.h5\n",
      "Epoch 29/30\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 0.8761 - accuracy: 0.7325 - val_loss: 0.9112 - val_accuracy: 0.7219\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.87955\n",
      "Epoch 30/30\n",
      "42000/42000 [==============================] - 8s 190us/step - loss: 0.8749 - accuracy: 0.7328 - val_loss: 0.8957 - val_accuracy: 0.7244\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.87955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2510d798fd0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training on the dataset and adding the all the callbacks to the fit function.\n",
    "model1.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val), \n",
    "          callbacks = [early_stopping,model_checkpoint1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From this model, we are getting a validation accuracy of 72.44% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 61us/step\n",
      "Val loss: 0.895746142522494\n",
      "Val accuracy: 0.7244333624839783\n",
      "18000/18000 [==============================] - 1s 61us/step\n",
      "Test loss: 0.9729620463583204\n",
      "Test accuracy: 0.7037222385406494\n"
     ]
    }
   ],
   "source": [
    "# Lets find the scores on validations and test data using this trained model\n",
    "scores_mv1 = model1.evaluate(X_val, y_val, verbose=1)\n",
    "print('Val loss:', scores_mv1[0])\n",
    "print('Val accuracy:', scores_mv1[1])\n",
    "\n",
    "scores_mt1 = model1.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores_mt1[0])\n",
    "print('Test accuracy:', scores_mt1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\">Disable Regularization by setting appropriate value for Lambda and check the loss of the NN</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have anyway not used any regularisation parameter in the previous model, however making the l1 & l2 parameters as 0 adding regularisation in each of the layers to create a 2nd model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model2 - Disabling regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disable regularisation\n",
    "l1=0\n",
    "l2=0\n",
    "\n",
    "# input size\n",
    "input_dim = X_train.shape[1:]\n",
    "\n",
    "#Initialising a Sequential model\n",
    "model2 = Sequential()\n",
    "\n",
    "# Adding first layer and relu activation fucntion and l1,l2 regularisation\n",
    "model2.add(Dense(num_classes * 8, input_shape=input_dim,  activation = 'relu', activity_regularizer=regularizers.l1(l1),kernel_regularizer=regularizers.l2(l2)))\n",
    "\n",
    "# Adding first hidden layer and relu activation fucntion and l1,l2 regularisation\n",
    "model2.add(Dense(num_classes * 8, activation = 'relu', activity_regularizer=regularizers.l1(l1),kernel_regularizer=regularizers.l2(l2)))\n",
    "\n",
    "# Adding second layer with relu activation fucntion and l1,l2 regularisation\n",
    "model2.add(Dense(num_classes * 6, activation = 'relu', activity_regularizer=regularizers.l1(l1),kernel_regularizer=regularizers.l2(l2)))\n",
    "\n",
    "# Adding third layer with relu activation fucntion and l1,l2 regularisation\n",
    "model2.add(Dense(num_classes * 4, activation = 'relu', activity_regularizer=regularizers.l1(l1),kernel_regularizer=regularizers.l2(l2)))\n",
    "\n",
    "# Adding forth layer with relu activation fucntion and l1,l2 regularisation\n",
    "model2.add(Dense(num_classes * 2, activation = 'relu', activity_regularizer=regularizers.l1(l1),kernel_regularizer=regularizers.l2(l2)))\n",
    "\n",
    "# Adding fifth layer with relu activation fucntion and l1,l2 regularisation\n",
    "model2.add(Dense(num_classes, activation = 'relu', activity_regularizer=regularizers.l1(l1),kernel_regularizer=regularizers.l2(l2)))\n",
    "\n",
    "# Flattening the output from previous layer\n",
    "model2.add(Flatten())\n",
    "\n",
    "#Output layer with Softmax activation function\n",
    "model2.add(Dense(num_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model with 'categorical_crossentropy' loss variable and accuracy metrics as this is a classification model\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Adding Early stopping callback to the fit function is going to stop the training,\n",
    "#if the val_loss is not going to change even '0.001' for more than 10 continous epochs\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)\n",
    "\n",
    "#Adding Model Checkpoint callback to the fit function is going to save the weights whenever val_loss achieves a new low value. \n",
    "#Hence saving the best weights occurred during training\n",
    "\n",
    "model_checkpoint2 =  ModelCheckpoint('mnist_cnn_checkpoint_{epoch:02d}_loss{val_loss:.4f}.h5',\n",
    "                                                           monitor='val_loss',\n",
    "                                                           verbose=1,\n",
    "                                                           save_best_only=True,\n",
    "                                                           save_weights_only=True,\n",
    "                                                           mode='auto',\n",
    "                                                           period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/30\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 2.3031 - accuracy: 0.1011 - val_loss: 2.3022 - val_accuracy: 0.1040\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.30223, saving model to mnist_cnn_checkpoint_01_loss2.3022.h5\n",
      "Epoch 2/30\n",
      "42000/42000 [==============================] - 7s 165us/step - loss: 2.2959 - accuracy: 0.1252 - val_loss: 2.2546 - val_accuracy: 0.1816\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.30223 to 2.25464, saving model to mnist_cnn_checkpoint_02_loss2.2546.h5\n",
      "Epoch 3/30\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 2.0568 - accuracy: 0.2944 - val_loss: 1.7899 - val_accuracy: 0.4220\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.25464 to 1.78992, saving model to mnist_cnn_checkpoint_03_loss1.7899.h5\n",
      "Epoch 4/30\n",
      "42000/42000 [==============================] - 7s 170us/step - loss: 1.7768 - accuracy: 0.4260 - val_loss: 1.6795 - val_accuracy: 0.4682\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.78992 to 1.67950, saving model to mnist_cnn_checkpoint_04_loss1.6795.h5\n",
      "Epoch 5/30\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 1.6324 - accuracy: 0.4826 - val_loss: 1.4706 - val_accuracy: 0.5398\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.67950 to 1.47058, saving model to mnist_cnn_checkpoint_05_loss1.4706.h5\n",
      "Epoch 6/30\n",
      "42000/42000 [==============================] - 7s 174us/step - loss: 1.4290 - accuracy: 0.5530 - val_loss: 1.3514 - val_accuracy: 0.5818\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.47058 to 1.35142, saving model to mnist_cnn_checkpoint_06_loss1.3514.h5\n",
      "Epoch 7/30\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.2813 - accuracy: 0.6056 - val_loss: 1.1777 - val_accuracy: 0.6406\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.35142 to 1.17768, saving model to mnist_cnn_checkpoint_07_loss1.1777.h5\n",
      "Epoch 8/30\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.1823 - accuracy: 0.6384 - val_loss: 1.1358 - val_accuracy: 0.6524\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.17768 to 1.13577, saving model to mnist_cnn_checkpoint_08_loss1.1358.h5\n",
      "Epoch 9/30\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.1377 - accuracy: 0.6491 - val_loss: 1.0961 - val_accuracy: 0.6650\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.13577 to 1.09605, saving model to mnist_cnn_checkpoint_09_loss1.0961.h5\n",
      "Epoch 10/30\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.1025 - accuracy: 0.6607 - val_loss: 1.0830 - val_accuracy: 0.6678\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.09605 to 1.08299, saving model to mnist_cnn_checkpoint_10_loss1.0830.h5\n",
      "Epoch 11/30\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.0783 - accuracy: 0.6674 - val_loss: 1.0557 - val_accuracy: 0.6730\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.08299 to 1.05572, saving model to mnist_cnn_checkpoint_11_loss1.0557.h5\n",
      "Epoch 12/30\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.0558 - accuracy: 0.6757 - val_loss: 1.0453 - val_accuracy: 0.6811\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.05572 to 1.04534, saving model to mnist_cnn_checkpoint_12_loss1.0453.h5\n",
      "Epoch 13/30\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.0358 - accuracy: 0.6813 - val_loss: 1.0294 - val_accuracy: 0.6842\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.04534 to 1.02938, saving model to mnist_cnn_checkpoint_13_loss1.0294.h5\n",
      "Epoch 14/30\n",
      "42000/42000 [==============================] - 8s 190us/step - loss: 1.0235 - accuracy: 0.6863 - val_loss: 1.0112 - val_accuracy: 0.6889\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.02938 to 1.01123, saving model to mnist_cnn_checkpoint_14_loss1.0112.h5\n",
      "Epoch 15/30\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.0052 - accuracy: 0.6895 - val_loss: 0.9894 - val_accuracy: 0.6960\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.01123 to 0.98943, saving model to mnist_cnn_checkpoint_15_loss0.9894.h5\n",
      "Epoch 16/30\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 0.9984 - accuracy: 0.6929 - val_loss: 1.0260 - val_accuracy: 0.6843\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.98943\n",
      "Epoch 17/30\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 0.9837 - accuracy: 0.6969 - val_loss: 0.9708 - val_accuracy: 0.6995\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.98943 to 0.97080, saving model to mnist_cnn_checkpoint_17_loss0.9708.h5\n",
      "Epoch 18/30\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 0.9752 - accuracy: 0.7014 - val_loss: 1.0061 - val_accuracy: 0.6896\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.97080\n",
      "Epoch 19/30\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 0.9671 - accuracy: 0.7000 - val_loss: 0.9631 - val_accuracy: 0.7015\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.97080 to 0.96314, saving model to mnist_cnn_checkpoint_19_loss0.9631.h5\n",
      "Epoch 20/30\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 0.9575 - accuracy: 0.7079 - val_loss: 0.9395 - val_accuracy: 0.7096\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.96314 to 0.93950, saving model to mnist_cnn_checkpoint_20_loss0.9395.h5\n",
      "Epoch 21/30\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 0.9494 - accuracy: 0.7092 - val_loss: 0.9490 - val_accuracy: 0.7060\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.93950\n",
      "Epoch 22/30\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 0.9410 - accuracy: 0.7104 - val_loss: 0.9636 - val_accuracy: 0.7040\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.93950\n",
      "Epoch 23/30\n",
      "42000/42000 [==============================] - 8s 196us/step - loss: 0.9372 - accuracy: 0.7113 - val_loss: 0.9533 - val_accuracy: 0.7009\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.93950\n",
      "Epoch 24/30\n",
      "42000/42000 [==============================] - 8s 192us/step - loss: 0.9285 - accuracy: 0.7129 - val_loss: 0.9240 - val_accuracy: 0.7156\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.93950 to 0.92400, saving model to mnist_cnn_checkpoint_24_loss0.9240.h5\n",
      "Epoch 25/30\n",
      "42000/42000 [==============================] - 8s 192us/step - loss: 0.9238 - accuracy: 0.7157 - val_loss: 0.9155 - val_accuracy: 0.7183\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.92400 to 0.91547, saving model to mnist_cnn_checkpoint_25_loss0.9155.h5\n",
      "Epoch 26/30\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 0.9160 - accuracy: 0.7207 - val_loss: 0.9293 - val_accuracy: 0.7136\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.91547\n",
      "Epoch 27/30\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 0.9122 - accuracy: 0.7174 - val_loss: 0.8935 - val_accuracy: 0.7221\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.91547 to 0.89355, saving model to mnist_cnn_checkpoint_27_loss0.8935.h5\n",
      "Epoch 28/30\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 0.9033 - accuracy: 0.7226 - val_loss: 0.8917 - val_accuracy: 0.7278\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.89355 to 0.89166, saving model to mnist_cnn_checkpoint_28_loss0.8917.h5\n",
      "Epoch 29/30\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 0.9038 - accuracy: 0.7200 - val_loss: 0.8877 - val_accuracy: 0.7265\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.89166 to 0.88771, saving model to mnist_cnn_checkpoint_29_loss0.8877.h5\n",
      "Epoch 30/30\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 0.8934 - accuracy: 0.7243 - val_loss: 0.9076 - val_accuracy: 0.7245\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.88771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2510d3e5f98>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training on the dataset and adding the all the callbacks to the fit function.\n",
    "model2.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val), \n",
    "          callbacks = [early_stopping,model_checkpoint2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the new model with l1 & l2 as 0 , after 30 epocs, we see a validation accuracy of 77.245 and loss of 0.8934, however the best values have been achieved even before 30 epocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 62us/step\n",
      "Val loss: 0.9076027659654617\n",
      "Val accuracy: 0.7244666814804077\n",
      "18000/18000 [==============================] - 1s 62us/step\n",
      "Test loss: 0.9741645645565457\n",
      "Test accuracy: 0.7059444189071655\n"
     ]
    }
   ],
   "source": [
    "# Lets find the scores on validations and test data using this trained model\n",
    "scores_mv2 = model2.evaluate(X_val, y_val, verbose=1)\n",
    "print('Val loss:', scores_mv2[0])\n",
    "print('Val accuracy:', scores_mv2[1])\n",
    "\n",
    "scores_mt2 = model2.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores_mt2[0])\n",
    "print('Test accuracy:', scores_mt2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The test accuracy on 2nd model is 70.59% and the earlier model without regularisation was 70.37%, there is no much appreaciable change in the values.\n",
    "#### Lets proceed to tune the parameters further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducing another regularisation parameter Dropout with an initial value of 0.25 and build a new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model3 : Introducing Drop Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input size\n",
    "input_dim = X_train.shape[1:]\n",
    "\n",
    "#Initialising a Sequential model\n",
    "model3 = Sequential()\n",
    "\n",
    "# Adding first layer and relu activation fucntion\n",
    "model3.add(Dense(num_classes * 8, input_shape=input_dim,  activation = 'relu'))\n",
    "#Add a drop out of 0.25\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "# Adding first hidden layer and relu activation fucntion\n",
    "model3.add(Dense(num_classes * 8, activation = 'relu'))\n",
    "#Add a drop out of 0.25\n",
    "model3.add(Dropout(0.25))\n",
    "           \n",
    "# Adding second hidden layer with relu activation fucntion\n",
    "model3.add(Dense(num_classes * 6, activation = 'relu'))\n",
    "#Add a drop out of 0.25\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "# Adding third hidden layer with relu activation fucntion\n",
    "model3.add(Dense(num_classes * 4, activation = 'relu'))\n",
    "#Add a drop out of 0.25\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "# Adding forth hidden layer with relu activation fucntion\n",
    "model3.add(Dense(num_classes * 2, activation = 'relu'))\n",
    "#Add a drop out of 0.25\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "# Adding fifth hidden layer with relu activation fucntion\n",
    "model3.add(Dense(num_classes, activation = 'relu'))\n",
    "#Add a drop out of 0.25\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "# Flattening the output from previous layer\n",
    "model3.add(Flatten())\n",
    "\n",
    "#Output layer with Softmax activation function\n",
    "model3.add(Dense(num_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model with 'categorical_crossentropy' loss variable and accuracy metrics as this is a classification model\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Adding Early stopping callback to the fit function is going to stop the training,\n",
    "#if the val_loss is not going to change even '0.001' for more than 10 continous epochs\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)\n",
    "\n",
    "#Adding Model Checkpoint callback to the fit function is going to save the weights whenever val_loss achieves a new low value. \n",
    "#Hence saving the best weights occurred during training\n",
    "\n",
    "model_checkpoint3 =  ModelCheckpoint('mnist_cnn_checkpoint_{epoch:02d}_loss{val_loss:.4f}.h5',\n",
    "                                                           monitor='val_loss',\n",
    "                                                           verbose=1,\n",
    "                                                           save_best_only=True,\n",
    "                                                           save_weights_only=True,\n",
    "                                                           mode='auto',\n",
    "                                                           period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/30\n",
      "42000/42000 [==============================] - 15s 362us/step - loss: 2.3039 - accuracy: 0.1009 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.30278, saving model to mnist_cnn_checkpoint_01_loss2.3028.h5\n",
      "Epoch 2/30\n",
      "42000/42000 [==============================] - 14s 329us/step - loss: 2.3030 - accuracy: 0.0984 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.30278\n",
      "Epoch 3/30\n",
      "42000/42000 [==============================] - 14s 331us/step - loss: 2.3030 - accuracy: 0.1015 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.30278 to 2.30272, saving model to mnist_cnn_checkpoint_03_loss2.3027.h5\n",
      "Epoch 4/30\n",
      "42000/42000 [==============================] - 14s 328us/step - loss: 2.3030 - accuracy: 0.1010 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.30272\n",
      "Epoch 5/30\n",
      "42000/42000 [==============================] - 15s 355us/step - loss: 2.3031 - accuracy: 0.1003 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.30272\n",
      "Epoch 6/30\n",
      "42000/42000 [==============================] - 16s 381us/step - loss: 2.3030 - accuracy: 0.1006 - val_loss: 2.3029 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.30272\n",
      "Epoch 7/30\n",
      "42000/42000 [==============================] - 15s 365us/step - loss: 2.3029 - accuracy: 0.0986 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.30272\n",
      "Epoch 8/30\n",
      "42000/42000 [==============================] - 16s 374us/step - loss: 2.3026 - accuracy: 0.1022 - val_loss: 2.3020 - val_accuracy: 0.1001\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.30272 to 2.30198, saving model to mnist_cnn_checkpoint_08_loss2.3020.h5\n",
      "Epoch 9/30\n",
      "42000/42000 [==============================] - 16s 370us/step - loss: 2.3017 - accuracy: 0.1056 - val_loss: 2.2983 - val_accuracy: 0.1224\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.30198 to 2.29833, saving model to mnist_cnn_checkpoint_09_loss2.2983.h5\n",
      "Epoch 10/30\n",
      "42000/42000 [==============================] - 16s 378us/step - loss: 2.2841 - accuracy: 0.1337 - val_loss: 2.2115 - val_accuracy: 0.1888\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.29833 to 2.21150, saving model to mnist_cnn_checkpoint_10_loss2.2115.h5\n",
      "Epoch 11/30\n",
      "42000/42000 [==============================] - 16s 373us/step - loss: 2.1526 - accuracy: 0.2159 - val_loss: 1.9512 - val_accuracy: 0.3227\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.21150 to 1.95115, saving model to mnist_cnn_checkpoint_11_loss1.9512.h5\n",
      "Epoch 12/30\n",
      "42000/42000 [==============================] - 16s 373us/step - loss: 1.9870 - accuracy: 0.2927 - val_loss: 1.8650 - val_accuracy: 0.3544\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.95115 to 1.86496, saving model to mnist_cnn_checkpoint_12_loss1.8650.h5\n",
      "Epoch 13/30\n",
      "42000/42000 [==============================] - 16s 385us/step - loss: 1.9314 - accuracy: 0.3209 - val_loss: 1.7769 - val_accuracy: 0.4058\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.86496 to 1.77685, saving model to mnist_cnn_checkpoint_13_loss1.7769.h5\n",
      "Epoch 14/30\n",
      "42000/42000 [==============================] - 16s 375us/step - loss: 1.8008 - accuracy: 0.3875 - val_loss: 1.6065 - val_accuracy: 0.4779\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.77685 to 1.60652, saving model to mnist_cnn_checkpoint_14_loss1.6065.h5\n",
      "Epoch 15/30\n",
      "42000/42000 [==============================] - 16s 375us/step - loss: 1.6780 - accuracy: 0.4433 - val_loss: 1.4940 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.60652 to 1.49398, saving model to mnist_cnn_checkpoint_15_loss1.4940.h5\n",
      "Epoch 16/30\n",
      "42000/42000 [==============================] - 16s 382us/step - loss: 1.6127 - accuracy: 0.4662 - val_loss: 1.4837 - val_accuracy: 0.5232\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.49398 to 1.48374, saving model to mnist_cnn_checkpoint_16_loss1.4837.h5\n",
      "Epoch 17/30\n",
      "42000/42000 [==============================] - 16s 373us/step - loss: 1.5550 - accuracy: 0.4886 - val_loss: 1.4292 - val_accuracy: 0.5407\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.48374 to 1.42920, saving model to mnist_cnn_checkpoint_17_loss1.4292.h5\n",
      "Epoch 18/30\n",
      "42000/42000 [==============================] - 16s 372us/step - loss: 1.5306 - accuracy: 0.4985 - val_loss: 1.3717 - val_accuracy: 0.5613\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.42920 to 1.37175, saving model to mnist_cnn_checkpoint_18_loss1.3717.h5\n",
      "Epoch 19/30\n",
      "42000/42000 [==============================] - 16s 372us/step - loss: 1.5067 - accuracy: 0.5070 - val_loss: 1.3482 - val_accuracy: 0.5719\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.37175 to 1.34824, saving model to mnist_cnn_checkpoint_19_loss1.3482.h5\n",
      "Epoch 20/30\n",
      "42000/42000 [==============================] - 16s 374us/step - loss: 1.4875 - accuracy: 0.5145 - val_loss: 1.3486 - val_accuracy: 0.5677\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.34824\n",
      "Epoch 21/30\n",
      "42000/42000 [==============================] - 16s 382us/step - loss: 1.4696 - accuracy: 0.5190 - val_loss: 1.3448 - val_accuracy: 0.5662\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.34824 to 1.34480, saving model to mnist_cnn_checkpoint_21_loss1.3448.h5\n",
      "Epoch 22/30\n",
      "42000/42000 [==============================] - 15s 365us/step - loss: 1.4511 - accuracy: 0.5274 - val_loss: 1.3128 - val_accuracy: 0.5783\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.34480 to 1.31283, saving model to mnist_cnn_checkpoint_22_loss1.3128.h5\n",
      "Epoch 23/30\n",
      "42000/42000 [==============================] - 15s 358us/step - loss: 1.4427 - accuracy: 0.5305 - val_loss: 1.3032 - val_accuracy: 0.5835\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.31283 to 1.30316, saving model to mnist_cnn_checkpoint_23_loss1.3032.h5\n",
      "Epoch 24/30\n",
      "42000/42000 [==============================] - 15s 364us/step - loss: 1.4203 - accuracy: 0.5364 - val_loss: 1.3019 - val_accuracy: 0.5856\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.30316 to 1.30191, saving model to mnist_cnn_checkpoint_24_loss1.3019.h5\n",
      "Epoch 25/30\n",
      "42000/42000 [==============================] - 15s 368us/step - loss: 1.4160 - accuracy: 0.5418 - val_loss: 1.2918 - val_accuracy: 0.5938\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.30191 to 1.29177, saving model to mnist_cnn_checkpoint_25_loss1.2918.h5\n",
      "Epoch 26/30\n",
      "42000/42000 [==============================] - 15s 364us/step - loss: 1.4073 - accuracy: 0.5431 - val_loss: 1.2561 - val_accuracy: 0.6005\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.29177 to 1.25607, saving model to mnist_cnn_checkpoint_26_loss1.2561.h5\n",
      "Epoch 27/30\n",
      "42000/42000 [==============================] - 15s 359us/step - loss: 1.3943 - accuracy: 0.5482 - val_loss: 1.2601 - val_accuracy: 0.5950\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.25607\n",
      "Epoch 28/30\n",
      "42000/42000 [==============================] - 15s 360us/step - loss: 1.3766 - accuracy: 0.5548 - val_loss: 1.2513 - val_accuracy: 0.6024\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.25607 to 1.25131, saving model to mnist_cnn_checkpoint_28_loss1.2513.h5\n",
      "Epoch 29/30\n",
      "42000/42000 [==============================] - 15s 360us/step - loss: 1.3706 - accuracy: 0.5581 - val_loss: 1.2219 - val_accuracy: 0.6163\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.25131 to 1.22186, saving model to mnist_cnn_checkpoint_29_loss1.2219.h5\n",
      "Epoch 30/30\n",
      "42000/42000 [==============================] - 16s 391us/step - loss: 1.3450 - accuracy: 0.5690 - val_loss: 1.1903 - val_accuracy: 0.6253\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.22186 to 1.19027, saving model to mnist_cnn_checkpoint_30_loss1.1903.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2510fc23e80>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training on the dataset and adding the all the callbacks to the fit function.\n",
    "model3.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val), \n",
    "          callbacks = [early_stopping,model_checkpoint3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With dropout, we could achieve 62.53% accuracy at a loss of 1.345 on vlaidation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 66us/step\n",
      "Val loss: 1.190269003756841\n",
      "Val accuracy: 0.625333309173584\n",
      "18000/18000 [==============================] - 1s 66us/step\n",
      "Test loss: 1.1979000857671103\n",
      "Test accuracy: 0.6243888735771179\n"
     ]
    }
   ],
   "source": [
    "# Lets find the scores on validations and test data using this trained model\n",
    "scores_mv3 = model3.evaluate(X_val, y_val, verbose=1)\n",
    "print('Val loss:', scores_mv3[0])\n",
    "print('Val accuracy:', scores_mv3[1])\n",
    "\n",
    "scores_mt3 = model3.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores_mt3[0])\n",
    "print('Test accuracy:', scores_mt3[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above, we could achieve an accuracy of 62.44% on test data\n",
    "#### Based on all the 3 models, adding dropout didnt prove any beneficial in this case with other parameters being same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\"> Increase the Regularization parameter (Lambda) and check how the loss is for the NN. Record findings </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model4 - Increase Regularisation Dropput to 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To validate the above, lets repeat the dropout scenario first by increasing the dropout to 0.5\n",
    "## input size\n",
    "input_dim = X_train.shape[1:]\n",
    "\n",
    "#Initialising a Sequential model\n",
    "model4 = Sequential()\n",
    "\n",
    "# Adding first layer and relu activation fucntion\n",
    "model4.add(Dense(num_classes * 8, input_shape=input_dim,  activation = 'relu'))\n",
    "#Add a drop out of 0.5\n",
    "model4.add(Dropout(0.5))\n",
    "\n",
    "# Adding first hidden layer and relu activation fucntion\n",
    "model4.add(Dense(num_classes * 8, activation = 'relu'))\n",
    "#Add a drop out of 0.5\n",
    "model4.add(Dropout(0.5))\n",
    "           \n",
    "# Adding second hidden layer with relu activation fucntion\n",
    "model4.add(Dense(num_classes * 6, activation = 'relu'))\n",
    "#Add a drop out of 0.5\n",
    "model4.add(Dropout(0.5))\n",
    "\n",
    "# Adding third hidden layer with relu activation fucntion\n",
    "model4.add(Dense(num_classes * 4, activation = 'relu'))\n",
    "#Add a drop out of 0.5\n",
    "model4.add(Dropout(0.5))\n",
    "\n",
    "# Adding forth hidden layer with relu activation fucntion\n",
    "model3.add(Dense(num_classes * 2, activation = 'relu'))\n",
    "#Add a drop out of 0.5\n",
    "model4.add(Dropout(0.5))\n",
    "\n",
    "# Adding fifth hidden layer with relu activation fucntion\n",
    "model4.add(Dense(num_classes, activation = 'relu'))\n",
    "#Add a drop out of 0.5\n",
    "model4.add(Dropout(0.5))\n",
    "\n",
    "# Flattening the output from previous layer\n",
    "model4.add(Flatten())\n",
    "\n",
    "#Output layer with Softmax activation function\n",
    "model4.add(Dense(num_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model with 'categorical_crossentropy' loss variable and accuracy metrics as this is a classification model\n",
    "model4.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Adding Early stopping callback to the fit function is going to stop the training,\n",
    "#if the val_loss is not going to change even '0.001' for more than 10 continous epochs\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)\n",
    "\n",
    "#Adding Model Checkpoint callback to the fit function is going to save the weights whenever val_loss achieves a new low value. \n",
    "#Hence saving the best weights occurred during training\n",
    "\n",
    "model_checkpoint4 =  ModelCheckpoint('mnist_cnn_checkpoint_{epoch:02d}_loss{val_loss:.4f}.h5',\n",
    "                                                           monitor='val_loss',\n",
    "                                                           verbose=1,\n",
    "                                                           save_best_only=True,\n",
    "                                                           save_weights_only=True,\n",
    "                                                           mode='auto',\n",
    "                                                           period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/30\n",
      "42000/42000 [==============================] - 14s 332us/step - loss: 2.3076 - accuracy: 0.1001 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.30276, saving model to mnist_cnn_checkpoint_01_loss2.3028.h5\n",
      "Epoch 2/30\n",
      "42000/42000 [==============================] - 14s 338us/step - loss: 2.3030 - accuracy: 0.0995 - val_loss: 2.3029 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.30276\n",
      "Epoch 3/30\n",
      "42000/42000 [==============================] - 14s 335us/step - loss: 2.3030 - accuracy: 0.0985 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.30276\n",
      "Epoch 4/30\n",
      "42000/42000 [==============================] - 15s 347us/step - loss: 2.3031 - accuracy: 0.0995 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.30276 to 2.30274, saving model to mnist_cnn_checkpoint_04_loss2.3027.h5\n",
      "Epoch 5/30\n",
      "42000/42000 [==============================] - 15s 346us/step - loss: 2.3029 - accuracy: 0.0985 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.30274 to 2.30273, saving model to mnist_cnn_checkpoint_05_loss2.3027.h5\n",
      "Epoch 6/30\n",
      "42000/42000 [==============================] - 15s 365us/step - loss: 2.3029 - accuracy: 0.0994 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.30273\n",
      "Epoch 7/30\n",
      "42000/42000 [==============================] - 15s 368us/step - loss: 2.3030 - accuracy: 0.1005 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.30273\n",
      "Epoch 8/30\n",
      "42000/42000 [==============================] - 15s 366us/step - loss: 2.3029 - accuracy: 0.1031 - val_loss: 2.3030 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.30273\n",
      "Epoch 9/30\n",
      "42000/42000 [==============================] - 16s 377us/step - loss: 2.3029 - accuracy: 0.1016 - val_loss: 2.3032 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.30273\n",
      "Epoch 10/30\n",
      "42000/42000 [==============================] - 16s 373us/step - loss: 2.3031 - accuracy: 0.0993 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.30273 to 2.30272, saving model to mnist_cnn_checkpoint_10_loss2.3027.h5\n",
      "Epoch 11/30\n",
      "42000/42000 [==============================] - 15s 366us/step - loss: 2.3030 - accuracy: 0.0989 - val_loss: 2.3029 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.30272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x251159d3f28>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training on the dataset and adding the all the callbacks to the fit function.\n",
    "model4.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val), \n",
    "          callbacks = [early_stopping,model_checkpoint4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increasing the dropout regularisation to 0.5 -> Accuracy has reduced to just 10% on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 63us/step\n",
      "Val loss: 2.302866602071126\n",
      "Val accuracy: 0.10000000149011612\n",
      "18000/18000 [==============================] - 1s 62us/step\n",
      "Test loss: 2.303016716003418\n",
      "Test accuracy: 0.09549999982118607\n"
     ]
    }
   ],
   "source": [
    "# Lets find the scores on validations and test data using this trained model\n",
    "scores_mv4 = model4.evaluate(X_val, y_val, verbose=1)\n",
    "print('Val loss:', scores_mv4[0])\n",
    "print('Val accuracy:', scores_mv4[1])\n",
    "\n",
    "scores_mt4 = model4.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores_mt4[0])\n",
    "print('Test accuracy:', scores_mt4[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuracy is 9.54% which is very very less compared to previous accuracy and loss increased from 1.19 to 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that accuracy is dropping and loss is increasing when we increase the regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model5 l1,l2 @ 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disable regularisation\n",
    "l1=0.001\n",
    "l2=0.001\n",
    "\n",
    "# input size\n",
    "input_dim = X_train.shape[1:]\n",
    "\n",
    "#Initialising a Sequential model\n",
    "model5 = Sequential()\n",
    "\n",
    "# Adding first layer and relu activation fucntion and l1,l2 regularisation\n",
    "model5.add(Dense(num_classes * 8, input_shape=input_dim,  activation = 'relu', activity_regularizer=regularizers.l1(l1),kernel_regularizer=regularizers.l2(l2)))\n",
    "\n",
    "# Adding first hidden layer and relu activation fucntion and l1,l2 regularisation\n",
    "model5.add(Dense(num_classes * 8, activation = 'relu', activity_regularizer=regularizers.l1(l1),kernel_regularizer=regularizers.l2(l2)))\n",
    "\n",
    "# Adding second layer with relu activation fucntion and l1,l2 regularisation\n",
    "model5.add(Dense(num_classes * 6, activation = 'relu', activity_regularizer=regularizers.l1(l1),kernel_regularizer=regularizers.l2(l2)))\n",
    "\n",
    "# Adding third layer with relu activation fucntion and l1,l2 regularisation\n",
    "model5.add(Dense(num_classes * 4, activation = 'relu', activity_regularizer=regularizers.l1(l1),kernel_regularizer=regularizers.l2(l2)))\n",
    "\n",
    "# Adding forth layer with relu activation fucntion and l1,l2 regularisation\n",
    "model5.add(Dense(num_classes * 2, activation = 'relu', activity_regularizer=regularizers.l1(l1),kernel_regularizer=regularizers.l2(l2)))\n",
    "\n",
    "# Adding fifth layer with relu activation fucntion and l1,l2 regularisation\n",
    "model5.add(Dense(num_classes, activation = 'relu', activity_regularizer=regularizers.l1(l1),kernel_regularizer=regularizers.l2(l2)))\n",
    "\n",
    "# Flattening the output from previous layer\n",
    "model5.add(Flatten())\n",
    "\n",
    "#Output layer with Softmax activation function\n",
    "model5.add(Dense(num_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model with 'categorical_crossentropy' loss variable and accuracy metrics as this is a classification model\n",
    "model5.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Adding Early stopping callback to the fit function is going to stop the training,\n",
    "#if the val_loss is not going to change even '0.001' for more than 10 continous epochs\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)\n",
    "\n",
    "#Adding Model Checkpoint callback to the fit function is going to save the weights whenever val_loss achieves a new low value. \n",
    "#Hence saving the best weights occurred during training\n",
    "\n",
    "model_checkpoint5 =  ModelCheckpoint('mnist_cnn_checkpoint_{epoch:02d}_loss{val_loss:.4f}.h5',\n",
    "                                                           monitor='val_loss',\n",
    "                                                           verbose=1,\n",
    "                                                           save_best_only=True,\n",
    "                                                           save_weights_only=True,\n",
    "                                                           mode='auto',\n",
    "                                                           period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/30\n",
      "42000/42000 [==============================] - 11s 251us/step - loss: 17.5560 - accuracy: 0.0984 - val_loss: 15.1686 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 15.16856, saving model to mnist_cnn_checkpoint_01_loss15.1686.h5\n",
      "Epoch 2/30\n",
      "42000/42000 [==============================] - 10s 240us/step - loss: 13.7044 - accuracy: 0.1002 - val_loss: 12.3433 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00002: val_loss improved from 15.16856 to 12.34332, saving model to mnist_cnn_checkpoint_02_loss12.3433.h5\n",
      "Epoch 3/30\n",
      "42000/42000 [==============================] - 10s 238us/step - loss: 11.2009 - accuracy: 0.0992 - val_loss: 10.1387 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00003: val_loss improved from 12.34332 to 10.13867, saving model to mnist_cnn_checkpoint_03_loss10.1387.h5\n",
      "Epoch 4/30\n",
      "42000/42000 [==============================] - 10s 247us/step - loss: 9.2474 - accuracy: 0.0989 - val_loss: 8.4186 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00004: val_loss improved from 10.13867 to 8.41856, saving model to mnist_cnn_checkpoint_04_loss8.4186.h5\n",
      "Epoch 5/30\n",
      "42000/42000 [==============================] - 11s 268us/step - loss: 7.7230 - accuracy: 0.0984 - val_loss: 7.0764 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00005: val_loss improved from 8.41856 to 7.07638, saving model to mnist_cnn_checkpoint_05_loss7.0764.h5\n",
      "Epoch 6/30\n",
      "42000/42000 [==============================] - 11s 261us/step - loss: 6.5336 - accuracy: 0.0980 - val_loss: 6.0288 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00006: val_loss improved from 7.07638 to 6.02883, saving model to mnist_cnn_checkpoint_06_loss6.0288.h5\n",
      "Epoch 7/30\n",
      "42000/42000 [==============================] - 11s 251us/step - loss: 5.6054 - accuracy: 0.0995 - val_loss: 5.2114 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00007: val_loss improved from 6.02883 to 5.21135, saving model to mnist_cnn_checkpoint_07_loss5.2114.h5\n",
      "Epoch 8/30\n",
      "42000/42000 [==============================] - 11s 250us/step - loss: 4.8808 - accuracy: 0.0992 - val_loss: 4.5733 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00008: val_loss improved from 5.21135 to 4.57328, saving model to mnist_cnn_checkpoint_08_loss4.5733.h5\n",
      "Epoch 9/30\n",
      "42000/42000 [==============================] - 10s 246us/step - loss: 4.3154 - accuracy: 0.1006 - val_loss: 4.0754 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00009: val_loss improved from 4.57328 to 4.07535, saving model to mnist_cnn_checkpoint_09_loss4.0754.h5\n",
      "Epoch 10/30\n",
      "42000/42000 [==============================] - 10s 250us/step - loss: 3.8741 - accuracy: 0.0978 - val_loss: 3.6865 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00010: val_loss improved from 4.07535 to 3.68654, saving model to mnist_cnn_checkpoint_10_loss3.6865.h5\n",
      "Epoch 11/30\n",
      "42000/42000 [==============================] - 10s 248us/step - loss: 3.5295 - accuracy: 0.1008 - val_loss: 3.3832 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00011: val_loss improved from 3.68654 to 3.38324, saving model to mnist_cnn_checkpoint_11_loss3.3832.h5\n",
      "Epoch 12/30\n",
      "42000/42000 [==============================] - 10s 248us/step - loss: 3.2607 - accuracy: 0.0991 - val_loss: 3.1464 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00012: val_loss improved from 3.38324 to 3.14644, saving model to mnist_cnn_checkpoint_12_loss3.1464.h5\n",
      "Epoch 13/30\n",
      "42000/42000 [==============================] - 11s 250us/step - loss: 3.0508 - accuracy: 0.0978 - val_loss: 2.9615 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00013: val_loss improved from 3.14644 to 2.96149, saving model to mnist_cnn_checkpoint_13_loss2.9615.h5\n",
      "Epoch 14/30\n",
      "42000/42000 [==============================] - 11s 250us/step - loss: 2.8868 - accuracy: 0.0976 - val_loss: 2.8171 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.96149 to 2.81715, saving model to mnist_cnn_checkpoint_14_loss2.8171.h5\n",
      "Epoch 15/30\n",
      "42000/42000 [==============================] - 10s 247us/step - loss: 2.7589 - accuracy: 0.1003 - val_loss: 2.7046 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.81715 to 2.70463, saving model to mnist_cnn_checkpoint_15_loss2.7046.h5\n",
      "Epoch 16/30\n",
      "42000/42000 [==============================] - 10s 249us/step - loss: 2.6590 - accuracy: 0.1007 - val_loss: 2.6165 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.70463 to 2.61645, saving model to mnist_cnn_checkpoint_16_loss2.6165.h5\n",
      "Epoch 17/30\n",
      "42000/42000 [==============================] - 10s 249us/step - loss: 2.5810 - accuracy: 0.0992 - val_loss: 2.5479 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.61645 to 2.54789, saving model to mnist_cnn_checkpoint_17_loss2.5479.h5\n",
      "Epoch 18/30\n",
      "42000/42000 [==============================] - 10s 250us/step - loss: 2.5201 - accuracy: 0.1002 - val_loss: 2.4942 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.54789 to 2.49417, saving model to mnist_cnn_checkpoint_18_loss2.4942.h5\n",
      "Epoch 19/30\n",
      "42000/42000 [==============================] - 11s 251us/step - loss: 2.4726 - accuracy: 0.0998 - val_loss: 2.4522 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.49417 to 2.45222, saving model to mnist_cnn_checkpoint_19_loss2.4522.h5\n",
      "Epoch 20/30\n",
      "42000/42000 [==============================] - 10s 249us/step - loss: 2.4353 - accuracy: 0.1009 - val_loss: 2.4196 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.45222 to 2.41960, saving model to mnist_cnn_checkpoint_20_loss2.4196.h5\n",
      "Epoch 21/30\n",
      "42000/42000 [==============================] - 11s 252us/step - loss: 2.4065 - accuracy: 0.0998 - val_loss: 2.3940 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.41960 to 2.39401, saving model to mnist_cnn_checkpoint_21_loss2.3940.h5\n",
      "Epoch 22/30\n",
      "42000/42000 [==============================] - 11s 258us/step - loss: 2.3838 - accuracy: 0.1003 - val_loss: 2.3742 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00022: val_loss improved from 2.39401 to 2.37416, saving model to mnist_cnn_checkpoint_22_loss2.3742.h5\n",
      "Epoch 23/30\n",
      "42000/42000 [==============================] - 11s 255us/step - loss: 2.3661 - accuracy: 0.0996 - val_loss: 2.3584 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00023: val_loss improved from 2.37416 to 2.35845, saving model to mnist_cnn_checkpoint_23_loss2.3584.h5\n",
      "Epoch 24/30\n",
      "42000/42000 [==============================] - 11s 253us/step - loss: 2.3523 - accuracy: 0.1007 - val_loss: 2.3463 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00024: val_loss improved from 2.35845 to 2.34627, saving model to mnist_cnn_checkpoint_24_loss2.3463.h5\n",
      "Epoch 25/30\n",
      "42000/42000 [==============================] - 11s 263us/step - loss: 2.3416 - accuracy: 0.0996 - val_loss: 2.3368 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00025: val_loss improved from 2.34627 to 2.33681, saving model to mnist_cnn_checkpoint_25_loss2.3368.h5\n",
      "Epoch 26/30\n",
      "42000/42000 [==============================] - 11s 253us/step - loss: 2.3332 - accuracy: 0.0969 - val_loss: 2.3293 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00026: val_loss improved from 2.33681 to 2.32931, saving model to mnist_cnn_checkpoint_26_loss2.3293.h5\n",
      "Epoch 27/30\n",
      "42000/42000 [==============================] - 11s 253us/step - loss: 2.3265 - accuracy: 0.0980 - val_loss: 2.3235 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00027: val_loss improved from 2.32931 to 2.32348, saving model to mnist_cnn_checkpoint_27_loss2.3235.h5\n",
      "Epoch 28/30\n",
      "42000/42000 [==============================] - 10s 249us/step - loss: 2.3214 - accuracy: 0.1006 - val_loss: 2.3191 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00028: val_loss improved from 2.32348 to 2.31908, saving model to mnist_cnn_checkpoint_28_loss2.3191.h5\n",
      "Epoch 29/30\n",
      "42000/42000 [==============================] - 11s 253us/step - loss: 2.3174 - accuracy: 0.1001 - val_loss: 2.3154 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00029: val_loss improved from 2.31908 to 2.31545, saving model to mnist_cnn_checkpoint_29_loss2.3154.h5\n",
      "Epoch 30/30\n",
      "42000/42000 [==============================] - 10s 250us/step - loss: 2.3141 - accuracy: 0.0993 - val_loss: 2.3127 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00030: val_loss improved from 2.31545 to 2.31267, saving model to mnist_cnn_checkpoint_30_loss2.3127.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25115c48f60>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training on the dataset and adding the all the callbacks to the fit function.\n",
    "model5.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val), \n",
    "          callbacks = [early_stopping,model_checkpoint5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increasing the values of l1 & l2 from 0 to 0.01 has resulted in drop in accuracy and increase in loss on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 71us/step\n",
      "Val loss: 2.3126665828704835\n",
      "Val accuracy: 0.10000000149011612\n",
      "18000/18000 [==============================] - 1s 74us/step\n",
      "Test loss: 2.3128197659386527\n",
      "Test accuracy: 0.10044444352388382\n"
     ]
    }
   ],
   "source": [
    "# Lets find the scores on validations and test data using this trained model\n",
    "scores_mv5 = model5.evaluate(X_val, y_val, verbose=1)\n",
    "print('Val loss:', scores_mv5[0])\n",
    "print('Val accuracy:', scores_mv5[1])\n",
    "\n",
    "scores_mt5 = model5.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores_mt5[0])\n",
    "print('Test accuracy:', scores_mt5[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see with l1 & l2 at 0.01, accuracy is only 10% and loss is 2.31 on test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #0000FF\">Below table gives the observations from 5 models built. We have observed a best accuracy of 70.59 & and least loss of 0.9741 when regularisation parameters l1 & l2 is 0. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>S.no</td>\n",
    "        <td>No.of Layers </td>\n",
    "        <td>Regularisation Lambda() Dropout </td>\n",
    "        <td>Regularisation L1, L2</td>\n",
    "        <td>Optimiser</td>\n",
    "        <td>Learning Rate</td>\n",
    "        <td>Loss (test data)</td>\n",
    "        <td>Accuracy (test data) % </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>5</td>\n",
    "        <td>NA</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.03</td>\n",
    "        <td>0.972</td>\n",
    "        <td>70.37</td>\n",
    "    </tr>\n",
    "   <tr style=\"font-weight:bold; color:blue\">\n",
    "        <td>2</td>\n",
    "        <td>5</td>\n",
    "        <td>NA</td>\n",
    "        <td>0</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.03</td>\n",
    "        <td>0.9741</td>\n",
    "        <td>70.59</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>5</td>\n",
    "        <td>0.25</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.03</td>\n",
    "        <td>1.19</td>\n",
    "        <td>62.44</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>4</td>\n",
    "        <td>5</td>\n",
    "        <td>0.50</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.03</td>\n",
    "        <td>2.30</td>\n",
    "        <td>9.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>5</td>\n",
    "        <td>5</td>\n",
    "        <td>NA</td>\n",
    "        <td>0.01</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.03</td>\n",
    "        <td>2.31</td>\n",
    "        <td>10.04</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\"> Network overfit with a small subset of the dataset. Check if the network will overfit when you use no regularization and the loss is very small and accuracy is 100%. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a subset of 1000 rows from the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test100 = X_test[0:1000]\n",
    "X_train100 = X_train[0:1000]\n",
    "X_val100 = X_val[0:1000]\n",
    "y_test100 = y_test[0:1000]\n",
    "y_train100 = y_train[0:1000]\n",
    "y_val100 = y_val[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1000, 32, 32)\n",
      "X_test shape: (1000, 32, 32)\n",
      "X_val shape: (1000, 32, 32)\n",
      "y_train shape: (1000, 10)\n",
      "y_test shape: (1000, 10)\n",
      "y_val shape: (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train100.shape)\n",
    "print('X_test shape:', X_test100.shape)\n",
    "print('X_val shape:', X_val100.shape)\n",
    "print('y_train shape:', y_train100.shape)\n",
    "print('y_test shape:', y_test100.shape)\n",
    "print('y_val shape:', y_val100.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input size\n",
    "input_dim = X_train100.shape[1:]\n",
    "\n",
    "#Initialising a Sequential model\n",
    "model14 = Sequential()\n",
    "\n",
    "# Adding input layer\n",
    "model14.add(Dense(num_classes * 8, input_shape=input_dim,  activation = 'relu'))\n",
    "\n",
    "# Adding first hidden layer and relu activation fucntion\n",
    "model14.add(Dense(num_classes * 8, activation = 'relu'))\n",
    "\n",
    "# Adding second hidden layer with relu activation fucntion\n",
    "model14.add(Dense(num_classes * 6, activation = 'relu'))\n",
    "\n",
    "# Adding third hidden layer with relu activation fucntion\n",
    "model14.add(Dense(num_classes * 4, activation = 'relu'))\n",
    "\n",
    "# Adding forth hidden layer with relu activation fucntion\n",
    "model14.add(Dense(num_classes * 2, activation = 'relu'))\n",
    "\n",
    "# Adding fifth hidden layer with relu activation fucntion\n",
    "model14.add(Dense(num_classes, activation = 'relu'))\n",
    "\n",
    "# Flattening the output from previous layer\n",
    "model14.add(Flatten())\n",
    "\n",
    "#Output layer with Softmax activation function\n",
    "model14.add(Dense(num_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining SGD optimizer\n",
    "optimizer_ss = keras.optimizers.SGD(lr=learning_rate, decay=1e-6, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model with 'categorical_crossentropy' loss variable and accuracy metrics as this is a classification model\n",
    "model14.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer_ss,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Early stopping callback to the fit function is going to stop the training,\n",
    "#if the val_loss is not going to change even '0.001' for more than 10 continous epochs\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Model Checkpoint callback to the fit function is going to save the weights whenever val_loss achieves a new low value. \n",
    "#Hence saving the best weights occurred during training\n",
    "\n",
    "model_checkpoint14 =  ModelCheckpoint('mnist_cnn_checkpoint_{epoch:02d}_loss{val_loss:.4f}.h5',\n",
    "                                                           monitor='val_loss',\n",
    "                                                           verbose=1,\n",
    "                                                           save_best_only=True,\n",
    "                                                           save_weights_only=True,\n",
    "                                                           mode='auto',\n",
    "                                                           period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "1000/1000 [==============================] - 0s 365us/step - loss: 2.3030 - accuracy: 0.0780 - val_loss: 2.2879 - val_accuracy: 0.0130\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.28791, saving model to mnist_cnn_checkpoint_01_loss2.2879.h5\n",
      "Epoch 2/30\n",
      "1000/1000 [==============================] - 0s 163us/step - loss: 2.3011 - accuracy: 0.0980 - val_loss: 2.2836 - val_accuracy: 0.0330\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.28791 to 2.28365, saving model to mnist_cnn_checkpoint_02_loss2.2836.h5\n",
      "Epoch 3/30\n",
      "1000/1000 [==============================] - 0s 161us/step - loss: 2.2996 - accuracy: 0.1220 - val_loss: 2.2722 - val_accuracy: 0.0170\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.28365 to 2.27222, saving model to mnist_cnn_checkpoint_03_loss2.2722.h5\n",
      "Epoch 4/30\n",
      "1000/1000 [==============================] - 0s 164us/step - loss: 2.2981 - accuracy: 0.1270 - val_loss: 2.2614 - val_accuracy: 0.0130\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.27222 to 2.26135, saving model to mnist_cnn_checkpoint_04_loss2.2614.h5\n",
      "Epoch 5/30\n",
      "1000/1000 [==============================] - 0s 164us/step - loss: 2.2972 - accuracy: 0.1230 - val_loss: 2.2452 - val_accuracy: 0.0140\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.26135 to 2.24517, saving model to mnist_cnn_checkpoint_05_loss2.2452.h5\n",
      "Epoch 6/30\n",
      "1000/1000 [==============================] - 0s 167us/step - loss: 2.2962 - accuracy: 0.1190 - val_loss: 2.2318 - val_accuracy: 0.0200\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.24517 to 2.23184, saving model to mnist_cnn_checkpoint_06_loss2.2318.h5\n",
      "Epoch 7/30\n",
      "1000/1000 [==============================] - 0s 184us/step - loss: 2.2954 - accuracy: 0.1180 - val_loss: 2.2244 - val_accuracy: 0.0270\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.23184 to 2.22444, saving model to mnist_cnn_checkpoint_07_loss2.2244.h5\n",
      "Epoch 8/30\n",
      "1000/1000 [==============================] - 0s 157us/step - loss: 2.2948 - accuracy: 0.1180 - val_loss: 2.2208 - val_accuracy: 0.0390\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.22444 to 2.22076, saving model to mnist_cnn_checkpoint_08_loss2.2208.h5\n",
      "Epoch 9/30\n",
      "1000/1000 [==============================] - 0s 154us/step - loss: 2.2943 - accuracy: 0.1260 - val_loss: 2.2070 - val_accuracy: 0.0750\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.22076 to 2.20699, saving model to mnist_cnn_checkpoint_09_loss2.2070.h5\n",
      "Epoch 10/30\n",
      "1000/1000 [==============================] - 0s 169us/step - loss: 2.2935 - accuracy: 0.1200 - val_loss: 2.2004 - val_accuracy: 0.0760\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.20699 to 2.20040, saving model to mnist_cnn_checkpoint_10_loss2.2004.h5\n",
      "Epoch 11/30\n",
      "1000/1000 [==============================] - 0s 166us/step - loss: 2.2929 - accuracy: 0.1230 - val_loss: 2.2058 - val_accuracy: 0.0930\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.20040\n",
      "Epoch 12/30\n",
      "1000/1000 [==============================] - 0s 174us/step - loss: 2.2930 - accuracy: 0.1230 - val_loss: 2.2056 - val_accuracy: 0.0510\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.20040\n",
      "Epoch 13/30\n",
      "1000/1000 [==============================] - 0s 166us/step - loss: 2.2923 - accuracy: 0.1170 - val_loss: 2.1925 - val_accuracy: 0.0910\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.20040 to 2.19250, saving model to mnist_cnn_checkpoint_13_loss2.1925.h5\n",
      "Epoch 14/30\n",
      "1000/1000 [==============================] - 0s 166us/step - loss: 2.2915 - accuracy: 0.1240 - val_loss: 2.1972 - val_accuracy: 0.2150\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.19250\n",
      "Epoch 15/30\n",
      "1000/1000 [==============================] - 0s 165us/step - loss: 2.2909 - accuracy: 0.1350 - val_loss: 2.1928 - val_accuracy: 0.3060\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.19250\n",
      "Epoch 16/30\n",
      "1000/1000 [==============================] - 0s 157us/step - loss: 2.2900 - accuracy: 0.1420 - val_loss: 2.1831 - val_accuracy: 0.3440\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.19250 to 2.18309, saving model to mnist_cnn_checkpoint_16_loss2.1831.h5\n",
      "Epoch 17/30\n",
      "1000/1000 [==============================] - 0s 162us/step - loss: 2.2880 - accuracy: 0.1480 - val_loss: 2.2025 - val_accuracy: 0.2150\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 2.18309\n",
      "Epoch 18/30\n",
      "1000/1000 [==============================] - 0s 157us/step - loss: 2.2881 - accuracy: 0.1410 - val_loss: 2.2060 - val_accuracy: 0.1740\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.18309\n",
      "Epoch 19/30\n",
      "1000/1000 [==============================] - 0s 153us/step - loss: 2.2865 - accuracy: 0.1510 - val_loss: 2.1870 - val_accuracy: 0.3510\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.18309\n",
      "Epoch 20/30\n",
      "1000/1000 [==============================] - 0s 166us/step - loss: 2.2854 - accuracy: 0.1520 - val_loss: 2.1882 - val_accuracy: 0.3720\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.18309\n",
      "Epoch 21/30\n",
      "1000/1000 [==============================] - 0s 164us/step - loss: 2.2830 - accuracy: 0.1480 - val_loss: 2.1875 - val_accuracy: 0.3520\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.18309\n",
      "Epoch 22/30\n",
      "1000/1000 [==============================] - 0s 155us/step - loss: 2.2833 - accuracy: 0.1570 - val_loss: 2.1820 - val_accuracy: 0.3910\n",
      "\n",
      "Epoch 00022: val_loss improved from 2.18309 to 2.18196, saving model to mnist_cnn_checkpoint_22_loss2.1820.h5\n",
      "Epoch 23/30\n",
      "1000/1000 [==============================] - 0s 170us/step - loss: 2.2820 - accuracy: 0.1550 - val_loss: 2.1736 - val_accuracy: 0.4620\n",
      "\n",
      "Epoch 00023: val_loss improved from 2.18196 to 2.17364, saving model to mnist_cnn_checkpoint_23_loss2.1736.h5\n",
      "Epoch 24/30\n",
      "1000/1000 [==============================] - 0s 172us/step - loss: 2.2779 - accuracy: 0.1600 - val_loss: 2.1873 - val_accuracy: 0.3480\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.17364\n",
      "Epoch 25/30\n",
      "1000/1000 [==============================] - 0s 182us/step - loss: 2.2759 - accuracy: 0.1630 - val_loss: 2.1659 - val_accuracy: 0.4490\n",
      "\n",
      "Epoch 00025: val_loss improved from 2.17364 to 2.16586, saving model to mnist_cnn_checkpoint_25_loss2.1659.h5\n",
      "Epoch 26/30\n",
      "1000/1000 [==============================] - 0s 163us/step - loss: 2.2724 - accuracy: 0.1650 - val_loss: 2.1581 - val_accuracy: 0.4900\n",
      "\n",
      "Epoch 00026: val_loss improved from 2.16586 to 2.15810, saving model to mnist_cnn_checkpoint_26_loss2.1581.h5\n",
      "Epoch 27/30\n",
      "1000/1000 [==============================] - 0s 173us/step - loss: 2.2697 - accuracy: 0.1740 - val_loss: 2.1750 - val_accuracy: 0.4370\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.15810\n",
      "Epoch 28/30\n",
      "1000/1000 [==============================] - 0s 196us/step - loss: 2.2680 - accuracy: 0.1710 - val_loss: 2.1627 - val_accuracy: 0.4750\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.15810\n",
      "Epoch 29/30\n",
      "1000/1000 [==============================] - 0s 166us/step - loss: 2.2617 - accuracy: 0.1640 - val_loss: 2.1750 - val_accuracy: 0.4030\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 2.15810\n",
      "Epoch 30/30\n",
      "1000/1000 [==============================] - 0s 183us/step - loss: 2.2563 - accuracy: 0.1890 - val_loss: 2.1306 - val_accuracy: 0.5400\n",
      "\n",
      "Epoch 00030: val_loss improved from 2.15810 to 2.13056, saving model to mnist_cnn_checkpoint_30_loss2.1306.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25121d62dd8>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training on the dataset and adding the all the callbacks to the fit function.\n",
    "model14.fit(X_train100, y_train100,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val100, y_val100), \n",
    "          callbacks = [early_stopping,model_checkpoint14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 78us/step\n",
      "Val loss: 2.1305630187988283\n",
      "Val accuracy: 0.5400000214576721\n",
      "1000/1000 [==============================] - 0s 71us/step\n",
      "Test loss: 2.2945315208435058\n",
      "Test accuracy: 0.13300000131130219\n"
     ]
    }
   ],
   "source": [
    "# Lets find the scores on validations and test data using this trained model\n",
    "scores_mv14 = model14.evaluate(X_val100, y_val100, verbose=1)\n",
    "print('Val loss:', scores_mv14[0])\n",
    "print('Val accuracy:', scores_mv14[1])\n",
    "\n",
    "scores_mt14 = model14.evaluate(X_test100, y_test100, verbose=1)\n",
    "print('Test loss:', scores_mt14[0])\n",
    "print('Test accuracy:', scores_mt14[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #0000FF\"> A subset of 1000 records has been taken from the original dataset and sequential model is created without any regularisation. After training for 30 epocs a validation accuracy of 54.00% and Test accuracy of 13.30 % is observed. </br> Could not observe any overfitting behavior. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\"> Load the original dataset with all the images and prepare the data for modelling </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X_test', 'X_train', 'X_val', 'y_test', 'y_train', 'y_val']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load SVHN_single_grey1 into a variable\n",
    "inputFile = h5py.File('SVHN_single_grey1.h5', 'r')\n",
    "\n",
    "#List the datasets available in the input h5 file\n",
    "list(inputFile.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = inputFile['X_test']\n",
    "X_train = inputFile['X_train']\n",
    "X_val = inputFile['X_val']\n",
    "y_test = inputFile['y_test']\n",
    "y_train = inputFile['y_train']\n",
    "y_val = inputFile['y_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (42000, 32, 32)\n",
      "X_test shape: (18000, 32, 32)\n",
      "X_val shape: (60000, 32, 32)\n",
      "y_train shape: (42000,)\n",
      "y_test shape: (18000,)\n",
      "y_val shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('y_val shape:', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the input\n",
    "X_train = X_train/a255\n",
    "X_test = X_test/a255\n",
    "X_val = X_val/a255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Keras 'utils.to_categorical' to convert class matrices to one hot vectors\n",
    "# Our data is having 10 output classes, so we declare the number of classes as 10.\n",
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\"> Start with a small Regularization. Keep adjusting the learning rate to check the loss. Record findings </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets consider dropout regularisation with a lambda value of 0.1 and build a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input size\n",
    "input_dim = X_train.shape[1:]\n",
    "\n",
    "#Initialising a Sequential model\n",
    "model6 = Sequential()\n",
    "\n",
    "# Adding first layer and relu activation fucntion\n",
    "model6.add(Dense(num_classes * 8, input_shape=input_dim,  activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model6.add(Dropout(0.1))\n",
    "\n",
    "# Adding first hidden layer and relu activation fucntion\n",
    "model6.add(Dense(num_classes * 8, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model6.add(Dropout(0.1))\n",
    "           \n",
    "# Adding second hidden layer with relu activation fucntion\n",
    "model6.add(Dense(num_classes * 6, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model6.add(Dropout(0.1))\n",
    "\n",
    "# Adding third hidden layer with relu activation fucntion\n",
    "model6.add(Dense(num_classes * 4, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model6.add(Dropout(0.1))\n",
    "\n",
    "# Adding forth hidden layer with relu activation fucntion\n",
    "model6.add(Dense(num_classes * 2, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model6.add(Dropout(0.1))\n",
    "\n",
    "# Adding fifth hidden layer with relu activation fucntion\n",
    "model6.add(Dense(num_classes, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model6.add(Dropout(0.1))\n",
    "\n",
    "# Flattening the output from previous layer\n",
    "model6.add(Flatten())\n",
    "\n",
    "#Output layer with Softmax activation function\n",
    "model6.add(Dense(num_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model with 'categorical_crossentropy' loss variable and accuracy metrics as this is a classification model\n",
    "model6.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Adding Early stopping callback to the fit function is going to stop the training,\n",
    "#if the val_loss is not going to change even '0.001' for more than 10 continous epochs\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)\n",
    "\n",
    "#Adding Model Checkpoint callback to the fit function is going to save the weights whenever val_loss achieves a new low value. \n",
    "#Hence saving the best weights occurred during training\n",
    "\n",
    "model_checkpoint6 =  ModelCheckpoint('mnist_cnn_checkpoint_{epoch:02d}_loss{val_loss:.4f}.h5',\n",
    "                                                           monitor='val_loss',\n",
    "                                                           verbose=1,\n",
    "                                                           save_best_only=True,\n",
    "                                                           save_weights_only=True,\n",
    "                                                           mode='auto',\n",
    "                                                           period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/30\n",
      "42000/42000 [==============================] - 15s 349us/step - loss: 2.3036 - accuracy: 0.0979 - val_loss: 2.3027 - val_accuracy: 0.1003\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.30271, saving model to mnist_cnn_checkpoint_01_loss2.3027.h5\n",
      "Epoch 2/30\n",
      "42000/42000 [==============================] - 14s 336us/step - loss: 2.3031 - accuracy: 0.0993 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.30271\n",
      "Epoch 3/30\n",
      "42000/42000 [==============================] - 14s 343us/step - loss: 2.3031 - accuracy: 0.0986 - val_loss: 2.3028 - val_accuracy: 0.1006\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.30271\n",
      "Epoch 4/30\n",
      "42000/42000 [==============================] - 14s 345us/step - loss: 2.3030 - accuracy: 0.1013 - val_loss: 2.3028 - val_accuracy: 0.1001\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.30271\n",
      "Epoch 5/30\n",
      "42000/42000 [==============================] - 16s 369us/step - loss: 2.3029 - accuracy: 0.0996 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.30271 to 2.30271, saving model to mnist_cnn_checkpoint_05_loss2.3027.h5\n",
      "Epoch 6/30\n",
      "42000/42000 [==============================] - 16s 370us/step - loss: 2.3029 - accuracy: 0.1014 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.30271 to 2.30267, saving model to mnist_cnn_checkpoint_06_loss2.3027.h5\n",
      "Epoch 7/30\n",
      "42000/42000 [==============================] - 16s 375us/step - loss: 2.3028 - accuracy: 0.1014 - val_loss: 2.3024 - val_accuracy: 0.1011\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.30267 to 2.30242, saving model to mnist_cnn_checkpoint_07_loss2.3024.h5\n",
      "Epoch 8/30\n",
      "42000/42000 [==============================] - 15s 366us/step - loss: 2.2998 - accuracy: 0.1099 - val_loss: 2.2789 - val_accuracy: 0.1502\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.30242 to 2.27893, saving model to mnist_cnn_checkpoint_08_loss2.2789.h5\n",
      "Epoch 9/30\n",
      "42000/42000 [==============================] - 16s 372us/step - loss: 2.2093 - accuracy: 0.1731 - val_loss: 2.1373 - val_accuracy: 0.2141\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.27893 to 2.13726, saving model to mnist_cnn_checkpoint_09_loss2.1373.h5\n",
      "Epoch 10/30\n",
      "42000/42000 [==============================] - 16s 371us/step - loss: 2.0186 - accuracy: 0.2736 - val_loss: 1.8460 - val_accuracy: 0.3491\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.13726 to 1.84602, saving model to mnist_cnn_checkpoint_10_loss1.8460.h5\n",
      "Epoch 11/30\n",
      "42000/42000 [==============================] - 15s 369us/step - loss: 1.8474 - accuracy: 0.3538 - val_loss: 1.7137 - val_accuracy: 0.4311\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.84602 to 1.71368, saving model to mnist_cnn_checkpoint_11_loss1.7137.h5\n",
      "Epoch 12/30\n",
      "42000/42000 [==============================] - 16s 372us/step - loss: 1.5951 - accuracy: 0.4720 - val_loss: 1.4478 - val_accuracy: 0.5349\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.71368 to 1.44779, saving model to mnist_cnn_checkpoint_12_loss1.4478.h5\n",
      "Epoch 13/30\n",
      "42000/42000 [==============================] - 16s 372us/step - loss: 1.4213 - accuracy: 0.5442 - val_loss: 1.2659 - val_accuracy: 0.5960\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.44779 to 1.26586, saving model to mnist_cnn_checkpoint_13_loss1.2659.h5\n",
      "Epoch 14/30\n",
      "42000/42000 [==============================] - 16s 370us/step - loss: 1.3116 - accuracy: 0.5815 - val_loss: 1.2159 - val_accuracy: 0.6200\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.26586 to 1.21588, saving model to mnist_cnn_checkpoint_14_loss1.2159.h5\n",
      "Epoch 15/30\n",
      "42000/42000 [==============================] - 16s 371us/step - loss: 1.2654 - accuracy: 0.6028 - val_loss: 1.1699 - val_accuracy: 0.6334\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.21588 to 1.16986, saving model to mnist_cnn_checkpoint_15_loss1.1699.h5\n",
      "Epoch 16/30\n",
      "42000/42000 [==============================] - 16s 380us/step - loss: 1.2196 - accuracy: 0.6159 - val_loss: 1.1349 - val_accuracy: 0.6499\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.16986 to 1.13491, saving model to mnist_cnn_checkpoint_16_loss1.1349.h5\n",
      "Epoch 17/30\n",
      "42000/42000 [==============================] - 16s 373us/step - loss: 1.1885 - accuracy: 0.6272 - val_loss: 1.0755 - val_accuracy: 0.6658\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.13491 to 1.07552, saving model to mnist_cnn_checkpoint_17_loss1.0755.h5\n",
      "Epoch 18/30\n",
      "42000/42000 [==============================] - 16s 370us/step - loss: 1.1593 - accuracy: 0.6385 - val_loss: 1.0692 - val_accuracy: 0.6748\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.07552 to 1.06919, saving model to mnist_cnn_checkpoint_18_loss1.0692.h5\n",
      "Epoch 19/30\n",
      "42000/42000 [==============================] - 16s 369us/step - loss: 1.1430 - accuracy: 0.6434 - val_loss: 1.0460 - val_accuracy: 0.6822\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.06919 to 1.04603, saving model to mnist_cnn_checkpoint_19_loss1.0460.h5\n",
      "Epoch 20/30\n",
      "42000/42000 [==============================] - 16s 379us/step - loss: 1.1295 - accuracy: 0.6487 - val_loss: 1.0161 - val_accuracy: 0.6907\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.04603 to 1.01615, saving model to mnist_cnn_checkpoint_20_loss1.0161.h5\n",
      "Epoch 21/30\n",
      "42000/42000 [==============================] - 15s 365us/step - loss: 1.1091 - accuracy: 0.6557 - val_loss: 1.0018 - val_accuracy: 0.6924\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.01615 to 1.00182, saving model to mnist_cnn_checkpoint_21_loss1.0018.h5\n",
      "Epoch 22/30\n",
      "42000/42000 [==============================] - 16s 373us/step - loss: 1.0974 - accuracy: 0.6593 - val_loss: 1.0114 - val_accuracy: 0.6869\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.00182\n",
      "Epoch 23/30\n",
      "42000/42000 [==============================] - 15s 364us/step - loss: 1.0900 - accuracy: 0.6581 - val_loss: 1.0044 - val_accuracy: 0.6913\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.00182\n",
      "Epoch 24/30\n",
      "42000/42000 [==============================] - 15s 367us/step - loss: 1.0791 - accuracy: 0.6639 - val_loss: 1.0006 - val_accuracy: 0.6905\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.00182 to 1.00057, saving model to mnist_cnn_checkpoint_24_loss1.0006.h5\n",
      "Epoch 25/30\n",
      "42000/42000 [==============================] - 15s 359us/step - loss: 1.0669 - accuracy: 0.6685 - val_loss: 0.9791 - val_accuracy: 0.7001\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.00057 to 0.97909, saving model to mnist_cnn_checkpoint_25_loss0.9791.h5\n",
      "Epoch 26/30\n",
      "42000/42000 [==============================] - 16s 370us/step - loss: 1.0599 - accuracy: 0.6721 - val_loss: 0.9641 - val_accuracy: 0.7028\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.97909 to 0.96412, saving model to mnist_cnn_checkpoint_26_loss0.9641.h5\n",
      "Epoch 27/30\n",
      "42000/42000 [==============================] - 15s 368us/step - loss: 1.0558 - accuracy: 0.6717 - val_loss: 0.9593 - val_accuracy: 0.7058\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.96412 to 0.95926, saving model to mnist_cnn_checkpoint_27_loss0.9593.h5\n",
      "Epoch 28/30\n",
      "42000/42000 [==============================] - 15s 364us/step - loss: 1.0496 - accuracy: 0.6739 - val_loss: 0.9601 - val_accuracy: 0.7046\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.95926\n",
      "Epoch 29/30\n",
      "42000/42000 [==============================] - 15s 366us/step - loss: 1.0509 - accuracy: 0.6756 - val_loss: 0.9738 - val_accuracy: 0.7063\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.95926\n",
      "Epoch 30/30\n",
      "42000/42000 [==============================] - 15s 360us/step - loss: 1.0333 - accuracy: 0.6807 - val_loss: 0.9618 - val_accuracy: 0.7002\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.95926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25110824f60>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training on the dataset and adding the all the callbacks to the fit function.\n",
    "model6.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val), \n",
    "          callbacks = [early_stopping,model_checkpoint6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation accuracy has increased to 70.02% (with dropout 0.1) from 62.4% (with dropout 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 67us/step\n",
      "Val loss: 0.9618066359361013\n",
      "Val accuracy: 0.700166642665863\n",
      "18000/18000 [==============================] - 1s 69us/step\n",
      "Test loss: 0.9953361388842265\n",
      "Test accuracy: 0.6899444460868835\n"
     ]
    }
   ],
   "source": [
    "# Lets find the scores on validations and test data using this trained model\n",
    "scores_mv6 = model6.evaluate(X_val, y_val, verbose=1)\n",
    "print('Val loss:', scores_mv6[0])\n",
    "print('Val accuracy:', scores_mv6[1])\n",
    "\n",
    "scores_mt6 = model6.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores_mt6[0])\n",
    "print('Test accuracy:', scores_mt6[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuarcy of 69% could be achieved using droput 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #0000FF\"> Increasing the learning Rate </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input size\n",
    "input_dim = X_train.shape[1:]\n",
    "\n",
    "#Initialising a Sequential model\n",
    "model7 = Sequential()\n",
    "\n",
    "# Adding first layer and relu activation fucntion\n",
    "model7.add(Dense(num_classes * 8, input_shape=input_dim,  activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model7.add(Dropout(0.1))\n",
    "\n",
    "# Adding first hidden layer and relu activation fucntion\n",
    "model7.add(Dense(num_classes * 8, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model7.add(Dropout(0.1))\n",
    "           \n",
    "# Adding second hidden layer with relu activation fucntion\n",
    "model7.add(Dense(num_classes * 6, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model7.add(Dropout(0.1))\n",
    "\n",
    "# Adding third hidden layer with relu activation fucntion\n",
    "model7.add(Dense(num_classes * 4, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model7.add(Dropout(0.1))\n",
    "\n",
    "# Adding forth hidden layer with relu activation fucntion\n",
    "model7.add(Dense(num_classes * 2, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model7.add(Dropout(0.1))\n",
    "\n",
    "# Adding fifth hidden layer with relu activation fucntion\n",
    "model7.add(Dense(num_classes, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model7.add(Dropout(0.1))\n",
    "\n",
    "# Flattening the output from previous layer\n",
    "model7.add(Flatten())\n",
    "\n",
    "#Output layer with Softmax activation function\n",
    "model7.add(Dense(num_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining SGD optimizer. Increasing the learnign rate lr from 0.03 (previous case) to 0.05\n",
    "optimizer_sgd_lr003 = keras.optimizers.SGD(lr=0.05, decay=1e-6, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model with 'categorical_crossentropy' loss variable and accuracy metrics as this is a classification model\n",
    "model7.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer_sgd_lr003,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Adding Early stopping callback to the fit function is going to stop the training,\n",
    "#if the val_loss is not going to change even '0.001' for more than 10 continous epochs\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)\n",
    "\n",
    "#Adding Model Checkpoint callback to the fit function is going to save the weights whenever val_loss achieves a new low value. \n",
    "#Hence saving the best weights occurred during training\n",
    "\n",
    "model_checkpoint7 =  ModelCheckpoint('mnist_cnn_checkpoint_{epoch:02d}_loss{val_loss:.4f}.h5',\n",
    "                                                           monitor='val_loss',\n",
    "                                                           verbose=1,\n",
    "                                                           save_best_only=True,\n",
    "                                                           save_weights_only=True,\n",
    "                                                           mode='auto',\n",
    "                                                           period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/30\n",
      "42000/42000 [==============================] - 14s 345us/step - loss: 2.3036 - accuracy: 0.0981 - val_loss: 2.3031 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.30306, saving model to mnist_cnn_checkpoint_01_loss2.3031.h5\n",
      "Epoch 2/30\n",
      "42000/42000 [==============================] - 15s 345us/step - loss: 2.3032 - accuracy: 0.0996 - val_loss: 2.3030 - val_accuracy: 0.0997\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.30306 to 2.30301, saving model to mnist_cnn_checkpoint_02_loss2.3030.h5\n",
      "Epoch 3/30\n",
      "42000/42000 [==============================] - 15s 362us/step - loss: 2.3032 - accuracy: 0.1008 - val_loss: 2.3029 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.30301 to 2.30291, saving model to mnist_cnn_checkpoint_03_loss2.3029.h5\n",
      "Epoch 4/30\n",
      "42000/42000 [==============================] - 15s 360us/step - loss: 2.3033 - accuracy: 0.0963 - val_loss: 2.3029 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.30291 to 2.30289, saving model to mnist_cnn_checkpoint_04_loss2.3029.h5\n",
      "Epoch 5/30\n",
      "42000/42000 [==============================] - 16s 381us/step - loss: 2.3031 - accuracy: 0.0997 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.30289 to 2.30271, saving model to mnist_cnn_checkpoint_05_loss2.3027.h5\n",
      "Epoch 6/30\n",
      "42000/42000 [==============================] - 16s 376us/step - loss: 2.3031 - accuracy: 0.1008 - val_loss: 2.3030 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.30271\n",
      "Epoch 7/30\n",
      "42000/42000 [==============================] - 16s 379us/step - loss: 2.3033 - accuracy: 0.0971 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.30271\n",
      "Epoch 8/30\n",
      "42000/42000 [==============================] - 16s 383us/step - loss: 2.3032 - accuracy: 0.1011 - val_loss: 2.3031 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.30271\n",
      "Epoch 9/30\n",
      "42000/42000 [==============================] - 16s 382us/step - loss: 2.3032 - accuracy: 0.0993 - val_loss: 2.3034 - val_accuracy: 0.1027\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.30271\n",
      "Epoch 10/30\n",
      "42000/42000 [==============================] - 16s 376us/step - loss: 2.3032 - accuracy: 0.1001 - val_loss: 2.3029 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.30271\n",
      "Epoch 11/30\n",
      "42000/42000 [==============================] - 16s 381us/step - loss: 2.3032 - accuracy: 0.0995 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.30271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25111bc5f28>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training on the dataset and adding the all the callbacks to the fit function.\n",
    "model7.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val), \n",
    "          callbacks = [early_stopping,model_checkpoint7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 69us/step\n",
      "Val loss: 2.302724337641398\n",
      "Val accuracy: 0.10000000149011612\n",
      "18000/18000 [==============================] - 1s 68us/step\n",
      "Test loss: 2.302761360168457\n",
      "Test accuracy: 0.10066666454076767\n"
     ]
    }
   ],
   "source": [
    "# Lets find the scores on validations and test data using this trained model\n",
    "scores_mv7 = model7.evaluate(X_val, y_val, verbose=1)\n",
    "print('Val loss:', scores_mv7[0])\n",
    "print('Val accuracy:', scores_mv7[1])\n",
    "\n",
    "scores_mt7 = model7.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores_mt7[0])\n",
    "print('Test accuracy:', scores_mt7[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing the learning rate from 0.03 to 0.05 keeping all the other parameters to be same has decreased the accuracy drastically to 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model8 - Changing the learning rate to 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input size\n",
    "input_dim = X_train.shape[1:]\n",
    "\n",
    "#Initialising a Sequential model\n",
    "model8 = Sequential()\n",
    "\n",
    "# Adding first layer and relu activation fucntion\n",
    "model8.add(Dense(num_classes * 8, input_shape=input_dim,  activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model8.add(Dropout(0.1))\n",
    "\n",
    "# Adding first hidden layer and relu activation fucntion\n",
    "model8.add(Dense(num_classes * 8, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model8.add(Dropout(0.1))\n",
    "           \n",
    "# Adding second hidden layer with relu activation fucntion\n",
    "model8.add(Dense(num_classes * 6, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model8.add(Dropout(0.1))\n",
    "\n",
    "# Adding third hidden layer with relu activation fucntion\n",
    "model8.add(Dense(num_classes * 4, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model8.add(Dropout(0.1))\n",
    "\n",
    "# Adding forth hidden layer with relu activation fucntion\n",
    "model8.add(Dense(num_classes * 2, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model8.add(Dropout(0.1))\n",
    "\n",
    "# Adding fifth hidden layer with relu activation fucntion\n",
    "model8.add(Dense(num_classes, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model8.add(Dropout(0.1))\n",
    "\n",
    "# Flattening the output from previous layer\n",
    "model8.add(Flatten())\n",
    "\n",
    "#Output layer with Softmax activation function\n",
    "model8.add(Dense(num_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining SGD optimizer. Increasing the learnign rate lr from 0.03 (previous case) to 0.05\n",
    "optimizer_sgd_lr003 = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "#Compiling the model with 'categorical_crossentropy' loss variable and accuracy metrics as this is a classification model\n",
    "model8.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer_sgd_lr003,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Adding Early stopping callback to the fit function is going to stop the training,\n",
    "#if the val_loss is not going to change even '0.001' for more than 10 continous epochs\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)\n",
    "\n",
    "#Adding Model Checkpoint callback to the fit function is going to save the weights whenever val_loss achieves a new low value. \n",
    "#Hence saving the best weights occurred during training\n",
    "\n",
    "model_checkpoint8 =  ModelCheckpoint('mnist_cnn_checkpoint_{epoch:02d}_loss{val_loss:.4f}.h5',\n",
    "                                                           monitor='val_loss',\n",
    "                                                           verbose=1,\n",
    "                                                           save_best_only=True,\n",
    "                                                           save_weights_only=True,\n",
    "                                                           mode='auto',\n",
    "                                                           period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/30\n",
      "42000/42000 [==============================] - 14s 341us/step - loss: 2.3032 - accuracy: 0.1005 - val_loss: 2.3025 - val_accuracy: 0.1014\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.30250, saving model to mnist_cnn_checkpoint_01_loss2.3025.h5\n",
      "Epoch 2/30\n",
      "42000/42000 [==============================] - 14s 333us/step - loss: 2.3029 - accuracy: 0.0999 - val_loss: 2.3025 - val_accuracy: 0.0999\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.30250\n",
      "Epoch 3/30\n",
      "42000/42000 [==============================] - 14s 330us/step - loss: 2.3027 - accuracy: 0.1007 - val_loss: 2.3022 - val_accuracy: 0.1004\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.30250 to 2.30220, saving model to mnist_cnn_checkpoint_03_loss2.3022.h5\n",
      "Epoch 4/30\n",
      "42000/42000 [==============================] - 14s 338us/step - loss: 2.3026 - accuracy: 0.1019 - val_loss: 2.3020 - val_accuracy: 0.1060\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.30220 to 2.30199, saving model to mnist_cnn_checkpoint_04_loss2.3020.h5\n",
      "Epoch 5/30\n",
      "42000/42000 [==============================] - 15s 361us/step - loss: 2.3022 - accuracy: 0.1053 - val_loss: 2.3003 - val_accuracy: 0.1061\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.30199 to 2.30033, saving model to mnist_cnn_checkpoint_05_loss2.3003.h5\n",
      "Epoch 6/30\n",
      "42000/42000 [==============================] - 16s 372us/step - loss: 2.3012 - accuracy: 0.1108 - val_loss: 2.2969 - val_accuracy: 0.1145\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.30033 to 2.29693, saving model to mnist_cnn_checkpoint_06_loss2.2969.h5\n",
      "Epoch 7/30\n",
      "42000/42000 [==============================] - 16s 371us/step - loss: 2.2953 - accuracy: 0.1247 - val_loss: 2.2741 - val_accuracy: 0.1985\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.29693 to 2.27414, saving model to mnist_cnn_checkpoint_07_loss2.2741.h5\n",
      "Epoch 8/30\n",
      "42000/42000 [==============================] - 16s 375us/step - loss: 2.1918 - accuracy: 0.2049 - val_loss: 1.9773 - val_accuracy: 0.3030\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.27414 to 1.97731, saving model to mnist_cnn_checkpoint_08_loss1.9773.h5\n",
      "Epoch 9/30\n",
      "42000/42000 [==============================] - 16s 371us/step - loss: 1.9296 - accuracy: 0.3221 - val_loss: 1.8353 - val_accuracy: 0.3680\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.97731 to 1.83534, saving model to mnist_cnn_checkpoint_09_loss1.8353.h5\n",
      "Epoch 10/30\n",
      "42000/42000 [==============================] - 15s 369us/step - loss: 1.8235 - accuracy: 0.3698 - val_loss: 1.7574 - val_accuracy: 0.4012\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.83534 to 1.75740, saving model to mnist_cnn_checkpoint_10_loss1.7574.h5\n",
      "Epoch 11/30\n",
      "42000/42000 [==============================] - 15s 369us/step - loss: 1.7779 - accuracy: 0.3942 - val_loss: 1.6969 - val_accuracy: 0.4300\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.75740 to 1.69693, saving model to mnist_cnn_checkpoint_11_loss1.6969.h5\n",
      "Epoch 12/30\n",
      "42000/42000 [==============================] - 15s 367us/step - loss: 1.7433 - accuracy: 0.4115 - val_loss: 1.6941 - val_accuracy: 0.4387\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.69693 to 1.69412, saving model to mnist_cnn_checkpoint_12_loss1.6941.h5\n",
      "Epoch 13/30\n",
      "42000/42000 [==============================] - 15s 368us/step - loss: 1.7108 - accuracy: 0.4298 - val_loss: 1.6335 - val_accuracy: 0.4623\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.69412 to 1.63347, saving model to mnist_cnn_checkpoint_13_loss1.6335.h5\n",
      "Epoch 14/30\n",
      "42000/42000 [==============================] - 15s 367us/step - loss: 1.6716 - accuracy: 0.4481 - val_loss: 1.5440 - val_accuracy: 0.5009\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.63347 to 1.54397, saving model to mnist_cnn_checkpoint_14_loss1.5440.h5\n",
      "Epoch 15/30\n",
      "42000/42000 [==============================] - 16s 369us/step - loss: 1.5430 - accuracy: 0.4975 - val_loss: 1.4638 - val_accuracy: 0.5239\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.54397 to 1.46382, saving model to mnist_cnn_checkpoint_15_loss1.4638.h5\n",
      "Epoch 16/30\n",
      "42000/42000 [==============================] - 15s 366us/step - loss: 1.4665 - accuracy: 0.5256 - val_loss: 1.3562 - val_accuracy: 0.5701\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.46382 to 1.35624, saving model to mnist_cnn_checkpoint_16_loss1.3562.h5\n",
      "Epoch 17/30\n",
      "42000/42000 [==============================] - 15s 364us/step - loss: 1.4274 - accuracy: 0.5374 - val_loss: 1.3337 - val_accuracy: 0.5753\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.35624 to 1.33366, saving model to mnist_cnn_checkpoint_17_loss1.3337.h5\n",
      "Epoch 18/30\n",
      "42000/42000 [==============================] - 15s 364us/step - loss: 1.4039 - accuracy: 0.5471 - val_loss: 1.3111 - val_accuracy: 0.5819\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.33366 to 1.31114, saving model to mnist_cnn_checkpoint_18_loss1.3111.h5\n",
      "Epoch 19/30\n",
      "42000/42000 [==============================] - 15s 367us/step - loss: 1.3805 - accuracy: 0.5547 - val_loss: 1.3156 - val_accuracy: 0.5796\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.31114\n",
      "Epoch 20/30\n",
      "42000/42000 [==============================] - 15s 361us/step - loss: 1.3664 - accuracy: 0.5595 - val_loss: 1.2561 - val_accuracy: 0.6033\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.31114 to 1.25605, saving model to mnist_cnn_checkpoint_20_loss1.2561.h5\n",
      "Epoch 21/30\n",
      "42000/42000 [==============================] - 16s 373us/step - loss: 1.3335 - accuracy: 0.5723 - val_loss: 1.2235 - val_accuracy: 0.6143\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.25605 to 1.22354, saving model to mnist_cnn_checkpoint_21_loss1.2235.h5\n",
      "Epoch 22/30\n",
      "42000/42000 [==============================] - 16s 380us/step - loss: 1.2849 - accuracy: 0.5934 - val_loss: 1.1881 - val_accuracy: 0.6292\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.22354 to 1.18810, saving model to mnist_cnn_checkpoint_22_loss1.1881.h5\n",
      "Epoch 23/30\n",
      "42000/42000 [==============================] - 16s 372us/step - loss: 1.2349 - accuracy: 0.6105 - val_loss: 1.1370 - val_accuracy: 0.6476\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.18810 to 1.13696, saving model to mnist_cnn_checkpoint_23_loss1.1370.h5\n",
      "Epoch 24/30\n",
      "42000/42000 [==============================] - 15s 368us/step - loss: 1.2089 - accuracy: 0.6205 - val_loss: 1.1424 - val_accuracy: 0.6454\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.13696\n",
      "Epoch 25/30\n",
      "42000/42000 [==============================] - 16s 373us/step - loss: 1.1829 - accuracy: 0.6305 - val_loss: 1.0873 - val_accuracy: 0.6622\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.13696 to 1.08725, saving model to mnist_cnn_checkpoint_25_loss1.0873.h5\n",
      "Epoch 26/30\n",
      "42000/42000 [==============================] - 16s 372us/step - loss: 1.1694 - accuracy: 0.6323 - val_loss: 1.0755 - val_accuracy: 0.6681\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.08725 to 1.07548, saving model to mnist_cnn_checkpoint_26_loss1.0755.h5\n",
      "Epoch 27/30\n",
      "42000/42000 [==============================] - 15s 368us/step - loss: 1.1550 - accuracy: 0.6408 - val_loss: 1.0742 - val_accuracy: 0.6694\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.07548 to 1.07418, saving model to mnist_cnn_checkpoint_27_loss1.0742.h5\n",
      "Epoch 28/30\n",
      "42000/42000 [==============================] - 16s 379us/step - loss: 1.1429 - accuracy: 0.6437 - val_loss: 1.0545 - val_accuracy: 0.6734\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.07418 to 1.05452, saving model to mnist_cnn_checkpoint_28_loss1.0545.h5\n",
      "Epoch 29/30\n",
      "42000/42000 [==============================] - 15s 366us/step - loss: 1.1337 - accuracy: 0.6475 - val_loss: 1.0484 - val_accuracy: 0.6766\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.05452 to 1.04837, saving model to mnist_cnn_checkpoint_29_loss1.0484.h5\n",
      "Epoch 30/30\n",
      "42000/42000 [==============================] - 15s 365us/step - loss: 1.1221 - accuracy: 0.6511 - val_loss: 1.0471 - val_accuracy: 0.6763\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.04837 to 1.04710, saving model to mnist_cnn_checkpoint_30_loss1.0471.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25113f7bf60>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training on the dataset and adding the all the callbacks to the fit function.\n",
    "model8.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val), \n",
    "          callbacks = [early_stopping,model_checkpoint8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 67us/step\n",
      "Val loss: 1.0470961561520895\n",
      "Val accuracy: 0.6763333082199097\n",
      "18000/18000 [==============================] - 1s 67us/step\n",
      "Test loss: 1.0644049133724636\n",
      "Test accuracy: 0.670722246170044\n"
     ]
    }
   ],
   "source": [
    "# Lets find the scores on validations and test data using this trained model\n",
    "scores_mv8 = model8.evaluate(X_val, y_val, verbose=1)\n",
    "print('Val loss:', scores_mv8[0])\n",
    "print('Val accuracy:', scores_mv8[1])\n",
    "\n",
    "scores_mt8 = model8.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores_mt8[0])\n",
    "print('Test accuracy:', scores_mt8[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy has improved significantly with learnign rate of 0.01 compared to lr of 0.05, however it is less when compared to accuracy at learning rate of 0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>S.no</td>\n",
    "        <td>No.of Layers </td>\n",
    "        <td>Regularisation Lambda() Dropout </td>\n",
    "        <td>Regularisation L1, L2</td>\n",
    "        <td>Optimiser</td>\n",
    "        <td>Learning Rate</td>\n",
    "        <td>Loss (test data)</td>\n",
    "        <td>Accuracy (test data) % </td>\n",
    "    </tr>\n",
    "     <tr style=\"font-weight:bold; color:blue\">\n",
    "        <td>1</td>\n",
    "        <td>5</td>\n",
    "        <td>0.1</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.03</td>\n",
    "        <td>0.995</td>\n",
    "        <td>68.99</td>\n",
    "    </tr>\n",
    "   <tr>\n",
    "        <td>2</td>\n",
    "        <td>5</td>\n",
    "        <td>0.1</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.05</td>\n",
    "        <td>2.302</td>\n",
    "        <td>10.06</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>5</td>\n",
    "        <td>0.1</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.01</td>\n",
    "        <td>1.06</td>\n",
    "        <td>67.07</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #0000FF\">From the above Observations, we can see that maximum accuracy (68.99%) and least loss 0.995 is observed when we are have dropu out of 0.1 and learning rate of 0.03 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\">Perform Hyperparameter Optimization . Record findings </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to optimise the hyperparameters. \n",
    "# Increasing the number of hidden layers to 8\n",
    "# Change the learning rate\n",
    "# Change the value of regularisation parameter (dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1 : =0.1 , hidden layers =8 , optimiser = sgd, lr=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input size\n",
    "input_dim = X_train.shape[1:]\n",
    "\n",
    "#Initialising a Sequential model\n",
    "model9 = Sequential()\n",
    "\n",
    "# Adding first layer and relu activation fucntion\n",
    "model9.add(Dense(num_classes * 8, input_shape=input_dim,  activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model9.add(Dropout(0.1))\n",
    "\n",
    "# Adding first hidden layer and relu activation fucntion\n",
    "model9.add(Dense(num_classes * 8, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model9.add(Dropout(0.1))\n",
    "           \n",
    "# Adding second hidden layer with relu activation fucntion\n",
    "model9.add(Dense(num_classes * 6, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model9.add(Dropout(0.1))\n",
    "\n",
    "# Adding third hidden layer with relu activation fucntion\n",
    "model9.add(Dense(num_classes * 4, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model9.add(Dropout(0.1))\n",
    "\n",
    "# Adding forth hidden layer with relu activation fucntion\n",
    "model9.add(Dense(num_classes * 2, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model9.add(Dropout(0.1))\n",
    "\n",
    "# Adding fifth hidden layer with relu activation fucntion\n",
    "model9.add(Dense(num_classes, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model9.add(Dropout(0.1))\n",
    "\n",
    "# Adding sixth hidden layer with relu activation fucntion\n",
    "model9.add(Dense(num_classes, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model9.add(Dropout(0.1))\n",
    "\n",
    "# Adding seventh hidden layer with relu activation fucntion\n",
    "model9.add(Dense(num_classes, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model9.add(Dropout(0.1))\n",
    "\n",
    "# Adding eight hidden layer with relu activation fucntion\n",
    "model9.add(Dense(num_classes, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model9.add(Dropout(0.1))\n",
    "\n",
    "# Flattening the output from previous layer\n",
    "model9.add(Flatten())\n",
    "\n",
    "#Output layer with Softmax activation function\n",
    "model9.add(Dense(num_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining SGD optimizer. Increasing the learnign rate lr from 0.03 (previous case) to 0.05\n",
    "optimizer_sgd_lr003 = keras.optimizers.SGD(lr=0.03, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "#Compiling the model with 'categorical_crossentropy' loss variable and accuracy metrics as this is a classification model\n",
    "model9.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer_sgd_lr003,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Adding Early stopping callback to the fit function is going to stop the training,\n",
    "#if the val_loss is not going to change even '0.001' for more than 10 continous epochs\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)\n",
    "\n",
    "#Adding Model Checkpoint callback to the fit function is going to save the weights whenever val_loss achieves a new low value. \n",
    "#Hence saving the best weights occurred during training\n",
    "\n",
    "model_checkpoint9 =  ModelCheckpoint('mnist_cnn_checkpoint_{epoch:02d}_loss{val_loss:.4f}.h5',\n",
    "                                                           monitor='val_loss',\n",
    "                                                           verbose=1,\n",
    "                                                           save_best_only=True,\n",
    "                                                           save_weights_only=True,\n",
    "                                                           mode='auto',\n",
    "                                                           period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/30\n",
      "42000/42000 [==============================] - 17s 402us/step - loss: 2.3034 - accuracy: 0.0961 - val_loss: 2.3029 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.04710\n",
      "Epoch 2/30\n",
      "42000/42000 [==============================] - 16s 387us/step - loss: 2.3029 - accuracy: 0.1008 - val_loss: 2.3029 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.04710\n",
      "Epoch 3/30\n",
      "42000/42000 [==============================] - 16s 383us/step - loss: 2.3030 - accuracy: 0.0981 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.04710\n",
      "Epoch 4/30\n",
      "42000/42000 [==============================] - 16s 392us/step - loss: 2.3030 - accuracy: 0.1004 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.04710\n",
      "Epoch 5/30\n",
      "42000/42000 [==============================] - 17s 402us/step - loss: 2.3029 - accuracy: 0.1012 - val_loss: 2.3029 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.04710\n",
      "Epoch 6/30\n",
      "42000/42000 [==============================] - 18s 434us/step - loss: 2.3030 - accuracy: 0.0987 - val_loss: 2.3030 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.04710\n",
      "Epoch 7/30\n",
      "42000/42000 [==============================] - 17s 399us/step - loss: 2.3029 - accuracy: 0.0980 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.04710\n",
      "Epoch 8/30\n",
      "42000/42000 [==============================] - 18s 418us/step - loss: 2.3031 - accuracy: 0.0994 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.04710\n",
      "Epoch 9/30\n",
      "42000/42000 [==============================] - 17s 394us/step - loss: 2.3030 - accuracy: 0.0975 - val_loss: 2.3029 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.04710\n",
      "Epoch 10/30\n",
      "42000/42000 [==============================] - 16s 374us/step - loss: 2.3029 - accuracy: 0.1000 - val_loss: 2.3030 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.04710\n",
      "Epoch 11/30\n",
      "42000/42000 [==============================] - 16s 378us/step - loss: 2.3030 - accuracy: 0.1004 - val_loss: 2.3031 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.04710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25114511f28>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training on the dataset and adding the all the callbacks to the fit function.\n",
    "model9.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val), \n",
    "          callbacks = [early_stopping,model_checkpoint9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 75us/step\n",
      "Val loss: 2.3030743836720786\n",
      "Val accuracy: 0.10000000149011612\n",
      "18000/18000 [==============================] - 1s 75us/step\n",
      "Test loss: 2.3031316935221353\n",
      "Test accuracy: 0.10066666454076767\n"
     ]
    }
   ],
   "source": [
    "# Lets find the scores on validations and test data using this trained model\n",
    "scores_mv9 = model9.evaluate(X_val, y_val, verbose=1)\n",
    "print('Val loss:', scores_mv9[0])\n",
    "print('Val accuracy:', scores_mv9[1])\n",
    "\n",
    "scores_mt9 = model9.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores_mt9[0])\n",
    "print('Test accuracy:', scores_mt9[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2 : =0.1 , hidden layers =5 , optimiser = sgd, lr=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Case 2 : =0.1 , hidden layers =5 , optimiser = sgd, lr=0.02\n",
    "\n",
    "## input size\n",
    "input_dim = X_train.shape[1:]\n",
    "\n",
    "#Initialising a Sequential model\n",
    "model10 = Sequential()\n",
    "\n",
    "# Adding first layer and relu activation fucntion\n",
    "model10.add(Dense(num_classes * 8, input_shape=input_dim,  activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model10.add(Dropout(0.1))\n",
    "\n",
    "# Adding first hidden layer and relu activation fucntion\n",
    "model10.add(Dense(num_classes * 8, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model10.add(Dropout(0.1))\n",
    "           \n",
    "# Adding second hidden layer with relu activation fucntion\n",
    "model10.add(Dense(num_classes * 6, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model10.add(Dropout(0.1))\n",
    "\n",
    "# Adding third hidden layer with relu activation fucntion\n",
    "model10.add(Dense(num_classes * 4, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model10.add(Dropout(0.1))\n",
    "\n",
    "# Adding forth hidden layer with relu activation fucntion\n",
    "model10.add(Dense(num_classes * 2, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model10.add(Dropout(0.1))\n",
    "\n",
    "# Adding fifth hidden layer with relu activation fucntion\n",
    "model10.add(Dense(num_classes, activation = 'relu'))\n",
    "#Add a drop out of 0.1\n",
    "model10.add(Dropout(0.1))\n",
    "\n",
    "# Flattening the output from previous layer\n",
    "model10.add(Flatten())\n",
    "\n",
    "#Output layer with Softmax activation function\n",
    "model10.add(Dense(num_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining SGD optimizer. Increasing the learnign rate lr from 0.03 (previous case) to 0.05\n",
    "optimizer = keras.optimizers.SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "#Compiling the model with 'categorical_crossentropy' loss variable and accuracy metrics as this is a classification model\n",
    "model10.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Adding Early stopping callback to the fit function is going to stop the training,\n",
    "#if the val_loss is not going to change even '0.001' for more than 10 continous epochs\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)\n",
    "\n",
    "#Adding Model Checkpoint callback to the fit function is going to save the weights whenever val_loss achieves a new low value. \n",
    "#Hence saving the best weights occurred during training\n",
    "\n",
    "model_checkpoint10 =  ModelCheckpoint('mnist_cnn_checkpoint_{epoch:02d}_loss{val_loss:.4f}.h5',\n",
    "                                                           monitor='val_loss',\n",
    "                                                           verbose=1,\n",
    "                                                           save_best_only=True,\n",
    "                                                           save_weights_only=True,\n",
    "                                                           mode='auto',\n",
    "                                                           period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/30\n",
      "42000/42000 [==============================] - 14s 345us/step - loss: 2.3034 - accuracy: 0.0995 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.30269, saving model to mnist_cnn_checkpoint_01_loss2.3027.h5\n",
      "Epoch 2/30\n",
      "42000/42000 [==============================] - 14s 330us/step - loss: 2.3028 - accuracy: 0.1006 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.30269\n",
      "Epoch 3/30\n",
      "42000/42000 [==============================] - 15s 347us/step - loss: 2.3029 - accuracy: 0.1007 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.30269 to 2.30268, saving model to mnist_cnn_checkpoint_03_loss2.3027.h5\n",
      "Epoch 4/30\n",
      "42000/42000 [==============================] - 14s 339us/step - loss: 2.3030 - accuracy: 0.0960 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.30268 to 2.30266, saving model to mnist_cnn_checkpoint_04_loss2.3027.h5\n",
      "Epoch 5/30\n",
      "42000/42000 [==============================] - 15s 364us/step - loss: 2.3028 - accuracy: 0.0991 - val_loss: 2.3029 - val_accuracy: 0.0999\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.30266\n",
      "Epoch 6/30\n",
      "42000/42000 [==============================] - 16s 373us/step - loss: 2.3030 - accuracy: 0.0983 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.30266\n",
      "Epoch 7/30\n",
      "42000/42000 [==============================] - 16s 370us/step - loss: 2.3028 - accuracy: 0.0985 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.30266\n",
      "Epoch 8/30\n",
      "42000/42000 [==============================] - 15s 365us/step - loss: 2.3029 - accuracy: 0.0979 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.30266\n",
      "Epoch 9/30\n",
      "42000/42000 [==============================] - 15s 365us/step - loss: 2.3028 - accuracy: 0.1002 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.30266\n",
      "Epoch 10/30\n",
      "42000/42000 [==============================] - 15s 362us/step - loss: 2.3029 - accuracy: 0.0992 - val_loss: 2.3027 - val_accuracy: 0.1000\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.30266\n",
      "Epoch 11/30\n",
      "42000/42000 [==============================] - 15s 362us/step - loss: 2.3026 - accuracy: 0.1021 - val_loss: 2.3024 - val_accuracy: 0.1003\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.30266 to 2.30243, saving model to mnist_cnn_checkpoint_11_loss2.3024.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25118f93eb8>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training on the dataset and adding the all the callbacks to the fit function.\n",
    "model10.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val), \n",
    "          callbacks = [early_stopping,model_checkpoint10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 68us/step\n",
      "Val loss: 2.302430588531494\n",
      "Val accuracy: 0.10029999911785126\n",
      "18000/18000 [==============================] - 1s 68us/step\n",
      "Test loss: 2.302679297129313\n",
      "Test accuracy: 0.09661111235618591\n"
     ]
    }
   ],
   "source": [
    "# Lets find the scores on validations and test data using this trained model\n",
    "scores_mv10 = model10.evaluate(X_val, y_val, verbose=1)\n",
    "print('Val loss:', scores_mv10[0])\n",
    "print('Val accuracy:', scores_mv10[1])\n",
    "\n",
    "scores_mt10 = model10.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores_mt10[0])\n",
    "print('Test accuracy:', scores_mt10[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3 : =0.05 , hidden layers =5 , optimiser = sgd, lr=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input size\n",
    "input_dim = X_train.shape[1:]\n",
    "\n",
    "#Initialising a Sequential model\n",
    "model11 = Sequential()\n",
    "\n",
    "# Adding first layer and relu activation fucntion\n",
    "model11.add(Dense(num_classes * 8, input_shape=input_dim,  activation = 'relu'))\n",
    "#Add a drop out of 0.05\n",
    "model11.add(Dropout(0.05))\n",
    "\n",
    "# Adding first hidden layer and relu activation fucntion\n",
    "model11.add(Dense(num_classes * 8, activation = 'relu'))\n",
    "#Add a drop out of 0.05\n",
    "model11.add(Dropout(0.05))\n",
    "           \n",
    "# Adding second hidden layer with relu activation fucntion\n",
    "model11.add(Dense(num_classes * 6, activation = 'relu'))\n",
    "#Add a drop out of 0.05\n",
    "model11.add(Dropout(0.05))\n",
    "\n",
    "# Adding third hidden layer with relu activation fucntion\n",
    "model11.add(Dense(num_classes * 4, activation = 'relu'))\n",
    "#Add a drop out of 0.05\n",
    "model11.add(Dropout(0.05))\n",
    "\n",
    "# Adding forth hidden layer with relu activation fucntion\n",
    "model11.add(Dense(num_classes * 2, activation = 'relu'))\n",
    "#Add a drop out of 0.05\n",
    "model11.add(Dropout(0.05))\n",
    "\n",
    "# Adding fifth hidden layer with relu activation fucntion\n",
    "model11.add(Dense(num_classes, activation = 'relu'))\n",
    "#Add a drop out of 0.05\n",
    "model11.add(Dropout(0.05))\n",
    "\n",
    "# Flattening the output from previous layer\n",
    "model11.add(Flatten())\n",
    "\n",
    "#Output layer with Softmax activation function\n",
    "model11.add(Dense(num_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining SGD optimizer. \n",
    "optimizer = keras.optimizers.SGD(lr=0.03, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "#Compiling the model with 'categorical_crossentropy' loss variable and accuracy metrics as this is a classification model\n",
    "model11.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Adding Early stopping callback to the fit function is going to stop the training,\n",
    "#if the val_loss is not going to change even '0.001' for more than 10 continous epochs\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)\n",
    "\n",
    "#Adding Model Checkpoint callback to the fit function is going to save the weights whenever val_loss achieves a new low value. \n",
    "#Hence saving the best weights occurred during training\n",
    "\n",
    "model_checkpoint11 =  ModelCheckpoint('mnist_cnn_checkpoint_{epoch:02d}_loss{val_loss:.4f}.h5',\n",
    "                                                           monitor='val_loss',\n",
    "                                                           verbose=1,\n",
    "                                                           save_best_only=True,\n",
    "                                                           save_weights_only=True,\n",
    "                                                           mode='auto',\n",
    "                                                           period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/30\n",
      "42000/42000 [==============================] - 15s 345us/step - loss: 2.3030 - accuracy: 0.1010 - val_loss: 2.3014 - val_accuracy: 0.1134\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.30140, saving model to mnist_cnn_checkpoint_01_loss2.3014.h5\n",
      "Epoch 2/30\n",
      "42000/42000 [==============================] - 14s 333us/step - loss: 2.2900 - accuracy: 0.1295 - val_loss: 2.1602 - val_accuracy: 0.2579\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.30140 to 2.16022, saving model to mnist_cnn_checkpoint_02_loss2.1602.h5\n",
      "Epoch 3/30\n",
      "42000/42000 [==============================] - 14s 335us/step - loss: 1.8457 - accuracy: 0.3769 - val_loss: 1.5415 - val_accuracy: 0.5075\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.16022 to 1.54150, saving model to mnist_cnn_checkpoint_03_loss1.5415.h5\n",
      "Epoch 4/30\n",
      "42000/42000 [==============================] - 14s 344us/step - loss: 1.4882 - accuracy: 0.5266 - val_loss: 1.3191 - val_accuracy: 0.5885\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.54150 to 1.31907, saving model to mnist_cnn_checkpoint_04_loss1.3191.h5\n",
      "Epoch 5/30\n",
      "42000/42000 [==============================] - 15s 359us/step - loss: 1.3262 - accuracy: 0.5844 - val_loss: 1.1982 - val_accuracy: 0.6295\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.31907 to 1.19819, saving model to mnist_cnn_checkpoint_05_loss1.1982.h5\n",
      "Epoch 6/30\n",
      "42000/42000 [==============================] - 16s 379us/step - loss: 1.2364 - accuracy: 0.6168 - val_loss: 1.1215 - val_accuracy: 0.6574\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.19819 to 1.12155, saving model to mnist_cnn_checkpoint_06_loss1.1215.h5\n",
      "Epoch 7/30\n",
      "42000/42000 [==============================] - 15s 356us/step - loss: 1.1816 - accuracy: 0.6329 - val_loss: 1.0924 - val_accuracy: 0.6623\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.12155 to 1.09236, saving model to mnist_cnn_checkpoint_07_loss1.0924.h5\n",
      "Epoch 8/30\n",
      "42000/42000 [==============================] - 15s 358us/step - loss: 1.1421 - accuracy: 0.6459 - val_loss: 1.0501 - val_accuracy: 0.6784\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.09236 to 1.05009, saving model to mnist_cnn_checkpoint_08_loss1.0501.h5\n",
      "Epoch 9/30\n",
      "42000/42000 [==============================] - 15s 363us/step - loss: 1.1100 - accuracy: 0.6563 - val_loss: 1.0527 - val_accuracy: 0.6827\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.05009\n",
      "Epoch 10/30\n",
      "42000/42000 [==============================] - 15s 363us/step - loss: 1.0937 - accuracy: 0.6590 - val_loss: 1.0211 - val_accuracy: 0.6879\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.05009 to 1.02109, saving model to mnist_cnn_checkpoint_10_loss1.0211.h5\n",
      "Epoch 11/30\n",
      "42000/42000 [==============================] - 15s 355us/step - loss: 1.0714 - accuracy: 0.6696 - val_loss: 1.0161 - val_accuracy: 0.6885\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.02109 to 1.01612, saving model to mnist_cnn_checkpoint_11_loss1.0161.h5\n",
      "Epoch 12/30\n",
      "42000/42000 [==============================] - 16s 376us/step - loss: 1.0590 - accuracy: 0.6724 - val_loss: 1.0184 - val_accuracy: 0.6885\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.01612\n",
      "Epoch 13/30\n",
      "42000/42000 [==============================] - 15s 356us/step - loss: 1.0488 - accuracy: 0.6792 - val_loss: 0.9891 - val_accuracy: 0.6956\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.01612 to 0.98907, saving model to mnist_cnn_checkpoint_13_loss0.9891.h5\n",
      "Epoch 14/30\n",
      "42000/42000 [==============================] - 15s 358us/step - loss: 1.0345 - accuracy: 0.6809 - val_loss: 0.9782 - val_accuracy: 0.7006\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.98907 to 0.97816, saving model to mnist_cnn_checkpoint_14_loss0.9782.h5\n",
      "Epoch 15/30\n",
      "42000/42000 [==============================] - 15s 363us/step - loss: 1.0254 - accuracy: 0.6816 - val_loss: 0.9696 - val_accuracy: 0.7038\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.97816 to 0.96955, saving model to mnist_cnn_checkpoint_15_loss0.9696.h5\n",
      "Epoch 16/30\n",
      "42000/42000 [==============================] - 15s 362us/step - loss: 1.0135 - accuracy: 0.6899 - val_loss: 0.9585 - val_accuracy: 0.7033\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.96955 to 0.95854, saving model to mnist_cnn_checkpoint_16_loss0.9585.h5\n",
      "Epoch 17/30\n",
      "42000/42000 [==============================] - 15s 366us/step - loss: 1.0093 - accuracy: 0.6885 - val_loss: 0.9536 - val_accuracy: 0.7075\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.95854 to 0.95362, saving model to mnist_cnn_checkpoint_17_loss0.9536.h5\n",
      "Epoch 18/30\n",
      "42000/42000 [==============================] - 15s 363us/step - loss: 0.9998 - accuracy: 0.6933 - val_loss: 0.9353 - val_accuracy: 0.7149\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.95362 to 0.93531, saving model to mnist_cnn_checkpoint_18_loss0.9353.h5\n",
      "Epoch 19/30\n",
      "42000/42000 [==============================] - 15s 360us/step - loss: 0.9897 - accuracy: 0.6954 - val_loss: 0.9269 - val_accuracy: 0.7171\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.93531 to 0.92690, saving model to mnist_cnn_checkpoint_19_loss0.9269.h5\n",
      "Epoch 20/30\n",
      "42000/42000 [==============================] - 15s 361us/step - loss: 0.9882 - accuracy: 0.6956 - val_loss: 0.9305 - val_accuracy: 0.7145\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.92690\n",
      "Epoch 21/30\n",
      "42000/42000 [==============================] - 15s 353us/step - loss: 0.9757 - accuracy: 0.6981 - val_loss: 0.9300 - val_accuracy: 0.7127\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.92690\n",
      "Epoch 22/30\n",
      "42000/42000 [==============================] - 15s 362us/step - loss: 0.9694 - accuracy: 0.7008 - val_loss: 0.9018 - val_accuracy: 0.7270\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.92690 to 0.90176, saving model to mnist_cnn_checkpoint_22_loss0.9018.h5\n",
      "Epoch 23/30\n",
      "42000/42000 [==============================] - 16s 378us/step - loss: 0.9649 - accuracy: 0.7013 - val_loss: 0.9103 - val_accuracy: 0.7244\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.90176\n",
      "Epoch 24/30\n",
      "42000/42000 [==============================] - 16s 373us/step - loss: 0.9604 - accuracy: 0.7034 - val_loss: 0.9239 - val_accuracy: 0.7168\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.90176\n",
      "Epoch 25/30\n",
      "42000/42000 [==============================] - 15s 357us/step - loss: 0.9504 - accuracy: 0.7091 - val_loss: 0.8899 - val_accuracy: 0.7295\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.90176 to 0.88985, saving model to mnist_cnn_checkpoint_25_loss0.8899.h5\n",
      "Epoch 26/30\n",
      "42000/42000 [==============================] - 15s 360us/step - loss: 0.9526 - accuracy: 0.7071 - val_loss: 0.8986 - val_accuracy: 0.7252\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.88985\n",
      "Epoch 27/30\n",
      "42000/42000 [==============================] - 15s 360us/step - loss: 0.9483 - accuracy: 0.7082 - val_loss: 0.9027 - val_accuracy: 0.7250\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.88985\n",
      "Epoch 28/30\n",
      "42000/42000 [==============================] - 15s 360us/step - loss: 0.9395 - accuracy: 0.7084 - val_loss: 0.8875 - val_accuracy: 0.7303\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.88985 to 0.88754, saving model to mnist_cnn_checkpoint_28_loss0.8875.h5\n",
      "Epoch 29/30\n",
      "42000/42000 [==============================] - 15s 369us/step - loss: 0.9381 - accuracy: 0.7128 - val_loss: 0.8790 - val_accuracy: 0.7303\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.88754 to 0.87903, saving model to mnist_cnn_checkpoint_29_loss0.8790.h5\n",
      "Epoch 30/30\n",
      "42000/42000 [==============================] - 15s 357us/step - loss: 0.9336 - accuracy: 0.7122 - val_loss: 0.8669 - val_accuracy: 0.7353\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.87903 to 0.86687, saving model to mnist_cnn_checkpoint_30_loss0.8669.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2511a345eb8>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training on the dataset and adding the all the callbacks to the fit function.\n",
    "model11.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val), \n",
    "          callbacks = [early_stopping,model_checkpoint11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 69us/step\n",
      "Val loss: 0.8668682843049367\n",
      "Val accuracy: 0.7353166937828064\n",
      "18000/18000 [==============================] - 1s 68us/step\n",
      "Test loss: 0.9218880440923902\n",
      "Test accuracy: 0.721833348274231\n"
     ]
    }
   ],
   "source": [
    "# Lets find the scores on validations and test data using this trained model\n",
    "scores_mv11 = model11.evaluate(X_val, y_val, verbose=1)\n",
    "print('Val loss:', scores_mv11[0])\n",
    "print('Val accuracy:', scores_mv11[1])\n",
    "\n",
    "scores_mt11 = model11.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores_mt11[0])\n",
    "print('Test accuracy:', scores_mt11[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #0000FF\">Below are observations from hyperparameter tuning, we can see that accuracy is more when lambda is less (0.05), also loss is only 0.921 in this case</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>S.no</td>\n",
    "        <td>No.of Layers </td>\n",
    "        <td>Regularisation Lambda() Dropout </td>\n",
    "        <td>Regularisation L1, L2</td>\n",
    "        <td>Optimiser</td>\n",
    "        <td>Learning Rate</td>\n",
    "        <td>Loss (test data)</td>\n",
    "        <td>Accuracy (test data) % </td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>1</td>\n",
    "        <td>8</td>\n",
    "        <td>0.1</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.03</td>\n",
    "        <td>2.303</td>\n",
    "        <td>10.06</td>\n",
    "    </tr>\n",
    "   <tr>\n",
    "        <td>2</td>\n",
    "        <td>5</td>\n",
    "        <td>0.1</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.02</td>\n",
    "        <td>2.302</td>\n",
    "        <td>9.6</td>\n",
    "    </tr>\n",
    "    <tr style=\"font-weight:bold; color:blue\">\n",
    "        <td>3</td>\n",
    "        <td>5</td>\n",
    "        <td>0.05</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.03</td>\n",
    "        <td>0.921</td>\n",
    "        <td>72.18</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\">Run a finer search by using a finer range of the hyperparameter. Record findings</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To perform a finer search, lets reduce the lambda value of regularisation parameter to 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input size\n",
    "input_dim = X_train.shape[1:]\n",
    "\n",
    "#Initialising a Sequential model\n",
    "model12 = Sequential()\n",
    "\n",
    "# Adding first layer and relu activation fucntion\n",
    "model12.add(Dense(num_classes * 8, input_shape=input_dim,  activation = 'relu'))\n",
    "#Add a drop out of 0.01\n",
    "model12.add(Dropout(0.01))\n",
    "\n",
    "# Adding first hidden layer and relu activation fucntion\n",
    "model12.add(Dense(num_classes * 8, activation = 'relu'))\n",
    "#Add a drop out of 0.01\n",
    "model12.add(Dropout(0.01))\n",
    "           \n",
    "# Adding second hidden layer with relu activation fucntion\n",
    "model12.add(Dense(num_classes * 6, activation = 'relu'))\n",
    "#Add a drop out of 0.01\n",
    "model12.add(Dropout(0.01))\n",
    "\n",
    "# Adding third hidden layer with relu activation fucntion\n",
    "model12.add(Dense(num_classes * 4, activation = 'relu'))\n",
    "#Add a drop out of 0.01\n",
    "model12.add(Dropout(0.01))\n",
    "\n",
    "# Adding forth hidden layer with relu activation fucntion\n",
    "model12.add(Dense(num_classes * 2, activation = 'relu'))\n",
    "#Add a drop out of 0.01\n",
    "model12.add(Dropout(0.01))\n",
    "\n",
    "# Adding fifth hidden layer with relu activation fucntion\n",
    "model12.add(Dense(num_classes, activation = 'relu'))\n",
    "#Add a drop out of 0.01\n",
    "model12.add(Dropout(0.01))\n",
    "\n",
    "# Flattening the output from previous layer\n",
    "model12.add(Flatten())\n",
    "\n",
    "#Output layer with Softmax activation function\n",
    "model12.add(Dense(num_classes, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining SGD optimizer. \n",
    "optimizer = keras.optimizers.SGD(lr=0.03, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "#Compiling the model with 'categorical_crossentropy' loss variable and accuracy metrics as this is a classification model\n",
    "model12.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Adding Early stopping callback to the fit function is going to stop the training,\n",
    "#if the val_loss is not going to change even '0.001' for more than 10 continous epochs\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)\n",
    "\n",
    "#Adding Model Checkpoint callback to the fit function is going to save the weights whenever val_loss achieves a new low value. \n",
    "#Hence saving the best weights occurred during training\n",
    "\n",
    "model_checkpoint12 =  ModelCheckpoint('mnist_cnn_checkpoint_{epoch:02d}_loss{val_loss:.4f}.h5',\n",
    "                                                           monitor='val_loss',\n",
    "                                                           verbose=1,\n",
    "                                                           save_best_only=True,\n",
    "                                                           save_weights_only=True,\n",
    "                                                           mode='auto',\n",
    "                                                           period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/30\n",
      "42000/42000 [==============================] - 14s 343us/step - loss: 2.3028 - accuracy: 0.1014 - val_loss: 2.3019 - val_accuracy: 0.1056\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.30192, saving model to mnist_cnn_checkpoint_01_loss2.3019.h5\n",
      "Epoch 2/30\n",
      "42000/42000 [==============================] - 14s 340us/step - loss: 2.2869 - accuracy: 0.1308 - val_loss: 2.1961 - val_accuracy: 0.2085\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.30192 to 2.19614, saving model to mnist_cnn_checkpoint_02_loss2.1961.h5\n",
      "Epoch 3/30\n",
      "42000/42000 [==============================] - 14s 343us/step - loss: 1.8775 - accuracy: 0.3712 - val_loss: 1.5668 - val_accuracy: 0.5021\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.19614 to 1.56682, saving model to mnist_cnn_checkpoint_03_loss1.5668.h5\n",
      "Epoch 4/30\n",
      "42000/42000 [==============================] - 14s 336us/step - loss: 1.4497 - accuracy: 0.5439 - val_loss: 1.3199 - val_accuracy: 0.5911\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.56682 to 1.31991, saving model to mnist_cnn_checkpoint_04_loss1.3199.h5\n",
      "Epoch 5/30\n",
      "42000/42000 [==============================] - 15s 360us/step - loss: 1.2940 - accuracy: 0.5985 - val_loss: 1.2292 - val_accuracy: 0.6180\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.31991 to 1.22920, saving model to mnist_cnn_checkpoint_05_loss1.2292.h5\n",
      "Epoch 6/30\n",
      "42000/42000 [==============================] - 15s 359us/step - loss: 1.2086 - accuracy: 0.6279 - val_loss: 1.1399 - val_accuracy: 0.6531\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.22920 to 1.13989, saving model to mnist_cnn_checkpoint_06_loss1.1399.h5\n",
      "Epoch 7/30\n",
      "42000/42000 [==============================] - 16s 372us/step - loss: 1.1489 - accuracy: 0.6453 - val_loss: 1.1311 - val_accuracy: 0.6527\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.13989 to 1.13110, saving model to mnist_cnn_checkpoint_07_loss1.1311.h5\n",
      "Epoch 8/30\n",
      "42000/42000 [==============================] - 16s 378us/step - loss: 1.1111 - accuracy: 0.6574 - val_loss: 1.0513 - val_accuracy: 0.6791\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.13110 to 1.05129, saving model to mnist_cnn_checkpoint_08_loss1.0513.h5\n",
      "Epoch 9/30\n",
      "42000/42000 [==============================] - 16s 371us/step - loss: 1.0795 - accuracy: 0.6684 - val_loss: 1.0377 - val_accuracy: 0.6825\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.05129 to 1.03769, saving model to mnist_cnn_checkpoint_09_loss1.0377.h5\n",
      "Epoch 10/30\n",
      "42000/42000 [==============================] - 16s 376us/step - loss: 1.0569 - accuracy: 0.6765 - val_loss: 1.0088 - val_accuracy: 0.6926\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.03769 to 1.00876, saving model to mnist_cnn_checkpoint_10_loss1.0088.h5\n",
      "Epoch 11/30\n",
      "42000/42000 [==============================] - 15s 367us/step - loss: 1.0335 - accuracy: 0.6825 - val_loss: 0.9987 - val_accuracy: 0.6945\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.00876 to 0.99869, saving model to mnist_cnn_checkpoint_11_loss0.9987.h5\n",
      "Epoch 12/30\n",
      "42000/42000 [==============================] - 15s 360us/step - loss: 1.0160 - accuracy: 0.6884 - val_loss: 1.0394 - val_accuracy: 0.6841\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.99869\n",
      "Epoch 13/30\n",
      "42000/42000 [==============================] - 15s 359us/step - loss: 1.0024 - accuracy: 0.6938 - val_loss: 1.0252 - val_accuracy: 0.6824\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.99869\n",
      "Epoch 14/30\n",
      "42000/42000 [==============================] - 15s 360us/step - loss: 0.9856 - accuracy: 0.6985 - val_loss: 0.9648 - val_accuracy: 0.7037\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.99869 to 0.96475, saving model to mnist_cnn_checkpoint_14_loss0.9648.h5\n",
      "Epoch 15/30\n",
      "42000/42000 [==============================] - 15s 365us/step - loss: 0.9727 - accuracy: 0.7023 - val_loss: 1.0091 - val_accuracy: 0.6863\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.96475\n",
      "Epoch 16/30\n",
      "42000/42000 [==============================] - 15s 368us/step - loss: 0.9673 - accuracy: 0.7048 - val_loss: 0.9496 - val_accuracy: 0.7111\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.96475 to 0.94961, saving model to mnist_cnn_checkpoint_16_loss0.9496.h5\n",
      "Epoch 17/30\n",
      "42000/42000 [==============================] - 16s 370us/step - loss: 0.9588 - accuracy: 0.7091 - val_loss: 0.9518 - val_accuracy: 0.7134\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.94961\n",
      "Epoch 18/30\n",
      "42000/42000 [==============================] - 15s 365us/step - loss: 0.9460 - accuracy: 0.7124 - val_loss: 0.9207 - val_accuracy: 0.7181\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.94961 to 0.92065, saving model to mnist_cnn_checkpoint_18_loss0.9207.h5\n",
      "Epoch 19/30\n",
      "42000/42000 [==============================] - 15s 364us/step - loss: 0.9421 - accuracy: 0.7143 - val_loss: 0.9387 - val_accuracy: 0.7120\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.92065\n",
      "Epoch 20/30\n",
      "42000/42000 [==============================] - 15s 368us/step - loss: 0.9330 - accuracy: 0.7145 - val_loss: 0.9153 - val_accuracy: 0.7226\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.92065 to 0.91530, saving model to mnist_cnn_checkpoint_20_loss0.9153.h5\n",
      "Epoch 21/30\n",
      "42000/42000 [==============================] - 15s 354us/step - loss: 0.9283 - accuracy: 0.7184 - val_loss: 0.9172 - val_accuracy: 0.7223\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.91530\n",
      "Epoch 22/30\n",
      "42000/42000 [==============================] - 15s 366us/step - loss: 0.9179 - accuracy: 0.7198 - val_loss: 0.9282 - val_accuracy: 0.7170\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.91530\n",
      "Epoch 23/30\n",
      "42000/42000 [==============================] - 16s 372us/step - loss: 0.9129 - accuracy: 0.7228 - val_loss: 0.9116 - val_accuracy: 0.7218\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.91530 to 0.91165, saving model to mnist_cnn_checkpoint_23_loss0.9116.h5\n",
      "Epoch 24/30\n",
      "42000/42000 [==============================] - 15s 364us/step - loss: 0.9072 - accuracy: 0.7231 - val_loss: 0.9092 - val_accuracy: 0.7226\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.91165 to 0.90924, saving model to mnist_cnn_checkpoint_24_loss0.9092.h5\n",
      "Epoch 25/30\n",
      "42000/42000 [==============================] - 15s 360us/step - loss: 0.8989 - accuracy: 0.7271 - val_loss: 0.8893 - val_accuracy: 0.7310\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.90924 to 0.88931, saving model to mnist_cnn_checkpoint_25_loss0.8893.h5\n",
      "Epoch 26/30\n",
      "42000/42000 [==============================] - 15s 362us/step - loss: 0.8952 - accuracy: 0.7273 - val_loss: 0.8825 - val_accuracy: 0.7285\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.88931 to 0.88245, saving model to mnist_cnn_checkpoint_26_loss0.8825.h5\n",
      "Epoch 27/30\n",
      "42000/42000 [==============================] - 15s 366us/step - loss: 0.8935 - accuracy: 0.7255 - val_loss: 0.8711 - val_accuracy: 0.7343\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.88245 to 0.87106, saving model to mnist_cnn_checkpoint_27_loss0.8711.h5\n",
      "Epoch 28/30\n",
      "42000/42000 [==============================] - 15s 360us/step - loss: 0.8864 - accuracy: 0.7285 - val_loss: 0.8644 - val_accuracy: 0.7382\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.87106 to 0.86436, saving model to mnist_cnn_checkpoint_28_loss0.8644.h5\n",
      "Epoch 29/30\n",
      "42000/42000 [==============================] - 15s 362us/step - loss: 0.8812 - accuracy: 0.7302 - val_loss: 0.8635 - val_accuracy: 0.7365\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.86436 to 0.86351, saving model to mnist_cnn_checkpoint_29_loss0.8635.h5\n",
      "Epoch 30/30\n",
      "42000/42000 [==============================] - 15s 359us/step - loss: 0.8784 - accuracy: 0.7329 - val_loss: 0.8669 - val_accuracy: 0.7375\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.86351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2511fdcef28>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training on the dataset and adding the all the callbacks to the fit function.\n",
    "model12.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val), \n",
    "          callbacks = [early_stopping,model_checkpoint12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 67us/step\n",
      "Val loss: 0.8668870591004689\n",
      "Val accuracy: 0.737500011920929\n",
      "18000/18000 [==============================] - 1s 67us/step\n",
      "Test loss: 0.9380040101475186\n",
      "Test accuracy: 0.7201111316680908\n"
     ]
    }
   ],
   "source": [
    "# Lets find the scores on validations and test data using this trained model\n",
    "scores_mv12 = model12.evaluate(X_val, y_val, verbose=1)\n",
    "print('Val loss:', scores_mv12[0])\n",
    "print('Val accuracy:', scores_mv12[1])\n",
    "\n",
    "scores_mt12 = model12.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores_mt12[0])\n",
    "print('Test accuracy:', scores_mt12[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.RMSprop(learning_rate=0.03, decay=1e-6)\n",
    "\n",
    "#Compiling the model with 'categorical_crossentropy' loss variable and accuracy metrics as this is a classification model\n",
    "model12.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/30\n",
      "42000/42000 [==============================] - 16s 379us/step - loss: 0.8748 - accuracy: 0.7320 - val_loss: 0.8701 - val_accuracy: 0.7358\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.86351\n",
      "Epoch 2/30\n",
      "42000/42000 [==============================] - 15s 354us/step - loss: 0.8676 - accuracy: 0.7340 - val_loss: 0.8844 - val_accuracy: 0.7333- loss: 0.8691 - accu\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.86351\n",
      "Epoch 3/30\n",
      "42000/42000 [==============================] - 15s 357us/step - loss: 0.8671 - accuracy: 0.7358 - val_loss: 0.8619 - val_accuracy: 0.7363\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.86351 to 0.86187, saving model to mnist_cnn_checkpoint_03_loss0.8619.h5\n",
      "Epoch 4/30\n",
      "42000/42000 [==============================] - 15s 366us/step - loss: 0.8604 - accuracy: 0.7387 - val_loss: 0.8355 - val_accuracy: 0.7464\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.86187 to 0.83548, saving model to mnist_cnn_checkpoint_04_loss0.8355.h5\n",
      "Epoch 5/30\n",
      "42000/42000 [==============================] - 16s 378us/step - loss: 0.8613 - accuracy: 0.7366 - val_loss: 0.8490 - val_accuracy: 0.7419\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.83548\n",
      "Epoch 6/30\n",
      "42000/42000 [==============================] - 17s 396us/step - loss: 0.8561 - accuracy: 0.7396 - val_loss: 0.8426 - val_accuracy: 0.7451\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.83548\n",
      "Epoch 7/30\n",
      "42000/42000 [==============================] - 16s 380us/step - loss: 0.8524 - accuracy: 0.7404 - val_loss: 0.8403 - val_accuracy: 0.7448\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.83548\n",
      "Epoch 8/30\n",
      "42000/42000 [==============================] - 15s 363us/step - loss: 0.8499 - accuracy: 0.7404 - val_loss: 0.8418 - val_accuracy: 0.7439\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.83548\n",
      "Epoch 9/30\n",
      "42000/42000 [==============================] - 15s 360us/step - loss: 0.8473 - accuracy: 0.7405 - val_loss: 0.8443 - val_accuracy: 0.7428\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.83548\n",
      "Epoch 10/30\n",
      "42000/42000 [==============================] - 14s 338us/step - loss: 0.8445 - accuracy: 0.7431 - val_loss: 0.8456 - val_accuracy: 0.7442\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.83548\n",
      "Epoch 11/30\n",
      "42000/42000 [==============================] - 14s 344us/step - loss: 0.8440 - accuracy: 0.7415 - val_loss: 0.8359 - val_accuracy: 0.7462\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.83548\n",
      "Epoch 12/30\n",
      "42000/42000 [==============================] - 18s 423us/step - loss: 0.8409 - accuracy: 0.7425 - val_loss: 0.8231 - val_accuracy: 0.7489\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.83548 to 0.82307, saving model to mnist_cnn_checkpoint_12_loss0.8231.h5\n",
      "Epoch 13/30\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.8365 - accuracy: 0.7438 - val_loss: 0.8513 - val_accuracy: 0.7396\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.82307\n",
      "Epoch 14/30\n",
      "42000/42000 [==============================] - 15s 367us/step - loss: 0.8335 - accuracy: 0.7436 - val_loss: 0.8232 - val_accuracy: 0.7495\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.82307\n",
      "Epoch 15/30\n",
      "42000/42000 [==============================] - 15s 354us/step - loss: 0.8315 - accuracy: 0.7457 - val_loss: 0.8430 - val_accuracy: 0.7444\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.82307\n",
      "Epoch 16/30\n",
      "42000/42000 [==============================] - 16s 371us/step - loss: 0.8289 - accuracy: 0.7450 - val_loss: 0.8235 - val_accuracy: 0.7488\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.82307\n",
      "Epoch 17/30\n",
      "42000/42000 [==============================] - 15s 353us/step - loss: 0.8257 - accuracy: 0.7461 - val_loss: 0.8268 - val_accuracy: 0.7490\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.82307\n",
      "Epoch 18/30\n",
      "42000/42000 [==============================] - 15s 351us/step - loss: 0.8242 - accuracy: 0.7463 - val_loss: 0.8394 - val_accuracy: 0.7459\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.82307\n",
      "Epoch 19/30\n",
      "42000/42000 [==============================] - 15s 356us/step - loss: 0.8202 - accuracy: 0.7470 - val_loss: 0.8140 - val_accuracy: 0.7523\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.82307 to 0.81396, saving model to mnist_cnn_checkpoint_19_loss0.8140.h5\n",
      "Epoch 20/30\n",
      "42000/42000 [==============================] - 15s 355us/step - loss: 0.8202 - accuracy: 0.7450 - val_loss: 0.8126 - val_accuracy: 0.7540\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.81396 to 0.81259, saving model to mnist_cnn_checkpoint_20_loss0.8126.h5\n",
      "Epoch 21/30\n",
      "42000/42000 [==============================] - 15s 353us/step - loss: 0.8146 - accuracy: 0.7483 - val_loss: 0.8145 - val_accuracy: 0.7533\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.81259\n",
      "Epoch 22/30\n",
      "42000/42000 [==============================] - 15s 359us/step - loss: 0.8136 - accuracy: 0.7494 - val_loss: 0.8314 - val_accuracy: 0.7503\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.81259\n",
      "Epoch 23/30\n",
      "42000/42000 [==============================] - 15s 354us/step - loss: 0.8086 - accuracy: 0.7497 - val_loss: 0.8151 - val_accuracy: 0.7520\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.81259\n",
      "Epoch 24/30\n",
      "42000/42000 [==============================] - 16s 390us/step - loss: 0.8102 - accuracy: 0.7510 - val_loss: 0.7952 - val_accuracy: 0.7581\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.81259 to 0.79518, saving model to mnist_cnn_checkpoint_24_loss0.7952.h5\n",
      "Epoch 25/30\n",
      "42000/42000 [==============================] - 16s 386us/step - loss: 0.8094 - accuracy: 0.7506 - val_loss: 0.8234 - val_accuracy: 0.7466\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.79518\n",
      "Epoch 26/30\n",
      "42000/42000 [==============================] - 16s 387us/step - loss: 0.8039 - accuracy: 0.7510 - val_loss: 0.8198 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.79518\n",
      "Epoch 27/30\n",
      "42000/42000 [==============================] - 16s 376us/step - loss: 0.8080 - accuracy: 0.7512 - val_loss: 0.8456 - val_accuracy: 0.7366\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.79518\n",
      "Epoch 28/30\n",
      "42000/42000 [==============================] - 16s 372us/step - loss: 0.8009 - accuracy: 0.7520 - val_loss: 0.8135 - val_accuracy: 0.7521\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.79518\n",
      "Epoch 29/30\n",
      "42000/42000 [==============================] - 16s 372us/step - loss: 0.7997 - accuracy: 0.7528 - val_loss: 0.7969 - val_accuracy: 0.7599\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.79518\n",
      "Epoch 30/30\n",
      "42000/42000 [==============================] - 16s 375us/step - loss: 0.7927 - accuracy: 0.7559 - val_loss: 0.8023 - val_accuracy: 0.7562\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.79518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x25120183f60>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training on the dataset and adding the all the callbacks to the fit function.\n",
    "model12.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_val, y_val), \n",
    "          callbacks = [early_stopping,model_checkpoint12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 4s 70us/step\n",
      "Val loss: 0.8022605590105056\n",
      "Val accuracy: 0.7561666369438171\n",
      "18000/18000 [==============================] - 1s 69us/step\n",
      "Test loss: 0.936615271197425\n",
      "Test accuracy: 0.7249444723129272\n"
     ]
    }
   ],
   "source": [
    "# Lets find the scores on validations and test data using this trained model\n",
    "scores_mv12 = model12.evaluate(X_val, y_val, verbose=1)\n",
    "print('Val loss:', scores_mv12[0])\n",
    "print('Val accuracy:', scores_mv12[1])\n",
    "\n",
    "scores_mt12 = model12.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores_mt12[0])\n",
    "print('Test accuracy:', scores_mt12[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #0000FF\">By fine tuning the hyperparameters, we could increase our accuracy to 72.49% and reduce the loss to 0.936</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>S.no</td>\n",
    "        <td>No.of Layers </td>\n",
    "        <td>Regularisation Lambda() Dropout </td>\n",
    "        <td>Regularisation L1, L2</td>\n",
    "        <td>Optimiser</td>\n",
    "        <td>Learning Rate</td>\n",
    "        <td>Loss (test data)</td>\n",
    "        <td>Accuracy (test data) % </td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>1</td>\n",
    "        <td>8</td>\n",
    "        <td>0.01</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.03</td>\n",
    "        <td>0.938</td>\n",
    "        <td>72.01</td>\n",
    "    </tr>\n",
    "      <tr style=\"font-weight:bold; color:blue\">\n",
    "        <td>2</td>\n",
    "        <td>8</td>\n",
    "        <td>0.01</td>\n",
    "        <td>NA</td>\n",
    "        <td>RMSprop</td>\n",
    "        <td>0.03</td>\n",
    "        <td>0.936</td>\n",
    "        <td>72.49</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #ac6aad\">Set the best hyperparameters found in the previous step. Check the Networks accuracy.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating the Loss and Accuracy scores of the Network based on accuracy on validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 1s 71us/step\n",
      "Test loss: 0.936615271197425\n",
      "Test accuracy: 0.7249444723129272\n"
     ]
    }
   ],
   "source": [
    "scores_mt12 = model12.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores_mt12[0])\n",
    "print('Test accuracy:', scores_mt12[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #0000FF\"> Below table shows the observations from all the models, hyper parameters, regularisation, tuning and their relationship with loss and accuracy. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>S.no</td>\n",
    "        <td>No.of Layers </td>\n",
    "        <td>Regularisation Lambda() Dropout </td>\n",
    "        <td>Regularisation L1, L2</td>\n",
    "        <td>Optimiser</td>\n",
    "        <td>Learning Rate</td>\n",
    "        <td>Loss (test data)</td>\n",
    "        <td>Accuracy (test data) % </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>5</td>\n",
    "        <td>NA</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.03</td>\n",
    "        <td>0.972</td>\n",
    "        <td>70.37</td>\n",
    "    </tr>\n",
    "   <tr style=\"font-weight:bold; color:blue\">\n",
    "        <td>2</td>\n",
    "        <td>5</td>\n",
    "        <td>NA</td>\n",
    "        <td>0</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.03</td>\n",
    "        <td>0.9741</td>\n",
    "        <td>70.59</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>5</td>\n",
    "        <td>0.25</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.03</td>\n",
    "        <td>1.19</td>\n",
    "        <td>62.44</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>4</td>\n",
    "        <td>5</td>\n",
    "        <td>0.50</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.03</td>\n",
    "        <td>2.30</td>\n",
    "        <td>9.5</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>5</td>\n",
    "        <td>5</td>\n",
    "        <td>NA</td>\n",
    "        <td>0.01</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.03</td>\n",
    "        <td>2.31</td>\n",
    "        <td>10.04</td>\n",
    "    </tr>\n",
    "\t<tr style=\"font-weight:bold; color:blue\">\n",
    "        <td>6</td>\n",
    "        <td>5</td>\n",
    "        <td>0.1</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.03</td>\n",
    "        <td>0.995</td>\n",
    "        <td>68.99</td>\n",
    "    </tr>\n",
    "   <tr>\n",
    "        <td>7</td>\n",
    "        <td>5</td>\n",
    "        <td>0.1</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.05</td>\n",
    "        <td>2.302</td>\n",
    "        <td>10.06</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>8</td>\n",
    "        <td>5</td>\n",
    "        <td>0.1</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.01</td>\n",
    "        <td>1.06</td>\n",
    "        <td>67.07</td>\n",
    "    </tr>\n",
    "<tr>\n",
    "        <td>9</td>\n",
    "        <td>8</td>\n",
    "        <td>0.1</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.03</td>\n",
    "        <td>2.303</td>\n",
    "        <td>10.06</td>\n",
    "    </tr>\n",
    "   <tr>\n",
    "        <td>10</td>\n",
    "        <td>5</td>\n",
    "        <td>0.1</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.02</td>\n",
    "        <td>2.302</td>\n",
    "        <td>9.6</td>\n",
    "    </tr>\n",
    "    <tr style=\"font-weight:bold; color:blue\">\n",
    "        <td>11</td>\n",
    "        <td>5</td>\n",
    "        <td>0.05</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.03</td>\n",
    "        <td>0.921</td>\n",
    "        <td>72.18</td>\n",
    "    </tr>\n",
    "\t <tr>\n",
    "        <td>12</td>\n",
    "        <td>8</td>\n",
    "        <td>0.01</td>\n",
    "        <td>NA</td>\n",
    "        <td>SGD</td>\n",
    "        <td>0.03</td>\n",
    "        <td>0.938</td>\n",
    "        <td>72.01</td>\n",
    "    </tr>\n",
    "      <tr style=\"font-weight:bold; color:blue\">\n",
    "        <td>13</td>\n",
    "        <td>8</td>\n",
    "        <td>0.01</td>\n",
    "        <td>NA</td>\n",
    "        <td>RMSprop</td>\n",
    "        <td>0.03</td>\n",
    "        <td>0.936</td>\n",
    "        <td>72.49</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color: #0000FF\"> From all the above steps, we can see we have got a max accuracy of 72.49% with RMS Prop optimiser. </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
